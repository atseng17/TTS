 > Using CUDA:  True
 > Number of GPUs:  2
 > `speakers.json` is saved to /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000/speakers.json.
 > `speakers_file` is updated in the config.json.

 > Model has 86432364 parameters

[4m[1m > EPOCH: 0/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Computing phonemes ...
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 16:12:19) [0m

[1m   --> STEP: 0/161 -- GLOBAL_STEP: 0[0m
     | > loss_gen: 6.11781  (6.11781)
     | > loss_kl: 196.69212  (196.69212)
     | > loss_feat: 0.23847  (0.23847)
     | > loss_mel: 81.95415  (81.95415)
     | > loss_duration: 1.53308  (1.53308)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > loss_0: 286.53564  (286.53564)
     | > grad_norm_0: 0.00000  (0.00000)
     | > loss_disc: 6.11874  (6.11874)
     | > amp_scaler-1: 16384.00000  (16384.00000)
     | > loss_1: 6.11874  (6.11874)
     | > grad_norm_1: 0.00000  (0.00000)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 4.62930  (4.62929)
     | > loader_time: 3.18850  (3.18851)


[1m   --> STEP: 25/161 -- GLOBAL_STEP: 25[0m
     | > loss_gen: 1.63906  (1.99354)
     | > loss_kl: 9.86139  (65.00486)
     | > loss_feat: 0.90708  (0.42547)
     | > loss_mel: 56.88949  (70.24760)
     | > loss_duration: 1.69411  (1.59180)
     | > amp_scaler: 1024.00000  (1474.56000)
     | > loss_0: 70.99114  (139.26327)
     | > grad_norm_0: 112.94186  (197.46393)
     | > loss_disc: 2.79128  (3.28237)
     | > amp_scaler-1: 1024.00000  (1474.56000)
     | > loss_1: 2.79128  (3.28237)
     | > grad_norm_1: 1.86887  (2.72167)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.04550  (2.23283)
     | > loader_time: 0.03270  (0.04048)


[1m   --> STEP: 50/161 -- GLOBAL_STEP: 50[0m
     | > loss_gen: 1.94976  (2.00041)
     | > loss_kl: 4.42051  (35.87654)
     | > loss_feat: 2.77406  (1.25165)
     | > loss_mel: 50.84436  (62.40133)
     | > loss_duration: 1.69649  (1.64724)
     | > amp_scaler: 1024.00000  (1249.28000)
     | > loss_0: 61.68518  (103.17717)
     | > grad_norm_0: 53.90862  (145.38329)
     | > loss_disc: 2.34202  (2.93403)
     | > amp_scaler-1: 1024.00000  (1249.28000)
     | > loss_1: 2.34202  (2.93403)
     | > grad_norm_1: 6.41701  (5.22396)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.14970  (2.16939)
     | > loader_time: 0.04040  (0.05257)


[1m   --> STEP: 75/161 -- GLOBAL_STEP: 75[0m
     | > loss_gen: 2.62677  (2.18113)
     | > loss_kl: 3.64911  (25.17011)
     | > loss_feat: 6.01177  (2.15608)
     | > loss_mel: 45.20457  (58.31732)
     | > loss_duration: 1.72938  (1.68007)
     | > amp_scaler: 1024.00000  (1174.18667)
     | > loss_0: 59.22160  (89.50471)
     | > grad_norm_0: 72.71642  (126.92078)
     | > loss_disc: 2.05696  (2.69846)
     | > amp_scaler-1: 1024.00000  (1174.18667)
     | > loss_1: 2.05696  (2.69846)
     | > grad_norm_1: 12.15815  (6.64995)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.58620  (2.12294)
     | > loader_time: 0.04140  (0.05325)


[1m   --> STEP: 100/161 -- GLOBAL_STEP: 100[0m
     | > loss_gen: 3.33699  (2.30705)
     | > loss_kl: 2.33196  (19.55827)
     | > loss_feat: 3.99153  (2.59186)
     | > loss_mel: 48.40157  (56.19413)
     | > loss_duration: 1.75827  (1.70225)
     | > amp_scaler: 1024.00000  (1136.64000)
     | > loss_0: 59.82032  (82.35357)
     | > grad_norm_0: 100.80658  (117.64897)
     | > loss_disc: 2.09324  (2.57646)
     | > amp_scaler-1: 1024.00000  (1136.64000)
     | > loss_1: 2.09324  (2.57646)
     | > grad_norm_1: 14.54362  (7.96107)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.30840  (2.11317)
     | > loader_time: 0.03940  (0.04891)


[1m   --> STEP: 125/161 -- GLOBAL_STEP: 125[0m
     | > loss_gen: 2.75129  (2.42791)
     | > loss_kl: 2.15788  (16.12371)
     | > loss_feat: 3.89969  (3.03162)
     | > loss_mel: 45.37381  (54.35909)
     | > loss_duration: 1.74948  (1.71745)
     | > amp_scaler: 1024.00000  (1114.11200)
     | > loss_0: 55.93216  (77.65979)
     | > grad_norm_0: 71.04271  (110.49580)
     | > loss_disc: 2.05164  (2.45631)
     | > amp_scaler-1: 1024.00000  (1114.11200)
     | > loss_1: 2.05164  (2.45631)
     | > grad_norm_1: 7.38378  (8.52650)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.37360  (2.14086)
     | > loader_time: 0.04500  (0.04922)


[1m   --> STEP: 150/161 -- GLOBAL_STEP: 150[0m
     | > loss_gen: 2.84594  (2.49313)
     | > loss_kl: 2.28425  (13.88072)
     | > loss_feat: 4.50508  (3.30997)
     | > loss_mel: 43.93891  (52.64810)
     | > loss_duration: 1.71678  (1.72684)
     | > amp_scaler: 1024.00000  (1099.09333)
     | > loss_0: 55.29096  (74.05876)
     | > grad_norm_0: 105.42804  (107.26292)
     | > loss_disc: 1.94061  (2.37695)
     | > amp_scaler-1: 1024.00000  (1099.09333)
     | > loss_1: 1.94061  (2.37695)
     | > grad_norm_1: 7.55824  (9.03486)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.21080  (2.14043)
     | > loader_time: 0.04230  (0.04946)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Computing phonemes ...
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time: 0.02829 [0m(+0.00000)
     | > avg_loss_gen: 3.12055 [0m(+0.00000)
     | > avg_loss_kl: 1.92490 [0m(+0.00000)
     | > avg_loss_feat: 4.97176 [0m(+0.00000)
     | > avg_loss_mel: 37.91220 [0m(+0.00000)
     | > avg_loss_duration: 1.79198 [0m(+0.00000)
     | > avg_loss_0: 49.72139 [0m(+0.00000)
     | > avg_loss_disc: 1.94996 [0m(+0.00000)
     | > avg_loss_1: 1.94996 [0m(+0.00000)

 > BEST MODEL : /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000/best_model_162.pth.tar

[4m[1m > EPOCH: 1/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 16:18:40) [0m

[1m   --> STEP: 13/161 -- GLOBAL_STEP: 175[0m
     | > loss_gen: 2.60771  (2.91136)
     | > loss_kl: 2.18186  (2.23047)
     | > loss_feat: 5.41730  (4.95340)
     | > loss_mel: 39.86864  (40.48216)
     | > loss_duration: 1.71479  (1.76927)
     | > amp_scaler: 1024.00000  (1024.00000)
     | > loss_0: 51.79031  (52.34665)
     | > grad_norm_0: 89.04612  (108.07494)
     | > loss_disc: 2.00215  (1.93057)
     | > amp_scaler-1: 1024.00000  (1024.00000)
     | > loss_1: 2.00215  (1.93057)
     | > grad_norm_1: 13.39524  (11.31656)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.48880  (2.03414)
     | > loader_time: 0.12500  (0.05212)


[1m   --> STEP: 38/161 -- GLOBAL_STEP: 200[0m
     | > loss_gen: 3.12358  (2.93851)
     | > loss_kl: 2.30973  (2.19517)
     | > loss_feat: 3.98101  (4.89680)
     | > loss_mel: 42.90244  (39.95160)
     | > loss_duration: 1.80801  (1.77941)
     | > amp_scaler: 512.00000  (754.52632)
     | > loss_0: 54.12476  (51.76150)
     | > grad_norm_0: 106.98134  (101.99193)
     | > loss_disc: 2.07760  (1.93032)
     | > amp_scaler-1: 512.00000  (754.52632)
     | > loss_1: 2.07760  (1.93032)
     | > grad_norm_1: 20.38487  (10.76528)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.44900  (2.13484)
     | > loader_time: 0.03700  (0.04761)


[1m   --> STEP: 63/161 -- GLOBAL_STEP: 225[0m
     | > loss_gen: 2.62011  (2.90657)
     | > loss_kl: 1.92038  (2.11647)
     | > loss_feat: 4.47876  (4.77906)
     | > loss_mel: 42.36949  (39.40358)
     | > loss_duration: 1.76899  (1.78239)
     | > amp_scaler: 512.00000  (658.28571)
     | > loss_0: 53.15773  (50.98806)
     | > grad_norm_0: 71.54852  (99.94643)
     | > loss_disc: 2.05258  (1.98152)
     | > amp_scaler-1: 512.00000  (658.28571)
     | > loss_1: 2.05258  (1.98152)
     | > grad_norm_1: 8.34587  (11.78148)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.96940  (2.08808)
     | > loader_time: 0.09990  (0.05071)


[1m   --> STEP: 88/161 -- GLOBAL_STEP: 250[0m
     | > loss_gen: 2.41090  (2.87852)
     | > loss_kl: 1.63298  (2.05832)
     | > loss_feat: 3.40989  (4.72328)
     | > loss_mel: 36.02320  (39.08864)
     | > loss_duration: 1.77082  (1.78687)
     | > amp_scaler: 512.00000  (616.72727)
     | > loss_0: 45.24778  (50.53563)
     | > grad_norm_0: 41.83914  (98.81703)
     | > loss_disc: 2.56617  (2.03827)
     | > amp_scaler-1: 512.00000  (616.72727)
     | > loss_1: 2.56617  (2.03827)
     | > grad_norm_1: 18.62510  (12.84888)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.26850  (2.09299)
     | > loader_time: 0.03840  (0.05294)


[1m   --> STEP: 113/161 -- GLOBAL_STEP: 275[0m
     | > loss_gen: 2.44417  (2.83174)
     | > loss_kl: 1.85219  (1.99310)
     | > loss_feat: 3.88832  (4.78404)
     | > loss_mel: 37.47760  (38.86692)
     | > loss_duration: 1.81116  (1.78984)
     | > amp_scaler: 512.00000  (593.55752)
     | > loss_0: 47.47345  (50.26564)
     | > grad_norm_0: 75.88291  (94.89159)
     | > loss_disc: 3.34864  (2.12098)
     | > amp_scaler-1: 512.00000  (593.55752)
     | > loss_1: 3.34864  (2.12098)
     | > grad_norm_1: 8.64040  (14.36819)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.59560  (2.11253)
     | > loader_time: 0.11940  (0.05319)


[1m   --> STEP: 138/161 -- GLOBAL_STEP: 300[0m
     | > loss_gen: 2.53863  (2.81327)
     | > loss_kl: 1.77183  (1.96024)
     | > loss_feat: 4.92080  (4.70239)
     | > loss_mel: 36.80051  (38.58385)
     | > loss_duration: 1.70603  (1.79205)
     | > amp_scaler: 512.00000  (578.78261)
     | > loss_0: 47.73780  (49.85181)
     | > grad_norm_0: 117.14632  (94.62144)
     | > loss_disc: 1.86374  (2.14653)
     | > amp_scaler-1: 512.00000  (578.78261)
     | > loss_1: 1.86374  (2.14653)
     | > grad_norm_1: 12.90957  (14.34533)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.35780  (2.14206)
     | > loader_time: 0.03900  (0.05591)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02670 [0m(-0.00159)
     | > avg_loss_gen:[92m 2.34317 [0m(-0.77738)
     | > avg_loss_kl:[92m 1.58409 [0m(-0.34082)
     | > avg_loss_feat:[91m 6.75171 [0m(+1.77994)
     | > avg_loss_mel:[92m 34.72591 [0m(-3.18629)
     | > avg_loss_duration:[91m 1.80081 [0m(+0.00883)
     | > avg_loss_0:[92m 47.20568 [0m(-2.51570)
     | > avg_loss_disc:[92m 1.89001 [0m(-0.05995)
     | > avg_loss_1:[92m 1.89001 [0m(-0.05995)

 > BEST MODEL : /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000/best_model_324.pth.tar

[4m[1m > EPOCH: 2/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 16:24:53) [0m

[1m   --> STEP: 1/161 -- GLOBAL_STEP: 325[0m
     | > loss_gen: 2.73285  (2.73285)
     | > loss_kl: 1.60485  (1.60485)
     | > loss_feat: 5.89717  (5.89717)
     | > loss_mel: 35.74664  (35.74664)
     | > loss_duration: 1.76975  (1.76975)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 47.75127  (47.75127)
     | > grad_norm_0: 145.77251  (145.77251)
     | > loss_disc: 1.84201  (1.84201)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.84201  (1.84201)
     | > grad_norm_1: 12.38595  (12.38595)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.87080  (1.87080)
     | > loader_time: 0.08370  (0.08374)


[1m   --> STEP: 26/161 -- GLOBAL_STEP: 350[0m
     | > loss_gen: 2.45974  (2.82256)
     | > loss_kl: 1.72356  (1.78198)
     | > loss_feat: 4.79308  (4.60442)
     | > loss_mel: 34.99393  (35.98997)
     | > loss_duration: 1.88380  (1.79559)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 45.85412  (46.99451)
     | > grad_norm_0: 133.63548  (129.87209)
     | > loss_disc: 2.13932  (2.02535)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.13932  (2.02535)
     | > grad_norm_1: 17.89604  (17.03968)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.53880  (2.06171)
     | > loader_time: 0.04650  (0.04392)


[1m   --> STEP: 51/161 -- GLOBAL_STEP: 375[0m
     | > loss_gen: 2.31798  (2.72006)
     | > loss_kl: 1.77037  (1.73738)
     | > loss_feat: 4.04769  (4.33402)
     | > loss_mel: 33.97344  (35.88718)
     | > loss_duration: 1.75875  (1.79675)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 43.86823  (46.47539)
     | > grad_norm_0: 88.99103  (116.35398)
     | > loss_disc: 2.20051  (2.08882)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.20051  (2.08882)
     | > grad_norm_1: 10.37099  (16.16776)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.13740  (2.08153)
     | > loader_time: 0.03460  (0.04691)


[1m   --> STEP: 76/161 -- GLOBAL_STEP: 400[0m
     | > loss_gen: 2.68108  (2.69560)
     | > loss_kl: 1.69039  (1.72721)
     | > loss_feat: 3.87534  (4.23444)
     | > loss_mel: 39.74608  (35.71962)
     | > loss_duration: 1.79128  (1.79997)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 49.78417  (46.17685)
     | > grad_norm_0: 150.87517  (114.05241)
     | > loss_disc: 2.08724  (2.10783)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.08724  (2.10783)
     | > grad_norm_1: 19.72024  (15.87123)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.40560  (2.07503)
     | > loader_time: 0.04240  (0.04766)


[1m   --> STEP: 101/161 -- GLOBAL_STEP: 425[0m
     | > loss_gen: 2.69080  (2.68523)
     | > loss_kl: 1.61467  (1.72415)
     | > loss_feat: 4.50893  (4.21201)
     | > loss_mel: 35.39896  (35.70086)
     | > loss_duration: 1.84445  (1.80339)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 46.05781  (46.12563)
     | > grad_norm_0: 100.29341  (119.54967)
     | > loss_disc: 1.95980  (2.11910)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.95980  (2.11910)
     | > grad_norm_1: 20.42050  (16.62939)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.93000  (2.08045)
     | > loader_time: 0.03280  (0.05135)


[1m   --> STEP: 126/161 -- GLOBAL_STEP: 450[0m
     | > loss_gen: 3.51037  (2.72768)
     | > loss_kl: 1.69607  (1.70619)
     | > loss_feat: 4.47197  (4.31187)
     | > loss_mel: 37.81918  (35.63097)
     | > loss_duration: 1.81563  (1.80566)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 49.31321  (46.18237)
     | > grad_norm_0: 95.46678  (120.22890)
     | > loss_disc: 2.12844  (2.09463)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.12844  (2.09463)
     | > grad_norm_1: 33.61366  (17.08635)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.92000  (2.11383)
     | > loader_time: 0.03360  (0.05009)


[1m   --> STEP: 151/161 -- GLOBAL_STEP: 475[0m
     | > loss_gen: 3.03168  (2.77530)
     | > loss_kl: 1.56187  (1.68572)
     | > loss_feat: 5.58800  (4.45172)
     | > loss_mel: 34.52475  (35.47054)
     | > loss_duration: 1.83403  (1.80260)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 46.54033  (46.18587)
     | > grad_norm_0: 144.85036  (121.49021)
     | > loss_disc: 1.79872  (2.06076)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.79872  (2.06076)
     | > grad_norm_1: 15.09819  (17.02164)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.04320  (2.12869)
     | > loader_time: 0.03670  (0.05054)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02672 [0m(+0.00002)
     | > avg_loss_gen:[91m 2.74854 [0m(+0.40537)
     | > avg_loss_kl:[91m 1.62998 [0m(+0.04589)
     | > avg_loss_feat:[92m 6.05934 [0m(-0.69237)
     | > avg_loss_mel:[92m 32.45451 [0m(-2.27141)
     | > avg_loss_duration:[92m 1.77300 [0m(-0.02781)
     | > avg_loss_0:[92m 44.66536 [0m(-2.54032)
     | > avg_loss_disc:[92m 1.72016 [0m(-0.16984)
     | > avg_loss_1:[92m 1.72016 [0m(-0.16984)

 > BEST MODEL : /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000/best_model_486.pth.tar

[4m[1m > EPOCH: 3/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 16:31:08) [0m

[1m   --> STEP: 14/161 -- GLOBAL_STEP: 500[0m
     | > loss_gen: 2.68889  (3.05398)
     | > loss_kl: 1.62200  (1.56332)
     | > loss_feat: 5.27276  (5.35201)
     | > loss_mel: 33.58908  (35.07390)
     | > loss_duration: 1.88845  (1.80903)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 45.06117  (46.85224)
     | > grad_norm_0: 116.89240  (138.01463)
     | > loss_disc: 1.91397  (1.79583)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.91397  (1.79583)
     | > grad_norm_1: 24.69252  (18.87168)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.18620  (2.07636)
     | > loader_time: 0.03600  (0.04403)


[1m   --> STEP: 39/161 -- GLOBAL_STEP: 525[0m
     | > loss_gen: 3.11801  (3.03772)
     | > loss_kl: 1.59264  (1.61858)
     | > loss_feat: 5.31507  (5.20083)
     | > loss_mel: 34.83323  (35.01586)
     | > loss_duration: 1.80219  (1.80120)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 46.66114  (46.67420)
     | > grad_norm_0: 148.37630  (138.37541)
     | > loss_disc: 1.87130  (1.85865)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.87130  (1.85865)
     | > grad_norm_1: 15.62566  (19.49757)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.27170  (2.12707)
     | > loader_time: 0.03230  (0.04262)


[1m   --> STEP: 64/161 -- GLOBAL_STEP: 550[0m
     | > loss_gen: 2.50385  (3.03366)
     | > loss_kl: 1.56968  (1.59505)
     | > loss_feat: 4.93248  (5.19576)
     | > loss_mel: 33.02816  (34.86713)
     | > loss_duration: 1.81055  (1.79945)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 43.84472  (46.49105)
     | > grad_norm_0: 105.16576  (129.27225)
     | > loss_disc: 1.97133  (1.87257)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.97133  (1.87257)
     | > grad_norm_1: 23.46064  (17.19129)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.36420  (2.08983)
     | > loader_time: 0.04820  (0.04417)


[1m   --> STEP: 89/161 -- GLOBAL_STEP: 575[0m
     | > loss_gen: 3.13285  (3.03246)
     | > loss_kl: 1.58628  (1.58535)
     | > loss_feat: 5.74813  (5.20718)
     | > loss_mel: 34.52381  (34.75290)
     | > loss_duration: 1.77145  (1.79732)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 46.76252  (46.37522)
     | > grad_norm_0: 142.49286  (122.17224)
     | > loss_disc: 1.77146  (1.87066)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.77146  (1.87066)
     | > grad_norm_1: 31.08655  (17.40241)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.51490  (2.08277)
     | > loader_time: 0.02500  (0.04283)


[1m   --> STEP: 114/161 -- GLOBAL_STEP: 600[0m
     | > loss_gen: 3.17942  (3.05961)
     | > loss_kl: 1.04924  (1.57444)
     | > loss_feat: 5.70686  (5.29581)
     | > loss_mel: 36.17276  (34.57176)
     | > loss_duration: 1.75899  (1.80281)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 47.86727  (46.30444)
     | > grad_norm_0: 96.67126  (121.82820)
     | > loss_disc: 1.82527  (1.85459)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.82527  (1.85459)
     | > grad_norm_1: 12.34459  (17.92594)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.01630  (2.09978)
     | > loader_time: 0.03490  (0.04258)


[1m   --> STEP: 139/161 -- GLOBAL_STEP: 625[0m
     | > loss_gen: 2.48734  (3.02923)
     | > loss_kl: 1.61199  (1.57359)
     | > loss_feat: 6.02595  (5.22131)
     | > loss_mel: 34.81499  (34.49020)
     | > loss_duration: 1.74769  (1.80328)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 46.68796  (46.11760)
     | > grad_norm_0: 87.84312  (120.56471)
     | > loss_disc: 1.70072  (1.87860)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.70072  (1.87860)
     | > grad_norm_1: 20.10001  (18.48411)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.00810  (2.13110)
     | > loader_time: 0.03010  (0.04509)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02612 [0m(-0.00060)
     | > avg_loss_gen:[92m 2.38435 [0m(-0.36419)
     | > avg_loss_kl:[91m 1.76873 [0m(+0.13875)
     | > avg_loss_feat:[92m 4.89537 [0m(-1.16397)
     | > avg_loss_mel:[92m 29.80161 [0m(-2.65289)
     | > avg_loss_duration:[92m 1.74328 [0m(-0.02972)
     | > avg_loss_0:[92m 40.59334 [0m(-4.07202)
     | > avg_loss_disc:[91m 2.09429 [0m(+0.37413)
     | > avg_loss_1:[91m 2.09429 [0m(+0.37413)

 > BEST MODEL : /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000/best_model_648.pth.tar

[4m[1m > EPOCH: 4/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 16:37:20) [0m

[1m   --> STEP: 2/161 -- GLOBAL_STEP: 650[0m
     | > loss_gen: 3.91107  (3.32968)
     | > loss_kl: 1.73187  (1.69722)
     | > loss_feat: 5.91130  (5.72381)
     | > loss_mel: 33.44694  (33.97263)
     | > loss_duration: 1.77258  (1.76595)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 46.77376  (46.48929)
     | > grad_norm_0: 143.61937  (127.91933)
     | > loss_disc: 1.83508  (1.86864)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.83508  (1.86864)
     | > grad_norm_1: 14.72231  (16.39246)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.59620  (1.77833)
     | > loader_time: 0.03460  (0.02958)


[1m   --> STEP: 27/161 -- GLOBAL_STEP: 675[0m
     | > loss_gen: 2.14173  (3.10620)
     | > loss_kl: 1.30832  (1.51251)
     | > loss_feat: 4.14797  (5.30871)
     | > loss_mel: 33.58617  (34.31013)
     | > loss_duration: 1.79277  (1.80352)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 42.97697  (46.04107)
     | > grad_norm_0: 153.02556  (148.05174)
     | > loss_disc: 2.29542  (1.91778)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.29542  (1.91778)
     | > grad_norm_1: 37.79813  (23.97161)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.96350  (2.09375)
     | > loader_time: 0.03470  (0.05759)


[1m   --> STEP: 52/161 -- GLOBAL_STEP: 700[0m
     | > loss_gen: 3.47115  (2.95850)
     | > loss_kl: 1.28652  (1.52003)
     | > loss_feat: 3.74869  (5.00221)
     | > loss_mel: 33.83636  (34.01245)
     | > loss_duration: 1.75555  (1.80696)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 44.09827  (45.30015)
     | > grad_norm_0: 154.97806  (145.39053)
     | > loss_disc: 2.33949  (1.99045)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.33949  (1.99045)
     | > grad_norm_1: 19.17546  (23.82252)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.82510  (2.09830)
     | > loader_time: 0.02660  (0.04615)


[1m   --> STEP: 77/161 -- GLOBAL_STEP: 725[0m
     | > loss_gen: 2.08648  (2.83339)
     | > loss_kl: 1.72679  (1.52533)
     | > loss_feat: 4.96994  (4.72078)
     | > loss_mel: 32.57360  (33.63532)
     | > loss_duration: 1.81459  (1.80420)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 43.17140  (44.51902)
     | > grad_norm_0: 88.95291  (139.90350)
     | > loss_disc: 2.24732  (2.06354)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.24732  (2.06354)
     | > grad_norm_1: 41.35555  (23.23542)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.32890  (2.08391)
     | > loader_time: 0.03690  (0.04560)


[1m   --> STEP: 102/161 -- GLOBAL_STEP: 750[0m
     | > loss_gen: 3.27195  (2.74342)
     | > loss_kl: 1.48900  (1.51196)
     | > loss_feat: 3.81740  (4.44976)
     | > loss_mel: 36.30988  (33.60465)
     | > loss_duration: 1.70080  (1.80595)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 46.58903  (44.11575)
     | > grad_norm_0: 151.59653  (138.21585)
     | > loss_disc: 2.21113  (2.12735)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.21113  (2.12735)
     | > grad_norm_1: 18.90994  (23.57779)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.60330  (2.08282)
     | > loader_time: 0.02340  (0.04793)


[1m   --> STEP: 127/161 -- GLOBAL_STEP: 775[0m
     | > loss_gen: 2.62429  (2.67201)
     | > loss_kl: 1.48902  (1.51024)
     | > loss_feat: 3.34461  (4.24680)
     | > loss_mel: 32.25940  (33.59899)
     | > loss_duration: 1.87312  (1.80537)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 41.59044  (43.83340)
     | > grad_norm_0: 137.04849  (137.08089)
     | > loss_disc: 2.35940  (2.17999)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.35940  (2.17999)
     | > grad_norm_1: 16.84930  (23.68401)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.96990  (2.13066)
     | > loader_time: 0.04950  (0.04675)


[1m   --> STEP: 152/161 -- GLOBAL_STEP: 800[0m
     | > loss_gen: 2.58244  (2.62804)
     | > loss_kl: 1.32005  (1.50856)
     | > loss_feat: 2.92002  (4.13190)
     | > loss_mel: 34.74470  (33.62452)
     | > loss_duration: 1.78689  (1.80570)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 43.35410  (43.69873)
     | > grad_norm_0: 124.55692  (138.13860)
     | > loss_disc: 2.59781  (2.20283)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.59781  (2.20283)
     | > grad_norm_1: 24.24681  (23.70101)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.68960  (2.12776)
     | > loader_time: 0.02270  (0.04505)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02633 [0m(+0.00021)
     | > avg_loss_gen:[91m 2.43192 [0m(+0.04757)
     | > avg_loss_kl:[91m 1.89222 [0m(+0.12349)
     | > avg_loss_feat:[92m 2.62675 [0m(-2.26861)
     | > avg_loss_mel:[92m 28.46877 [0m(-1.33285)
     | > avg_loss_duration:[91m 1.76960 [0m(+0.02631)
     | > avg_loss_0:[92m 37.18926 [0m(-3.40408)
     | > avg_loss_disc:[91m 2.55803 [0m(+0.46373)
     | > avg_loss_1:[91m 2.55803 [0m(+0.46373)

 > BEST MODEL : /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000/best_model_810.pth.tar

[4m[1m > EPOCH: 5/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 16:43:32) [0m

[1m   --> STEP: 15/161 -- GLOBAL_STEP: 825[0m
     | > loss_gen: 2.03325  (2.54546)
     | > loss_kl: 1.57330  (1.53947)
     | > loss_feat: 3.77935  (3.89628)
     | > loss_mel: 32.19394  (32.85338)
     | > loss_duration: 1.78260  (1.81255)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 41.36244  (42.64713)
     | > grad_norm_0: 128.29039  (130.49155)
     | > loss_disc: 2.30364  (2.20389)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.30364  (2.20389)
     | > grad_norm_1: 19.35896  (20.18162)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.08980  (2.03763)
     | > loader_time: 0.03590  (0.04336)


[1m   --> STEP: 40/161 -- GLOBAL_STEP: 850[0m
     | > loss_gen: 2.44924  (2.53831)
     | > loss_kl: 1.40671  (1.51692)
     | > loss_feat: 4.41024  (3.90563)
     | > loss_mel: 34.69962  (33.17649)
     | > loss_duration: 1.74544  (1.81234)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 44.71124  (42.94969)
     | > grad_norm_0: 137.10440  (141.10054)
     | > loss_disc: 2.05335  (2.22734)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.05335  (2.22734)
     | > grad_norm_1: 10.81134  (19.83609)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.04660  (2.10717)
     | > loader_time: 0.03440  (0.04467)


[1m   --> STEP: 65/161 -- GLOBAL_STEP: 875[0m
     | > loss_gen: 2.51098  (2.55076)
     | > loss_kl: 1.76736  (1.53647)
     | > loss_feat: 4.41092  (3.91730)
     | > loss_mel: 32.30466  (32.94991)
     | > loss_duration: 1.89449  (1.80284)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 42.88841  (42.75727)
     | > grad_norm_0: 144.11699  (131.33673)
     | > loss_disc: 2.23466  (2.26342)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.23466  (2.26342)
     | > grad_norm_1: 50.53150  (21.88463)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.76320  (2.07343)
     | > loader_time: 0.03040  (0.04532)


[1m   --> STEP: 90/161 -- GLOBAL_STEP: 900[0m
     | > loss_gen: 2.47784  (2.54676)
     | > loss_kl: 1.49380  (1.54345)
     | > loss_feat: 3.73369  (3.94698)
     | > loss_mel: 34.10925  (32.80118)
     | > loss_duration: 1.83174  (1.80285)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 43.64632  (42.64122)
     | > grad_norm_0: 187.60399  (131.69038)
     | > loss_disc: 2.29163  (2.23656)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.29163  (2.23656)
     | > grad_norm_1: 57.12716  (21.87618)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.66910  (2.05896)
     | > loader_time: 0.06450  (0.04798)


[1m   --> STEP: 115/161 -- GLOBAL_STEP: 925[0m
     | > loss_gen: 2.63562  (2.55557)
     | > loss_kl: 1.34731  (1.54521)
     | > loss_feat: 3.50586  (3.95393)
     | > loss_mel: 31.57704  (32.78509)
     | > loss_duration: 1.84286  (1.80837)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 40.90868  (42.64817)
     | > grad_norm_0: 124.25877  (131.52136)
     | > loss_disc: 2.14931  (2.23142)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.14931  (2.23142)
     | > grad_norm_1: 8.89837  (22.14692)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.39230  (2.08300)
     | > loader_time: 0.03890  (0.04722)


[1m   --> STEP: 140/161 -- GLOBAL_STEP: 950[0m
     | > loss_gen: 2.72483  (2.56536)
     | > loss_kl: 1.50908  (1.54994)
     | > loss_feat: 3.78015  (3.99027)
     | > loss_mel: 32.74353  (32.66016)
     | > loss_duration: 1.82101  (1.80997)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 42.57859  (42.57570)
     | > grad_norm_0: 181.54013  (136.61021)
     | > loss_disc: 2.15636  (2.21486)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.15636  (2.21486)
     | > grad_norm_1: 44.88822  (22.76389)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.93830  (2.11203)
     | > loader_time: 0.03020  (0.04563)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02666 [0m(+0.00033)
     | > avg_loss_gen:[91m 2.82478 [0m(+0.39285)
     | > avg_loss_kl:[91m 1.91623 [0m(+0.02402)
     | > avg_loss_feat:[91m 5.78543 [0m(+3.15868)
     | > avg_loss_mel:[91m 31.74703 [0m(+3.27827)
     | > avg_loss_duration:[91m 1.77117 [0m(+0.00157)
     | > avg_loss_0:[91m 44.04465 [0m(+6.85539)
     | > avg_loss_disc:[92m 1.80020 [0m(-0.75783)
     | > avg_loss_1:[92m 1.80020 [0m(-0.75783)


[4m[1m > EPOCH: 6/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 16:49:41) [0m

[1m   --> STEP: 3/161 -- GLOBAL_STEP: 975[0m
     | > loss_gen: 2.79421  (2.87854)
     | > loss_kl: 1.59484  (1.59574)
     | > loss_feat: 5.35876  (4.89714)
     | > loss_mel: 30.20215  (32.82219)
     | > loss_duration: 1.83390  (1.80325)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 41.78386  (43.99686)
     | > grad_norm_0: 124.33167  (126.45892)
     | > loss_disc: 1.76975  (1.90464)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.76975  (1.90464)
     | > grad_norm_1: 9.20959  (33.02342)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.38850  (1.98805)
     | > loader_time: 0.03890  (0.07597)


[1m   --> STEP: 28/161 -- GLOBAL_STEP: 1000[0m
     | > loss_gen: 2.28969  (2.73722)
     | > loss_kl: 1.55744  (1.51967)
     | > loss_feat: 4.35671  (4.59429)
     | > loss_mel: 31.98025  (32.12426)
     | > loss_duration: 1.77121  (1.81106)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 41.95529  (42.78650)
     | > grad_norm_0: 35.23400  (109.77223)
     | > loss_disc: 2.11158  (2.04711)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.11158  (2.04711)
     | > grad_norm_1: 13.52575  (20.59721)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.92470  (2.04603)
     | > loader_time: 0.03020  (0.04188)


[1m   --> STEP: 53/161 -- GLOBAL_STEP: 1025[0m
     | > loss_gen: 2.43332  (2.78409)
     | > loss_kl: 1.55671  (1.53716)
     | > loss_feat: 4.70928  (4.74180)
     | > loss_mel: 31.19909  (32.09718)
     | > loss_duration: 1.82297  (1.80671)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 41.72137  (42.96694)
     | > grad_norm_0: 190.38464  (107.02041)
     | > loss_disc: 2.04699  (2.03489)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.04699  (2.03489)
     | > grad_norm_1: 17.04357  (18.90400)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.81990  (2.08764)
     | > loader_time: 0.02230  (0.04402)


[1m   --> STEP: 78/161 -- GLOBAL_STEP: 1050[0m
     | > loss_gen: 2.56657  (2.78413)
     | > loss_kl: 1.52513  (1.53503)
     | > loss_feat: 5.91963  (4.72161)
     | > loss_mel: 32.57487  (31.90398)
     | > loss_duration: 1.76031  (1.80701)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 44.34652  (42.75177)
     | > grad_norm_0: 125.72846  (110.14587)
     | > loss_disc: 1.85841  (2.03649)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.85841  (2.03649)
     | > grad_norm_1: 14.90509  (20.28019)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.59750  (2.08664)
     | > loader_time: 0.02610  (0.04739)


[1m   --> STEP: 103/161 -- GLOBAL_STEP: 1075[0m
     | > loss_gen: 2.74893  (2.80806)
     | > loss_kl: 1.67553  (1.54682)
     | > loss_feat: 5.11105  (4.79613)
     | > loss_mel: 30.78168  (31.85127)
     | > loss_duration: 1.83131  (1.81047)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 42.14850  (42.81275)
     | > grad_norm_0: 116.91465  (112.26849)
     | > loss_disc: 1.88013  (2.01582)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.88013  (2.01582)
     | > grad_norm_1: 23.69810  (19.39568)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.22030  (2.08660)
     | > loader_time: 0.03210  (0.04518)


[1m   --> STEP: 128/161 -- GLOBAL_STEP: 1100[0m
     | > loss_gen: 2.80800  (2.81844)
     | > loss_kl: 1.52452  (1.54277)
     | > loss_feat: 5.27261  (4.83888)
     | > loss_mel: 34.38947  (31.74662)
     | > loss_duration: 1.86406  (1.81519)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 45.85867  (42.76190)
     | > grad_norm_0: 147.53647  (114.64813)
     | > loss_disc: 1.96213  (2.00646)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.96213  (2.00646)
     | > grad_norm_1: 23.44860  (18.97605)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.64920  (2.13047)
     | > loader_time: 0.05520  (0.04996)


[1m   --> STEP: 153/161 -- GLOBAL_STEP: 1125[0m
     | > loss_gen: 2.72117  (2.79769)
     | > loss_kl: 1.51692  (1.54299)
     | > loss_feat: 3.50453  (4.76833)
     | > loss_mel: 29.44571  (31.71178)
     | > loss_duration: 1.87393  (1.81594)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.06227  (42.63673)
     | > grad_norm_0: 132.99210  (115.80971)
     | > loss_disc: 2.19544  (2.02657)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.19544  (2.02657)
     | > grad_norm_1: 29.66117  (18.99034)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.44950  (2.13063)
     | > loader_time: 0.03770  (0.04839)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02677 [0m(+0.00012)
     | > avg_loss_gen:[92m 2.60586 [0m(-0.21892)
     | > avg_loss_kl:[92m 1.91380 [0m(-0.00244)
     | > avg_loss_feat:[92m 4.63956 [0m(-1.14587)
     | > avg_loss_mel:[92m 29.65264 [0m(-2.09439)
     | > avg_loss_duration:[91m 1.78355 [0m(+0.01237)
     | > avg_loss_0:[92m 40.59541 [0m(-3.44924)
     | > avg_loss_disc:[91m 1.84997 [0m(+0.04976)
     | > avg_loss_1:[91m 1.84997 [0m(+0.04976)


[4m[1m > EPOCH: 7/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 16:55:52) [0m

[1m   --> STEP: 16/161 -- GLOBAL_STEP: 1150[0m
     | > loss_gen: 2.24121  (2.75275)
     | > loss_kl: 1.48364  (1.55207)
     | > loss_feat: 4.21177  (4.62509)
     | > loss_mel: 30.09333  (31.53464)
     | > loss_duration: 1.81257  (1.80509)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.84253  (42.26964)
     | > grad_norm_0: 92.37350  (107.90942)
     | > loss_disc: 2.27501  (2.11823)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.27501  (2.11823)
     | > grad_norm_1: 14.32852  (16.79594)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.26650  (2.12460)
     | > loader_time: 0.03520  (0.04068)


[1m   --> STEP: 41/161 -- GLOBAL_STEP: 1175[0m
     | > loss_gen: 2.58063  (2.72949)
     | > loss_kl: 1.51542  (1.53961)
     | > loss_feat: 4.55060  (4.51511)
     | > loss_mel: 31.79025  (31.71855)
     | > loss_duration: 1.75051  (1.81093)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 42.18742  (42.31369)
     | > grad_norm_0: 244.61606  (129.07376)
     | > loss_disc: 2.01324  (2.12243)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.01324  (2.12243)
     | > grad_norm_1: 10.22677  (16.85037)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.56560  (2.13082)
     | > loader_time: 0.02450  (0.03938)


[1m   --> STEP: 66/161 -- GLOBAL_STEP: 1200[0m
     | > loss_gen: 3.26649  (2.75076)
     | > loss_kl: 1.64923  (1.52181)
     | > loss_feat: 5.27914  (4.65400)
     | > loss_mel: 32.22302  (31.59622)
     | > loss_duration: 1.78995  (1.80554)
     | > amp_scaler: 1024.00000  (667.15152)
     | > loss_0: 44.20784  (42.32833)
     | > grad_norm_0: 141.49203  (128.15288)
     | > loss_disc: 1.93503  (2.06754)
     | > amp_scaler-1: 1024.00000  (667.15152)
     | > loss_1: 1.93503  (2.06754)
     | > grad_norm_1: 12.75674  (16.74651)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.07890  (2.10548)
     | > loader_time: 0.17250  (0.04312)


[1m   --> STEP: 91/161 -- GLOBAL_STEP: 1225[0m
     | > loss_gen: 2.47072  (2.75330)
     | > loss_kl: 1.62139  (1.52387)
     | > loss_feat: 4.13115  (4.65315)
     | > loss_mel: 31.96136  (31.56871)
     | > loss_duration: 1.73841  (1.80353)
     | > amp_scaler: 1024.00000  (765.18681)
     | > loss_0: 41.92302  (42.30256)
     | > grad_norm_0: 77.97762  (129.92993)
     | > loss_disc: 2.22090  (2.07783)
     | > amp_scaler-1: 1024.00000  (765.18681)
     | > loss_1: 2.22090  (2.07783)
     | > grad_norm_1: 10.18939  (18.38359)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.07250  (2.09304)
     | > loader_time: 0.08600  (0.04460)


[1m   --> STEP: 116/161 -- GLOBAL_STEP: 1250[0m
     | > loss_gen: 3.13730  (2.75404)
     | > loss_kl: 1.40467  (1.52845)
     | > loss_feat: 5.05683  (4.66598)
     | > loss_mel: 29.18850  (31.50215)
     | > loss_duration: 1.80582  (1.80811)
     | > amp_scaler: 1024.00000  (820.96552)
     | > loss_0: 40.59313  (42.25873)
     | > grad_norm_0: 76.15854  (126.50647)
     | > loss_disc: 1.94246  (2.06983)
     | > amp_scaler-1: 1024.00000  (820.96552)
     | > loss_1: 1.94246  (2.06983)
     | > grad_norm_1: 25.14812  (19.79473)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.23140  (2.11365)
     | > loader_time: 0.03920  (0.04854)


[1m   --> STEP: 141/161 -- GLOBAL_STEP: 1275[0m
     | > loss_gen: 3.20838  (2.76155)
     | > loss_kl: 1.65759  (1.53713)
     | > loss_feat: 4.97623  (4.69260)
     | > loss_mel: 30.69684  (31.41617)
     | > loss_duration: 1.84740  (1.81307)
     | > amp_scaler: 1024.00000  (856.96454)
     | > loss_0: 42.38644  (42.22051)
     | > grad_norm_0: 127.26176  (123.83992)
     | > loss_disc: 1.93505  (2.06382)
     | > amp_scaler-1: 1024.00000  (856.96454)
     | > loss_1: 1.93505  (2.06382)
     | > grad_norm_1: 9.56345  (19.83777)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.63480  (2.13114)
     | > loader_time: 0.04270  (0.04825)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02741 [0m(+0.00063)
     | > avg_loss_gen:[91m 2.85176 [0m(+0.24590)
     | > avg_loss_kl:[91m 2.02119 [0m(+0.10739)
     | > avg_loss_feat:[92m 4.04679 [0m(-0.59278)
     | > avg_loss_mel:[92m 29.17676 [0m(-0.47588)
     | > avg_loss_duration:[91m 1.82683 [0m(+0.04329)
     | > avg_loss_0:[92m 39.92332 [0m(-0.67208)
     | > avg_loss_disc:[91m 2.64780 [0m(+0.79783)
     | > avg_loss_1:[91m 2.64780 [0m(+0.79783)


[4m[1m > EPOCH: 8/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 17:02:00) [0m

[1m   --> STEP: 4/161 -- GLOBAL_STEP: 1300[0m
     | > loss_gen: 2.74014  (2.93229)
     | > loss_kl: 1.48396  (1.59394)
     | > loss_feat: 4.52768  (4.29213)
     | > loss_mel: 33.92084  (32.41062)
     | > loss_duration: 1.77040  (1.83456)
     | > amp_scaler: 1024.00000  (1024.00000)
     | > loss_0: 44.44302  (43.06355)
     | > grad_norm_0: 51.63644  (49.19408)
     | > loss_disc: 2.25097  (2.61642)
     | > amp_scaler-1: 1024.00000  (1024.00000)
     | > loss_1: 2.25097  (2.61642)
     | > grad_norm_1: 9.68706  (12.53563)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.66040  (1.92282)
     | > loader_time: 0.10640  (0.06777)


[1m   --> STEP: 29/161 -- GLOBAL_STEP: 1325[0m
     | > loss_gen: 2.76329  (2.82721)
     | > loss_kl: 1.37686  (1.58139)
     | > loss_feat: 4.46960  (4.69740)
     | > loss_mel: 32.64946  (31.58293)
     | > loss_duration: 1.86968  (1.81383)
     | > amp_scaler: 1024.00000  (1024.00000)
     | > loss_0: 43.12888  (42.50275)
     | > grad_norm_0: 58.42665  (98.56966)
     | > loss_disc: 2.29068  (2.29384)
     | > amp_scaler-1: 1024.00000  (1024.00000)
     | > loss_1: 2.29068  (2.29384)
     | > grad_norm_1: 9.43031  (16.04222)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.95600  (2.05787)
     | > loader_time: 0.03320  (0.04197)


[1m   --> STEP: 54/161 -- GLOBAL_STEP: 1350[0m
     | > loss_gen: 2.27110  (2.90545)
     | > loss_kl: 1.45933  (1.59013)
     | > loss_feat: 3.77460  (4.78682)
     | > loss_mel: 29.31143  (31.69267)
     | > loss_duration: 1.86604  (1.81488)
     | > amp_scaler: 1024.00000  (1024.00000)
     | > loss_0: 38.68251  (42.78994)
     | > grad_norm_0: 123.43456  (103.59633)
     | > loss_disc: 2.32159  (2.36659)
     | > amp_scaler-1: 1024.00000  (1024.00000)
     | > loss_1: 2.32159  (2.36659)
     | > grad_norm_1: 23.29492  (19.30920)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.35750  (2.08014)
     | > loader_time: 0.03380  (0.04230)


[1m   --> STEP: 79/161 -- GLOBAL_STEP: 1375[0m
     | > loss_gen: 2.67786  (2.85167)
     | > loss_kl: 1.54928  (1.58255)
     | > loss_feat: 4.01578  (4.66907)
     | > loss_mel: 31.31284  (31.57567)
     | > loss_duration: 1.80567  (1.81317)
     | > amp_scaler: 1024.00000  (1024.00000)
     | > loss_0: 41.36144  (42.49213)
     | > grad_norm_0: 159.46432  (96.45120)
     | > loss_disc: 2.29581  (2.35768)
     | > amp_scaler-1: 1024.00000  (1024.00000)
     | > loss_1: 2.29581  (2.35768)
     | > grad_norm_1: 27.75385  (16.69079)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.04930  (2.06879)
     | > loader_time: 0.02790  (0.04637)


[1m   --> STEP: 104/161 -- GLOBAL_STEP: 1400[0m
     | > loss_gen: 2.76572  (2.80894)
     | > loss_kl: 1.63962  (1.57974)
     | > loss_feat: 4.54211  (4.57910)
     | > loss_mel: 28.66810  (31.20183)
     | > loss_duration: 1.82969  (1.81726)
     | > amp_scaler: 1024.00000  (1024.00000)
     | > loss_0: 39.44524  (41.98688)
     | > grad_norm_0: 54.11840  (97.54591)
     | > loss_disc: 1.97388  (2.30319)
     | > amp_scaler-1: 1024.00000  (1024.00000)
     | > loss_1: 1.97388  (2.30319)
     | > grad_norm_1: 5.11188  (16.60300)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.49260  (2.07108)
     | > loader_time: 0.02790  (0.04805)


[1m   --> STEP: 129/161 -- GLOBAL_STEP: 1425[0m
     | > loss_gen: 2.50941  (2.78997)
     | > loss_kl: 1.65473  (1.57837)
     | > loss_feat: 3.99765  (4.57153)
     | > loss_mel: 29.32468  (30.96329)
     | > loss_duration: 1.88385  (1.82085)
     | > amp_scaler: 1024.00000  (1024.00000)
     | > loss_0: 39.37032  (41.72402)
     | > grad_norm_0: 301.00674  (100.56072)
     | > loss_disc: 2.21642  (2.25353)
     | > amp_scaler-1: 1024.00000  (1024.00000)
     | > loss_1: 2.21642  (2.25353)
     | > grad_norm_1: 17.96010  (16.96718)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.75470  (2.13743)
     | > loader_time: 0.04660  (0.04793)


[1m   --> STEP: 154/161 -- GLOBAL_STEP: 1450[0m
     | > loss_gen: 2.78109  (2.79510)
     | > loss_kl: 1.60597  (1.58196)
     | > loss_feat: 4.44539  (4.62444)
     | > loss_mel: 32.30301  (30.82217)
     | > loss_duration: 1.70462  (1.81809)
     | > amp_scaler: 1024.00000  (1024.00000)
     | > loss_0: 42.84008  (41.64176)
     | > grad_norm_0: 131.31952  (101.19414)
     | > loss_disc: 2.14884  (2.21188)
     | > amp_scaler-1: 1024.00000  (1024.00000)
     | > loss_1: 2.14884  (2.21188)
     | > grad_norm_1: 6.52004  (15.63583)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.61280  (2.12893)
     | > loader_time: 0.03000  (0.04819)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02583 [0m(-0.00157)
     | > avg_loss_gen:[92m 2.69009 [0m(-0.16167)
     | > avg_loss_kl:[92m 1.49528 [0m(-0.52590)
     | > avg_loss_feat:[91m 4.54217 [0m(+0.49539)
     | > avg_loss_mel:[92m 27.71109 [0m(-1.46567)
     | > avg_loss_duration:[91m 1.83113 [0m(+0.00430)
     | > avg_loss_0:[92m 38.26976 [0m(-1.65356)
     | > avg_loss_disc:[92m 2.06128 [0m(-0.58652)
     | > avg_loss_1:[92m 2.06128 [0m(-0.58652)


[4m[1m > EPOCH: 9/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 17:08:07) [0m

[1m   --> STEP: 17/161 -- GLOBAL_STEP: 1475[0m
     | > loss_gen: 2.94316  (2.91355)
     | > loss_kl: 1.55588  (1.59401)
     | > loss_feat: 5.24524  (5.10071)
     | > loss_mel: 29.95198  (30.38337)
     | > loss_duration: 1.80011  (1.81404)
     | > amp_scaler: 1024.00000  (1024.00000)
     | > loss_0: 41.49638  (41.80568)
     | > grad_norm_0: 94.36834  (117.23272)
     | > loss_disc: 1.98023  (1.91814)
     | > amp_scaler-1: 1024.00000  (1024.00000)
     | > loss_1: 1.98023  (1.91814)
     | > grad_norm_1: 10.15508  (14.84535)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.12080  (2.08845)
     | > loader_time: 0.03320  (0.04316)


[1m   --> STEP: 42/161 -- GLOBAL_STEP: 1500[0m
     | > loss_gen: 2.80067  (2.88336)
     | > loss_kl: 1.56242  (1.55920)
     | > loss_feat: 4.53945  (5.03314)
     | > loss_mel: 27.78981  (29.94694)
     | > loss_duration: 1.72480  (1.81465)
     | > amp_scaler: 1024.00000  (1024.00000)
     | > loss_0: 38.41715  (41.23730)
     | > grad_norm_0: 99.48038  (105.31716)
     | > loss_disc: 1.99228  (1.96151)
     | > amp_scaler-1: 1024.00000  (1024.00000)
     | > loss_1: 1.99228  (1.96151)
     | > grad_norm_1: 10.07365  (15.13875)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.08900  (2.10371)
     | > loader_time: 0.12600  (0.04922)


[1m   --> STEP: 67/161 -- GLOBAL_STEP: 1525[0m
     | > loss_gen: 2.73549  (2.87127)
     | > loss_kl: 1.40853  (1.56955)
     | > loss_feat: 4.81470  (5.05233)
     | > loss_mel: 27.64588  (29.81685)
     | > loss_duration: 1.78902  (1.80888)
     | > amp_scaler: 1024.00000  (1024.00000)
     | > loss_0: 38.39362  (41.11889)
     | > grad_norm_0: 88.17961  (109.31232)
     | > loss_disc: 2.03280  (1.95089)
     | > amp_scaler-1: 1024.00000  (1024.00000)
     | > loss_1: 2.03280  (1.95089)
     | > grad_norm_1: 12.20775  (14.50705)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.16490  (2.09520)
     | > loader_time: 0.03020  (0.04452)


[1m   --> STEP: 92/161 -- GLOBAL_STEP: 1550[0m
     | > loss_gen: 3.20480  (2.88228)
     | > loss_kl: 1.41750  (1.56253)
     | > loss_feat: 5.15918  (5.07414)
     | > loss_mel: 28.85651  (29.84758)
     | > loss_duration: 1.86389  (1.80736)
     | > amp_scaler: 512.00000  (996.17391)
     | > loss_0: 40.50188  (41.17389)
     | > grad_norm_0: 50.09995  (104.55615)
     | > loss_disc: 1.92289  (1.95424)
     | > amp_scaler-1: 512.00000  (996.17391)
     | > loss_1: 1.92289  (1.95424)
     | > grad_norm_1: 10.15989  (14.79750)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.10000  (2.08063)
     | > loader_time: 0.03340  (0.04195)


[1m   --> STEP: 117/161 -- GLOBAL_STEP: 1575[0m
     | > loss_gen: 3.03600  (2.89789)
     | > loss_kl: 1.37552  (1.56105)
     | > loss_feat: 5.56368  (5.12854)
     | > loss_mel: 30.01816  (29.97271)
     | > loss_duration: 1.81012  (1.81181)
     | > amp_scaler: 512.00000  (892.71795)
     | > loss_0: 41.80348  (41.37199)
     | > grad_norm_0: 141.18155  (104.64886)
     | > loss_disc: 1.86318  (1.95880)
     | > amp_scaler-1: 512.00000  (892.71795)
     | > loss_1: 1.86318  (1.95880)
     | > grad_norm_1: 5.52177  (14.00528)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.42980  (2.11276)
     | > loader_time: 0.04530  (0.04546)


[1m   --> STEP: 142/161 -- GLOBAL_STEP: 1600[0m
     | > loss_gen: 3.09413  (2.90828)
     | > loss_kl: 1.54792  (1.56546)
     | > loss_feat: 4.59877  (5.15393)
     | > loss_mel: 26.43261  (29.94069)
     | > loss_duration: 1.83876  (1.81371)
     | > amp_scaler: 512.00000  (825.69014)
     | > loss_0: 37.51218  (41.38206)
     | > grad_norm_0: 112.23892  (104.20549)
     | > loss_disc: 2.15908  (1.95374)
     | > amp_scaler-1: 512.00000  (825.69014)
     | > loss_1: 2.15908  (1.95374)
     | > grad_norm_1: 7.99523  (13.70289)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.11250  (2.12130)
     | > loader_time: 0.05330  (0.04432)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02687 [0m(+0.00103)
     | > avg_loss_gen:[91m 3.10042 [0m(+0.41033)
     | > avg_loss_kl:[92m 1.42930 [0m(-0.06598)
     | > avg_loss_feat:[91m 4.96324 [0m(+0.42107)
     | > avg_loss_mel:[91m 29.35947 [0m(+1.64838)
     | > avg_loss_duration:[92m 1.81074 [0m(-0.02039)
     | > avg_loss_0:[91m 40.66317 [0m(+2.39341)
     | > avg_loss_disc:[91m 2.07131 [0m(+0.01003)
     | > avg_loss_1:[91m 2.07131 [0m(+0.01003)


[4m[1m > EPOCH: 10/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 17:14:16) [0m

[1m   --> STEP: 5/161 -- GLOBAL_STEP: 1625[0m
     | > loss_gen: 2.91732  (2.85830)
     | > loss_kl: 1.60014  (1.59607)
     | > loss_feat: 5.49852  (4.98230)
     | > loss_mel: 29.34809  (29.08990)
     | > loss_duration: 1.85665  (1.81940)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 41.22073  (40.34597)
     | > grad_norm_0: 75.03937  (102.90810)
     | > loss_disc: 1.81827  (1.99004)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.81827  (1.99004)
     | > grad_norm_1: 7.33783  (14.20856)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.26560  (2.01576)
     | > loader_time: 0.03470  (0.05302)


[1m   --> STEP: 30/161 -- GLOBAL_STEP: 1650[0m
     | > loss_gen: 2.86800  (2.93325)
     | > loss_kl: 1.41942  (1.55186)
     | > loss_feat: 5.50877  (5.23976)
     | > loss_mel: 29.04239  (29.59909)
     | > loss_duration: 1.84245  (1.80722)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 40.68102  (41.13118)
     | > grad_norm_0: 101.31302  (96.03695)
     | > loss_disc: 1.77327  (1.93499)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.77327  (1.93499)
     | > grad_norm_1: 27.92935  (15.23630)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.51390  (2.07623)
     | > loader_time: 0.03970  (0.03910)


[1m   --> STEP: 55/161 -- GLOBAL_STEP: 1675[0m
     | > loss_gen: 3.12099  (2.93642)
     | > loss_kl: 1.54248  (1.55869)
     | > loss_feat: 5.79773  (5.26298)
     | > loss_mel: 30.26328  (29.55518)
     | > loss_duration: 1.88595  (1.81336)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 42.61044  (41.12663)
     | > grad_norm_0: 53.53484  (96.51982)
     | > loss_disc: 1.78041  (1.92054)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.78041  (1.92054)
     | > grad_norm_1: 12.71054  (14.31665)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.38290  (2.07913)
     | > loader_time: 0.03980  (0.03901)


[1m   --> STEP: 80/161 -- GLOBAL_STEP: 1700[0m
     | > loss_gen: 2.82201  (2.96807)
     | > loss_kl: 1.53933  (1.54624)
     | > loss_feat: 6.04303  (5.36144)
     | > loss_mel: 32.28913  (29.51107)
     | > loss_duration: 1.81557  (1.81206)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 44.50908  (41.19887)
     | > grad_norm_0: 125.95918  (97.72774)
     | > loss_disc: 1.70946  (1.90120)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.70946  (1.90120)
     | > grad_norm_1: 13.89215  (13.95456)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.00550  (2.06632)
     | > loader_time: 0.03420  (0.04212)


[1m   --> STEP: 105/161 -- GLOBAL_STEP: 1725[0m
     | > loss_gen: 3.14540  (2.97833)
     | > loss_kl: 1.60096  (1.55105)
     | > loss_feat: 5.17499  (5.37656)
     | > loss_mel: 27.79594  (29.50264)
     | > loss_duration: 1.80913  (1.81160)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.52641  (41.22018)
     | > grad_norm_0: 126.48874  (101.64851)
     | > loss_disc: 1.96642  (1.89829)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.96642  (1.89829)
     | > grad_norm_1: 8.90580  (14.07883)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.14900  (2.06688)
     | > loader_time: 0.15310  (0.04387)


[1m   --> STEP: 130/161 -- GLOBAL_STEP: 1750[0m
     | > loss_gen: 2.87278  (2.98243)
     | > loss_kl: 1.56692  (1.55246)
     | > loss_feat: 6.08277  (5.39130)
     | > loss_mel: 28.66745  (29.37272)
     | > loss_duration: 1.81510  (1.81809)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 41.00502  (41.11699)
     | > grad_norm_0: 129.50650  (104.47165)
     | > loss_disc: 1.80456  (1.89196)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.80456  (1.89196)
     | > grad_norm_1: 10.61300  (14.27818)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.37790  (2.13440)
     | > loader_time: 0.03780  (0.04542)


[1m   --> STEP: 155/161 -- GLOBAL_STEP: 1775[0m
     | > loss_gen: 3.03249  (2.99300)
     | > loss_kl: 1.65619  (1.55654)
     | > loss_feat: 5.53262  (5.42694)
     | > loss_mel: 29.20248  (29.29679)
     | > loss_duration: 1.77763  (1.81934)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 41.20140  (41.09261)
     | > grad_norm_0: 61.01853  (104.20522)
     | > loss_disc: 1.76929  (1.88510)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.76929  (1.88510)
     | > grad_norm_1: 18.72948  (14.28069)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.78130  (2.12050)
     | > loader_time: 0.02900  (0.04539)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02623 [0m(-0.00064)
     | > avg_loss_gen:[92m 2.98555 [0m(-0.11487)
     | > avg_loss_kl:[91m 1.65515 [0m(+0.22585)
     | > avg_loss_feat:[91m 5.77115 [0m(+0.80790)
     | > avg_loss_mel:[92m 27.77343 [0m(-1.58604)
     | > avg_loss_duration:[91m 1.81649 [0m(+0.00575)
     | > avg_loss_0:[92m 40.00176 [0m(-0.66141)
     | > avg_loss_disc:[92m 1.72442 [0m(-0.34688)
     | > avg_loss_1:[92m 1.72442 [0m(-0.34688)


[4m[1m > EPOCH: 11/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 17:20:24) [0m

[1m   --> STEP: 18/161 -- GLOBAL_STEP: 1800[0m
     | > loss_gen: 3.37443  (3.11402)
     | > loss_kl: 1.59752  (1.64767)
     | > loss_feat: 5.31215  (5.74987)
     | > loss_mel: 29.35309  (29.15126)
     | > loss_duration: 1.81274  (1.81259)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 41.44993  (41.47540)
     | > grad_norm_0: 47.28689  (95.08639)
     | > loss_disc: 1.80682  (1.83808)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.80682  (1.83808)
     | > grad_norm_1: 6.50239  (13.80399)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.12870  (2.03120)
     | > loader_time: 0.03140  (0.05032)


[1m   --> STEP: 43/161 -- GLOBAL_STEP: 1825[0m
     | > loss_gen: 3.21364  (3.12398)
     | > loss_kl: 1.50913  (1.59991)
     | > loss_feat: 6.29123  (5.83781)
     | > loss_mel: 30.33964  (29.05396)
     | > loss_duration: 1.79370  (1.81744)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 43.14733  (41.43310)
     | > grad_norm_0: 101.94949  (101.31894)
     | > loss_disc: 1.73383  (1.82317)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.73383  (1.82317)
     | > grad_norm_1: 9.10528  (14.33516)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.35640  (2.10507)
     | > loader_time: 0.03560  (0.04176)


[1m   --> STEP: 68/161 -- GLOBAL_STEP: 1850[0m
     | > loss_gen: 3.19090  (3.14970)
     | > loss_kl: 1.42839  (1.59134)
     | > loss_feat: 5.48521  (5.89632)
     | > loss_mel: 29.01324  (29.18254)
     | > loss_duration: 1.81748  (1.82078)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 40.93521  (41.64068)
     | > grad_norm_0: 63.83253  (104.11430)
     | > loss_disc: 1.91460  (1.97430)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.91460  (1.97430)
     | > grad_norm_1: 9.29945  (20.60110)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.16810  (2.06615)
     | > loader_time: 0.03340  (0.04188)


[1m   --> STEP: 93/161 -- GLOBAL_STEP: 1875[0m
     | > loss_gen: 2.84925  (3.14119)
     | > loss_kl: 1.46200  (1.58811)
     | > loss_feat: 3.99296  (5.85497)
     | > loss_mel: 27.80090  (29.23508)
     | > loss_duration: 1.84541  (1.82110)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 37.95050  (41.64046)
     | > grad_norm_0: 109.06151  (106.34846)
     | > loss_disc: 2.07673  (2.03303)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.07673  (2.03303)
     | > grad_norm_1: 15.66764  (20.95568)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.95760  (2.06537)
     | > loader_time: 0.04620  (0.04119)


[1m   --> STEP: 118/161 -- GLOBAL_STEP: 1900[0m
     | > loss_gen: 2.29080  (3.03760)
     | > loss_kl: 1.45963  (1.57998)
     | > loss_feat: 4.45968  (5.56636)
     | > loss_mel: 29.47432  (29.19295)
     | > loss_duration: 1.94472  (1.82288)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.62914  (41.19977)
     | > grad_norm_0: 95.07348  (99.56693)
     | > loss_disc: 2.42399  (2.11061)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.42399  (2.11061)
     | > grad_norm_1: 6.75371  (18.62921)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.38420  (2.09733)
     | > loader_time: 0.04320  (0.04635)


[1m   --> STEP: 143/161 -- GLOBAL_STEP: 1925[0m
     | > loss_gen: 2.50127  (2.97666)
     | > loss_kl: 1.68879  (1.57922)
     | > loss_feat: 5.15213  (5.36723)
     | > loss_mel: 29.44964  (29.08780)
     | > loss_duration: 1.80753  (1.82409)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 40.59937  (40.83499)
     | > grad_norm_0: 80.31942  (101.28027)
     | > loss_disc: 2.02934  (2.10966)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.02934  (2.10966)
     | > grad_norm_1: 15.38249  (16.83274)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.30620  (2.11156)
     | > loader_time: 0.04020  (0.04666)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02688 [0m(+0.00065)
     | > avg_loss_gen:[91m 3.12319 [0m(+0.13765)
     | > avg_loss_kl:[91m 1.90193 [0m(+0.24678)
     | > avg_loss_feat:[92m 4.29718 [0m(-1.47397)
     | > avg_loss_mel:[92m 26.56025 [0m(-1.21317)
     | > avg_loss_duration:[91m 1.81724 [0m(+0.00075)
     | > avg_loss_0:[92m 37.69979 [0m(-2.30197)
     | > avg_loss_disc:[91m 2.17051 [0m(+0.44608)
     | > avg_loss_1:[91m 2.17051 [0m(+0.44608)


[4m[1m > EPOCH: 12/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 17:26:33) [0m

[1m   --> STEP: 6/161 -- GLOBAL_STEP: 1950[0m
     | > loss_gen: 3.11387  (2.90173)
     | > loss_kl: 1.72348  (1.55645)
     | > loss_feat: 4.57203  (4.84388)
     | > loss_mel: 29.96474  (28.60724)
     | > loss_duration: 1.80311  (1.80604)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 41.17723  (39.71535)
     | > grad_norm_0: 93.76791  (119.24016)
     | > loss_disc: 1.90044  (1.91791)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.90044  (1.91791)
     | > grad_norm_1: 9.07319  (14.15830)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.39970  (2.10429)
     | > loader_time: 0.04150  (0.08778)


[1m   --> STEP: 31/161 -- GLOBAL_STEP: 1975[0m
     | > loss_gen: 2.65464  (2.88605)
     | > loss_kl: 1.56243  (1.54308)
     | > loss_feat: 5.10030  (4.94521)
     | > loss_mel: 26.77972  (28.99785)
     | > loss_duration: 1.81915  (1.82992)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 37.91624  (40.20210)
     | > grad_norm_0: 97.76650  (94.96817)
     | > loss_disc: 1.91570  (2.00075)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.91570  (2.00075)
     | > grad_norm_1: 19.71710  (14.58603)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.48000  (2.06991)
     | > loader_time: 0.04440  (0.04854)


[1m   --> STEP: 56/161 -- GLOBAL_STEP: 2000[0m
     | > loss_gen: 3.07907  (2.90390)
     | > loss_kl: 1.41604  (1.55276)
     | > loss_feat: 5.34448  (5.07091)
     | > loss_mel: 27.61235  (28.79642)
     | > loss_duration: 1.74699  (1.82483)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.19894  (40.14882)
     | > grad_norm_0: 111.25063  (92.76159)
     | > loss_disc: 1.79065  (1.96096)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.79065  (1.96096)
     | > grad_norm_1: 16.39377  (13.00370)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.24790  (2.08569)
     | > loader_time: 0.03580  (0.04358)


[1m   --> STEP: 81/161 -- GLOBAL_STEP: 2025[0m
     | > loss_gen: 2.89013  (2.92522)
     | > loss_kl: 1.85542  (1.55792)
     | > loss_feat: 5.92215  (5.16986)
     | > loss_mel: 28.50157  (28.62713)
     | > loss_duration: 1.86753  (1.82801)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 41.03679  (40.10813)
     | > grad_norm_0: 63.19910  (96.88246)
     | > loss_disc: 1.77060  (1.94397)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.77060  (1.94397)
     | > grad_norm_1: 8.85541  (13.63448)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.90510  (2.06971)
     | > loader_time: 0.03020  (0.04147)


[1m   --> STEP: 106/161 -- GLOBAL_STEP: 2050[0m
     | > loss_gen: 3.11970  (2.92796)
     | > loss_kl: 1.41458  (1.57053)
     | > loss_feat: 5.16342  (5.20080)
     | > loss_mel: 26.99769  (28.49687)
     | > loss_duration: 1.86658  (1.82805)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 38.56196  (40.02421)
     | > grad_norm_0: 48.13021  (94.89365)
     | > loss_disc: 1.90956  (1.95141)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.90956  (1.95141)
     | > grad_norm_1: 12.15223  (12.94529)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.90950  (2.07584)
     | > loader_time: 0.03550  (0.04221)


[1m   --> STEP: 131/161 -- GLOBAL_STEP: 2075[0m
     | > loss_gen: 2.99195  (2.94667)
     | > loss_kl: 1.72769  (1.56577)
     | > loss_feat: 5.47843  (5.27422)
     | > loss_mel: 26.91367  (28.45841)
     | > loss_duration: 1.84924  (1.83208)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 38.96098  (40.07715)
     | > grad_norm_0: 94.17217  (95.15415)
     | > loss_disc: 1.94965  (1.92975)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.94965  (1.92975)
     | > grad_norm_1: 11.61343  (13.36891)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.09630  (2.13089)
     | > loader_time: 0.03780  (0.04675)


[1m   --> STEP: 156/161 -- GLOBAL_STEP: 2100[0m
     | > loss_gen: 3.02101  (2.96014)
     | > loss_kl: 1.65004  (1.56221)
     | > loss_feat: 5.85246  (5.32043)
     | > loss_mel: 30.50119  (28.44410)
     | > loss_duration: 1.81826  (1.82843)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 42.84296  (40.11531)
     | > grad_norm_0: 151.74335  (97.70741)
     | > loss_disc: 1.82542  (1.92266)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.82542  (1.92266)
     | > grad_norm_1: 8.36179  (13.13757)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.00820  (2.11527)
     | > loader_time: 0.03200  (0.04660)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02669 [0m(-0.00020)
     | > avg_loss_gen:[92m 2.45945 [0m(-0.66374)
     | > avg_loss_kl:[92m 1.53395 [0m(-0.36798)
     | > avg_loss_feat:[91m 5.34496 [0m(+1.04778)
     | > avg_loss_mel:[92m 26.50548 [0m(-0.05477)
     | > avg_loss_duration:[92m 1.80600 [0m(-0.01124)
     | > avg_loss_0:[92m 37.64984 [0m(-0.04995)
     | > avg_loss_disc:[92m 2.11059 [0m(-0.05991)
     | > avg_loss_1:[92m 2.11059 [0m(-0.05991)


[4m[1m > EPOCH: 13/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 17:32:41) [0m

[1m   --> STEP: 19/161 -- GLOBAL_STEP: 2125[0m
     | > loss_gen: 3.36170  (3.11844)
     | > loss_kl: 1.55104  (1.55156)
     | > loss_feat: 6.87005  (5.85171)
     | > loss_mel: 30.41010  (28.74138)
     | > loss_duration: 1.83273  (1.82528)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 44.02562  (41.08837)
     | > grad_norm_0: 68.88889  (118.37820)
     | > loss_disc: 1.56522  (1.78017)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.56522  (1.78017)
     | > grad_norm_1: 5.28817  (12.12617)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.92410  (2.04951)
     | > loader_time: 0.03430  (0.05064)


[1m   --> STEP: 44/161 -- GLOBAL_STEP: 2150[0m
     | > loss_gen: 2.88800  (3.08478)
     | > loss_kl: 1.67511  (1.55780)
     | > loss_feat: 5.40207  (5.72796)
     | > loss_mel: 26.59655  (28.29654)
     | > loss_duration: 1.77846  (1.83251)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 38.34019  (40.49959)
     | > grad_norm_0: 80.22629  (107.34921)
     | > loss_disc: 1.92532  (1.82145)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.92532  (1.82145)
     | > grad_norm_1: 9.45971  (12.44306)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.89160  (2.11431)
     | > loader_time: 0.03470  (0.04752)


[1m   --> STEP: 69/161 -- GLOBAL_STEP: 2175[0m
     | > loss_gen: 3.03051  (3.07630)
     | > loss_kl: 1.64783  (1.56124)
     | > loss_feat: 6.09389  (5.72442)
     | > loss_mel: 28.85287  (28.28425)
     | > loss_duration: 1.77329  (1.82495)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 41.39838  (40.47116)
     | > grad_norm_0: 57.18194  (106.62481)
     | > loss_disc: 1.92252  (1.83083)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.92252  (1.83083)
     | > grad_norm_1: 7.07143  (11.86940)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.63160  (2.08049)
     | > loader_time: 0.02900  (0.04491)


[1m   --> STEP: 94/161 -- GLOBAL_STEP: 2200[0m
     | > loss_gen: 3.06912  (3.08639)
     | > loss_kl: 1.44538  (1.53982)
     | > loss_feat: 5.89315  (5.74722)
     | > loss_mel: 29.33648  (28.20766)
     | > loss_duration: 1.88567  (1.82596)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 41.62981  (40.40705)
     | > grad_norm_0: 83.76609  (99.43869)
     | > loss_disc: 1.71059  (1.84024)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.71059  (1.84024)
     | > grad_norm_1: 5.29873  (11.63829)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.34830  (2.08526)
     | > loader_time: 0.11630  (0.04759)


[1m   --> STEP: 119/161 -- GLOBAL_STEP: 2225[0m
     | > loss_gen: 2.77343  (3.07722)
     | > loss_kl: 1.32331  (1.52686)
     | > loss_feat: 6.21581  (5.74711)
     | > loss_mel: 27.43924  (28.29435)
     | > loss_duration: 1.90880  (1.82351)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.66058  (40.46905)
     | > grad_norm_0: 94.16994  (101.86414)
     | > loss_disc: 1.72688  (1.83530)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.72688  (1.83530)
     | > grad_norm_1: 22.36667  (11.42803)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.04700  (2.10367)
     | > loader_time: 0.03640  (0.04754)


[1m   --> STEP: 144/161 -- GLOBAL_STEP: 2250[0m
     | > loss_gen: 3.33604  (3.08629)
     | > loss_kl: 1.38652  (1.52447)
     | > loss_feat: 6.01943  (5.76409)
     | > loss_mel: 28.82192  (28.31046)
     | > loss_duration: 1.83136  (1.82392)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 41.39528  (40.50924)
     | > grad_norm_0: 56.04100  (101.93163)
     | > loss_disc: 1.95551  (1.83550)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.95551  (1.83550)
     | > grad_norm_1: 11.67591  (11.52614)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.31540  (2.12409)
     | > loader_time: 0.05070  (0.04682)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02786 [0m(+0.00118)
     | > avg_loss_gen:[91m 2.61400 [0m(+0.15454)
     | > avg_loss_kl:[92m 1.43711 [0m(-0.09684)
     | > avg_loss_feat:[91m 5.66236 [0m(+0.31740)
     | > avg_loss_mel:[91m 26.80067 [0m(+0.29519)
     | > avg_loss_duration:[92m 1.79634 [0m(-0.00966)
     | > avg_loss_0:[91m 38.31047 [0m(+0.66063)
     | > avg_loss_disc:[92m 1.86599 [0m(-0.24461)
     | > avg_loss_1:[92m 1.86599 [0m(-0.24461)


[4m[1m > EPOCH: 14/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 17:38:49) [0m

[1m   --> STEP: 7/161 -- GLOBAL_STEP: 2275[0m
     | > loss_gen: 3.12501  (3.11034)
     | > loss_kl: 1.48671  (1.55049)
     | > loss_feat: 4.94856  (5.59545)
     | > loss_mel: 26.77096  (28.67009)
     | > loss_duration: 1.88729  (1.82633)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 38.21854  (40.75269)
     | > grad_norm_0: 107.86454  (154.49803)
     | > loss_disc: 1.98165  (1.83996)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.98165  (1.83996)
     | > grad_norm_1: 11.09283  (15.38587)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.02310  (2.07876)
     | > loader_time: 0.03450  (0.07024)


[1m   --> STEP: 32/161 -- GLOBAL_STEP: 2300[0m
     | > loss_gen: 3.61864  (3.07596)
     | > loss_kl: 1.51047  (1.50519)
     | > loss_feat: 8.10804  (5.73587)
     | > loss_mel: 29.43539  (28.31674)
     | > loss_duration: 1.91193  (1.82192)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 44.58447  (40.45567)
     | > grad_norm_0: 126.99393  (112.65361)
     | > loss_disc: 1.44016  (1.92301)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.44016  (1.92301)
     | > grad_norm_1: 8.04045  (15.68323)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.50770  (2.09622)
     | > loader_time: 0.03990  (0.04306)


[1m   --> STEP: 57/161 -- GLOBAL_STEP: 2325[0m
     | > loss_gen: 3.37872  (3.03071)
     | > loss_kl: 1.66968  (1.51973)
     | > loss_feat: 6.86705  (5.66601)
     | > loss_mel: 28.33511  (28.42958)
     | > loss_duration: 1.84209  (1.80730)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 42.09266  (40.45333)
     | > grad_norm_0: 86.82355  (108.36494)
     | > loss_disc: 1.50611  (2.00978)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.50611  (2.00978)
     | > grad_norm_1: 12.17080  (15.90821)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.14830  (2.08662)
     | > loader_time: 0.03540  (0.04049)


[1m   --> STEP: 82/161 -- GLOBAL_STEP: 2350[0m
     | > loss_gen: 3.00014  (3.04173)
     | > loss_kl: 1.49362  (1.48877)
     | > loss_feat: 5.74528  (5.68054)
     | > loss_mel: 27.96198  (28.57724)
     | > loss_duration: 1.75180  (1.80835)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.95282  (40.59663)
     | > grad_norm_0: 90.94051  (107.84978)
     | > loss_disc: 2.03370  (2.01933)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.03370  (2.01933)
     | > grad_norm_1: 10.89599  (15.06224)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.19180  (2.07083)
     | > loader_time: 0.02700  (0.04336)


[1m   --> STEP: 107/161 -- GLOBAL_STEP: 2375[0m
     | > loss_gen: 2.41420  (3.03879)
     | > loss_kl: 1.56725  (1.48993)
     | > loss_feat: 4.14620  (5.68179)
     | > loss_mel: 29.70966  (28.55454)
     | > loss_duration: 1.82032  (1.81289)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.65763  (40.57795)
     | > grad_norm_0: 85.68052  (107.91080)
     | > loss_disc: 2.32769  (2.01309)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.32769  (2.01309)
     | > grad_norm_1: 15.50820  (15.25456)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.40280  (2.07412)
     | > loader_time: 0.03980  (0.04448)


[1m   --> STEP: 132/161 -- GLOBAL_STEP: 2400[0m
     | > loss_gen: 2.69971  (3.01299)
     | > loss_kl: 1.56771  (1.48442)
     | > loss_feat: 5.91281  (5.62190)
     | > loss_mel: 29.15951  (28.52445)
     | > loss_duration: 1.84509  (1.81529)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 41.18483  (40.45906)
     | > grad_norm_0: 124.45194  (105.73845)
     | > loss_disc: 1.94055  (2.01584)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.94055  (2.01584)
     | > grad_norm_1: 11.96701  (14.80230)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.64000  (2.11975)
     | > loader_time: 0.03800  (0.04960)


[1m   --> STEP: 157/161 -- GLOBAL_STEP: 2425[0m
     | > loss_gen: 2.67359  (3.00383)
     | > loss_kl: 1.47880  (1.48369)
     | > loss_feat: 6.40147  (5.59678)
     | > loss_mel: 28.25228  (28.47145)
     | > loss_duration: 1.84345  (1.81249)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 40.64959  (40.36825)
     | > grad_norm_0: 70.34373  (103.05045)
     | > loss_disc: 1.73059  (2.00260)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.73059  (2.00260)
     | > grad_norm_1: 35.88588  (14.00668)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.09610  (2.11922)
     | > loader_time: 0.03630  (0.04821)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02736 [0m(-0.00051)
     | > avg_loss_gen:[91m 2.93745 [0m(+0.32345)
     | > avg_loss_kl:[91m 1.56387 [0m(+0.12677)
     | > avg_loss_feat:[92m 4.98199 [0m(-0.68037)
     | > avg_loss_mel:[92m 25.00997 [0m(-1.79070)
     | > avg_loss_duration:[92m 1.76443 [0m(-0.03191)
     | > avg_loss_0:[92m 36.25772 [0m(-2.05275)
     | > avg_loss_disc:[91m 2.10660 [0m(+0.24061)
     | > avg_loss_1:[91m 2.10660 [0m(+0.24061)

 > BEST MODEL : /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000/best_model_2430.pth.tar

[4m[1m > EPOCH: 15/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 17:44:58) [0m

[1m   --> STEP: 20/161 -- GLOBAL_STEP: 2450[0m
     | > loss_gen: 2.88983  (3.08695)
     | > loss_kl: 1.48726  (1.50633)
     | > loss_feat: 6.39842  (5.80063)
     | > loss_mel: 28.98042  (27.82600)
     | > loss_duration: 1.76323  (1.79910)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 41.51915  (40.01900)
     | > grad_norm_0: 56.04733  (96.17200)
     | > loss_disc: 1.65220  (1.81477)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.65220  (1.81477)
     | > grad_norm_1: 8.06256  (8.94522)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.99750  (2.07253)
     | > loader_time: 0.03190  (0.05477)


[1m   --> STEP: 45/161 -- GLOBAL_STEP: 2475[0m
     | > loss_gen: 3.20760  (3.08511)
     | > loss_kl: 1.48097  (1.51155)
     | > loss_feat: 5.76736  (5.83167)
     | > loss_mel: 27.70889  (27.95787)
     | > loss_duration: 1.77545  (1.79650)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.94028  (40.18269)
     | > grad_norm_0: 75.46926  (87.32949)
     | > loss_disc: 1.76229  (1.81186)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.76229  (1.81186)
     | > grad_norm_1: 6.30989  (8.48764)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.72630  (2.09113)
     | > loader_time: 0.02550  (0.04622)


[1m   --> STEP: 70/161 -- GLOBAL_STEP: 2500[0m
     | > loss_gen: 2.43478  (3.07475)
     | > loss_kl: 1.26617  (1.50107)
     | > loss_feat: 5.75721  (5.83549)
     | > loss_mel: 28.15994  (27.87969)
     | > loss_duration: 1.85973  (1.79338)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.47782  (40.08438)
     | > grad_norm_0: 133.74115  (87.55264)
     | > loss_disc: 2.06169  (1.80602)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.06169  (1.80602)
     | > grad_norm_1: 15.21043  (9.35275)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.68720  (2.05890)
     | > loader_time: 0.02430  (0.04825)


[1m   --> STEP: 95/161 -- GLOBAL_STEP: 2525[0m
     | > loss_gen: 2.69758  (3.10171)
     | > loss_kl: 1.54575  (1.48458)
     | > loss_feat: 5.53755  (5.87714)
     | > loss_mel: 26.98186  (27.77465)
     | > loss_duration: 1.77919  (1.79520)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 38.54194  (40.03328)
     | > grad_norm_0: 78.21717  (86.51508)
     | > loss_disc: 1.95729  (1.80642)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.95729  (1.80642)
     | > grad_norm_1: 6.43835  (10.03744)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.15230  (2.07282)
     | > loader_time: 0.03690  (0.04866)


[1m   --> STEP: 120/161 -- GLOBAL_STEP: 2550[0m
     | > loss_gen: 3.15204  (3.09029)
     | > loss_kl: 1.39177  (1.48698)
     | > loss_feat: 5.91918  (5.85510)
     | > loss_mel: 27.85079  (27.67265)
     | > loss_duration: 1.80404  (1.79632)
     | > amp_scaler: 1024.00000  (533.33333)
     | > loss_0: 40.11782  (39.90134)
     | > grad_norm_0: 40.30890  (87.10184)
     | > loss_disc: 1.87337  (1.81265)
     | > amp_scaler-1: 1024.00000  (533.33333)
     | > loss_1: 1.87337  (1.81265)
     | > grad_norm_1: 7.07941  (10.04027)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.48780  (2.09720)
     | > loader_time: 0.04610  (0.04848)


[1m   --> STEP: 145/161 -- GLOBAL_STEP: 2575[0m
     | > loss_gen: 2.95557  (3.08085)
     | > loss_kl: 1.52531  (1.48576)
     | > loss_feat: 6.19546  (5.83761)
     | > loss_mel: 26.95286  (27.62829)
     | > loss_duration: 1.75542  (1.79410)
     | > amp_scaler: 512.00000  (568.49655)
     | > loss_0: 39.38462  (39.82661)
     | > grad_norm_0: 43.32316  (84.14335)
     | > loss_disc: 1.74405  (1.81856)
     | > amp_scaler-1: 512.00000  (568.49655)
     | > loss_1: 1.74405  (1.81856)
     | > grad_norm_1: 5.90818  (9.77221)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.70420  (2.11590)
     | > loader_time: 0.04120  (0.05217)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02638 [0m(-0.00097)
     | > avg_loss_gen:[91m 2.96940 [0m(+0.03195)
     | > avg_loss_kl:[91m 1.64979 [0m(+0.08592)
     | > avg_loss_feat:[91m 5.66130 [0m(+0.67931)
     | > avg_loss_mel:[91m 26.91665 [0m(+1.90668)
     | > avg_loss_duration:[91m 1.76451 [0m(+0.00009)
     | > avg_loss_0:[91m 38.96166 [0m(+2.70394)
     | > avg_loss_disc:[92m 1.85079 [0m(-0.25581)
     | > avg_loss_1:[92m 1.85079 [0m(-0.25581)


[4m[1m > EPOCH: 16/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 17:51:05) [0m

[1m   --> STEP: 8/161 -- GLOBAL_STEP: 2600[0m
     | > loss_gen: 2.97726  (3.15496)
     | > loss_kl: 1.46032  (1.45790)
     | > loss_feat: 5.68834  (5.92574)
     | > loss_mel: 27.58856  (27.56723)
     | > loss_duration: 1.82632  (1.81229)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.54080  (39.91812)
     | > grad_norm_0: 98.52464  (73.75443)
     | > loss_disc: 1.79206  (1.79016)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.79206  (1.79016)
     | > grad_norm_1: 9.00471  (8.51515)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.84880  (2.06625)
     | > loader_time: 0.03130  (0.04477)


[1m   --> STEP: 33/161 -- GLOBAL_STEP: 2625[0m
     | > loss_gen: 2.85402  (3.09399)
     | > loss_kl: 1.43982  (1.45879)
     | > loss_feat: 5.17285  (5.87060)
     | > loss_mel: 26.79535  (27.57632)
     | > loss_duration: 1.74716  (1.78762)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 38.00920  (39.78732)
     | > grad_norm_0: 67.80023  (75.26595)
     | > loss_disc: 1.96094  (1.82809)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.96094  (1.82809)
     | > grad_norm_1: 6.64350  (9.07138)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.01510  (2.08429)
     | > loader_time: 0.10920  (0.05425)


[1m   --> STEP: 58/161 -- GLOBAL_STEP: 2650[0m
     | > loss_gen: 3.31194  (3.08631)
     | > loss_kl: 1.36338  (1.44185)
     | > loss_feat: 5.35287  (5.83628)
     | > loss_mel: 27.64452  (27.41627)
     | > loss_duration: 1.72411  (1.78903)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.39682  (39.56973)
     | > grad_norm_0: 73.17439  (77.98832)
     | > loss_disc: 1.87815  (1.83838)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.87815  (1.83838)
     | > grad_norm_1: 19.19740  (9.87443)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.12310  (2.08995)
     | > loader_time: 0.03290  (0.05051)


[1m   --> STEP: 83/161 -- GLOBAL_STEP: 2675[0m
     | > loss_gen: 3.18496  (3.09575)
     | > loss_kl: 1.47081  (1.46607)
     | > loss_feat: 6.34662  (5.87822)
     | > loss_mel: 27.59739  (27.54107)
     | > loss_duration: 1.82446  (1.78106)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 40.42424  (39.76217)
     | > grad_norm_0: 104.72891  (81.52883)
     | > loss_disc: 1.68768  (1.82937)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.68768  (1.82937)
     | > grad_norm_1: 7.54905  (9.75384)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.32250  (2.06850)
     | > loader_time: 0.10700  (0.04986)


[1m   --> STEP: 108/161 -- GLOBAL_STEP: 2700[0m
     | > loss_gen: 2.86567  (3.07918)
     | > loss_kl: 1.59178  (1.46429)
     | > loss_feat: 5.27983  (5.81376)
     | > loss_mel: 27.27259  (27.59418)
     | > loss_duration: 1.71965  (1.78164)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 38.72952  (39.73305)
     | > grad_norm_0: 143.92410  (84.26761)
     | > loss_disc: 1.93049  (1.84818)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.93049  (1.84818)
     | > grad_norm_1: 10.25531  (10.38302)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.28340  (2.07463)
     | > loader_time: 0.04070  (0.05075)


[1m   --> STEP: 133/161 -- GLOBAL_STEP: 2725[0m
     | > loss_gen: 2.85161  (3.06984)
     | > loss_kl: 1.48308  (1.46249)
     | > loss_feat: 6.01688  (5.78760)
     | > loss_mel: 27.42253  (27.55893)
     | > loss_duration: 1.78876  (1.78144)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.56286  (39.66030)
     | > grad_norm_0: 88.77262  (84.11589)
     | > loss_disc: 1.81311  (1.85268)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.81311  (1.85268)
     | > grad_norm_1: 10.61683  (11.27218)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.88030  (2.12650)
     | > loader_time: 0.03010  (0.05147)


[1m   --> STEP: 158/161 -- GLOBAL_STEP: 2750[0m
     | > loss_gen: 3.31858  (3.06689)
     | > loss_kl: 1.63523  (1.47160)
     | > loss_feat: 6.25721  (5.77912)
     | > loss_mel: 26.21346  (27.53057)
     | > loss_duration: 1.76056  (1.78065)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.18503  (39.62882)
     | > grad_norm_0: 86.04551  (84.32819)
     | > loss_disc: 1.59089  (1.85058)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.59089  (1.85058)
     | > grad_norm_1: 8.78063  (10.91706)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.33860  (2.12752)
     | > loader_time: 0.03800  (0.05028)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02689 [0m(+0.00050)
     | > avg_loss_gen:[91m 3.07245 [0m(+0.10305)
     | > avg_loss_kl:[92m 1.53927 [0m(-0.11052)
     | > avg_loss_feat:[91m 5.76163 [0m(+0.10033)
     | > avg_loss_mel:[92m 24.85037 [0m(-2.06628)
     | > avg_loss_duration:[92m 1.75946 [0m(-0.00506)
     | > avg_loss_0:[92m 36.98318 [0m(-1.97848)
     | > avg_loss_disc:[91m 1.89992 [0m(+0.04913)
     | > avg_loss_1:[91m 1.89992 [0m(+0.04913)


[4m[1m > EPOCH: 17/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 17:57:10) [0m

[1m   --> STEP: 21/161 -- GLOBAL_STEP: 2775[0m
     | > loss_gen: 3.13465  (3.04502)
     | > loss_kl: 1.45676  (1.45743)
     | > loss_feat: 6.09573  (5.74303)
     | > loss_mel: 27.26153  (27.73085)
     | > loss_duration: 1.86481  (1.77466)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.81349  (39.75099)
     | > grad_norm_0: 56.02514  (108.47681)
     | > loss_disc: 1.79569  (1.88772)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.79569  (1.88772)
     | > grad_norm_1: 11.72253  (11.48863)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.90040  (2.07110)
     | > loader_time: 0.02990  (0.04753)


[1m   --> STEP: 46/161 -- GLOBAL_STEP: 2800[0m
     | > loss_gen: 3.20963  (3.08426)
     | > loss_kl: 1.50861  (1.43329)
     | > loss_feat: 5.47047  (5.90323)
     | > loss_mel: 28.25498  (27.68034)
     | > loss_duration: 1.71842  (1.76929)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 40.16210  (39.87040)
     | > grad_norm_0: 89.79501  (104.81534)
     | > loss_disc: 2.72622  (1.87709)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.72622  (1.87709)
     | > grad_norm_1: 35.99207  (13.03794)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.65420  (2.10100)
     | > loader_time: 0.14300  (0.04684)


[1m   --> STEP: 71/161 -- GLOBAL_STEP: 2825[0m
     | > loss_gen: 2.83393  (3.06864)
     | > loss_kl: 1.40699  (1.43243)
     | > loss_feat: 5.33075  (5.85748)
     | > loss_mel: 27.50634  (27.71073)
     | > loss_duration: 1.82830  (1.77138)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 38.90630  (39.84066)
     | > grad_norm_0: 120.90494  (105.69084)
     | > loss_disc: 1.92930  (1.92525)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.92930  (1.92525)
     | > grad_norm_1: 10.26747  (13.58067)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.39080  (2.06616)
     | > loader_time: 0.03590  (0.04688)


[1m   --> STEP: 96/161 -- GLOBAL_STEP: 2850[0m
     | > loss_gen: 3.60153  (3.03894)
     | > loss_kl: 1.43338  (1.44439)
     | > loss_feat: 5.74902  (5.81093)
     | > loss_mel: 25.65201  (27.78661)
     | > loss_duration: 1.78338  (1.77541)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 38.21933  (39.85628)
     | > grad_norm_0: 92.68688  (100.44287)
     | > loss_disc: 1.71836  (1.98999)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.71836  (1.98999)
     | > grad_norm_1: 15.78852  (13.78566)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.59540  (2.07742)
     | > loader_time: 0.24360  (0.05168)


[1m   --> STEP: 121/161 -- GLOBAL_STEP: 2875[0m
     | > loss_gen: 2.81883  (3.01745)
     | > loss_kl: 1.53567  (1.44768)
     | > loss_feat: 5.16357  (5.74870)
     | > loss_mel: 28.55422  (27.79670)
     | > loss_duration: 1.85242  (1.77829)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.92471  (39.78883)
     | > grad_norm_0: 88.33037  (98.07432)
     | > loss_disc: 1.92361  (2.00376)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.92361  (2.00376)
     | > grad_norm_1: 7.23953  (12.99291)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.92060  (2.10492)
     | > loader_time: 0.04630  (0.05274)


[1m   --> STEP: 146/161 -- GLOBAL_STEP: 2900[0m
     | > loss_gen: 2.79966  (3.00982)
     | > loss_kl: 1.29960  (1.44204)
     | > loss_feat: 6.05892  (5.71623)
     | > loss_mel: 26.49628  (27.63990)
     | > loss_duration: 1.84132  (1.77620)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 38.49579  (39.58420)
     | > grad_norm_0: 41.01124  (95.62818)
     | > loss_disc: 1.81029  (1.98769)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.81029  (1.98769)
     | > grad_norm_1: 12.41381  (12.31527)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.68330  (2.12669)
     | > loader_time: 0.05760  (0.05549)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02776 [0m(+0.00087)
     | > avg_loss_gen:[91m 3.17174 [0m(+0.09929)
     | > avg_loss_kl:[91m 1.68877 [0m(+0.14950)
     | > avg_loss_feat:[91m 5.86294 [0m(+0.10131)
     | > avg_loss_mel:[91m 25.67767 [0m(+0.82730)
     | > avg_loss_duration:[91m 1.76027 [0m(+0.00081)
     | > avg_loss_0:[91m 38.16139 [0m(+1.17821)
     | > avg_loss_disc:[92m 1.77601 [0m(-0.12390)
     | > avg_loss_1:[92m 1.77601 [0m(-0.12390)


[4m[1m > EPOCH: 18/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 18:03:18) [0m

[1m   --> STEP: 9/161 -- GLOBAL_STEP: 2925[0m
     | > loss_gen: 3.12043  (3.00288)
     | > loss_kl: 1.53834  (1.49091)
     | > loss_feat: 5.80313  (5.79968)
     | > loss_mel: 25.13389  (27.38544)
     | > loss_duration: 1.75415  (1.75365)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 37.34995  (39.43257)
     | > grad_norm_0: 40.35618  (99.42101)
     | > loss_disc: 1.81861  (1.81788)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.81861  (1.81788)
     | > grad_norm_1: 7.32208  (12.09643)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.68070  (2.02716)
     | > loader_time: 0.02670  (0.04140)


[1m   --> STEP: 34/161 -- GLOBAL_STEP: 2950[0m
     | > loss_gen: 2.80203  (3.06504)
     | > loss_kl: 1.46151  (1.48276)
     | > loss_feat: 4.71211  (5.82068)
     | > loss_mel: 26.25901  (27.28376)
     | > loss_duration: 1.73785  (1.76459)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 36.97250  (39.41682)
     | > grad_norm_0: 208.42110  (96.42690)
     | > loss_disc: 2.15705  (1.81668)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.15705  (1.81668)
     | > grad_norm_1: 13.33147  (11.78200)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.24070  (2.12915)
     | > loader_time: 0.11890  (0.05092)


[1m   --> STEP: 59/161 -- GLOBAL_STEP: 2975[0m
     | > loss_gen: 3.05000  (3.08249)
     | > loss_kl: 1.46630  (1.49580)
     | > loss_feat: 6.20631  (5.89466)
     | > loss_mel: 26.97946  (27.34606)
     | > loss_duration: 1.75970  (1.76218)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.46176  (39.58119)
     | > grad_norm_0: 78.24308  (90.38694)
     | > loss_disc: 1.68722  (1.80556)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.68722  (1.80556)
     | > grad_norm_1: 6.51900  (10.42015)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.83910  (2.11987)
     | > loader_time: 0.03020  (0.04494)


[1m   --> STEP: 84/161 -- GLOBAL_STEP: 3000[0m
     | > loss_gen: 3.22425  (3.09456)
     | > loss_kl: 1.42085  (1.49210)
     | > loss_feat: 5.58837  (5.92508)
     | > loss_mel: 26.02458  (27.31994)
     | > loss_duration: 1.77031  (1.76129)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 38.02836  (39.59297)
     | > grad_norm_0: 37.40685  (89.39302)
     | > loss_disc: 1.94169  (1.81484)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.94169  (1.81484)
     | > grad_norm_1: 6.89705  (10.46449)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.95370  (2.11035)
     | > loader_time: 0.03760  (0.05102)


[1m   --> STEP: 109/161 -- GLOBAL_STEP: 3025[0m
     | > loss_gen: 3.75369  (3.11770)
     | > loss_kl: 1.43144  (1.49053)
     | > loss_feat: 5.58600  (5.99275)
     | > loss_mel: 28.70834  (27.42967)
     | > loss_duration: 1.75170  (1.76489)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 41.23117  (39.79554)
     | > grad_norm_0: 255.60323  (97.15696)
     | > loss_disc: 1.85485  (1.80393)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.85485  (1.80393)
     | > grad_norm_1: 32.17875  (12.07495)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.67630  (2.13291)
     | > loader_time: 0.04860  (0.05053)


[1m   --> STEP: 134/161 -- GLOBAL_STEP: 3050[0m
     | > loss_gen: 2.98284  (3.08818)
     | > loss_kl: 1.35872  (1.47727)
     | > loss_feat: 5.50807  (5.88728)
     | > loss_mel: 25.91009  (27.45382)
     | > loss_duration: 1.76315  (1.76748)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 37.52286  (39.67403)
     | > grad_norm_0: 107.89704  (94.12306)
     | > loss_disc: 1.86769  (1.86314)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.86769  (1.86314)
     | > grad_norm_1: 6.15862  (12.40140)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.58200  (2.16264)
     | > loader_time: 0.02700  (0.05283)


[1m   --> STEP: 159/161 -- GLOBAL_STEP: 3075[0m
     | > loss_gen: 3.24138  (3.07219)
     | > loss_kl: 1.30861  (1.47475)
     | > loss_feat: 6.10147  (5.83561)
     | > loss_mel: 27.51714  (27.34445)
     | > loss_duration: 1.70304  (1.76650)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.87165  (39.49350)
     | > grad_norm_0: 118.14488  (92.96286)
     | > loss_disc: 1.61134  (1.87053)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.61134  (1.87053)
     | > grad_norm_1: 7.24005  (11.79471)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.87800  (2.16174)
     | > loader_time: 0.03530  (0.05389)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02710 [0m(-0.00066)
     | > avg_loss_gen:[91m 3.19398 [0m(+0.02224)
     | > avg_loss_kl:[92m 1.53701 [0m(-0.15176)
     | > avg_loss_feat:[91m 6.40748 [0m(+0.54454)
     | > avg_loss_mel:[91m 26.97880 [0m(+1.30113)
     | > avg_loss_duration:[92m 1.74117 [0m(-0.01910)
     | > avg_loss_0:[91m 39.85843 [0m(+1.69704)
     | > avg_loss_disc:[92m 1.71180 [0m(-0.06421)
     | > avg_loss_1:[92m 1.71180 [0m(-0.06421)


[4m[1m > EPOCH: 19/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 18:09:35) [0m

[1m   --> STEP: 22/161 -- GLOBAL_STEP: 3100[0m
     | > loss_gen: 2.82942  (3.15845)
     | > loss_kl: 1.52326  (1.47633)
     | > loss_feat: 6.22866  (6.18300)
     | > loss_mel: 26.27522  (27.19110)
     | > loss_duration: 1.73006  (1.76532)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 38.58661  (39.77420)
     | > grad_norm_0: 53.96810  (79.15486)
     | > loss_disc: 1.79400  (1.75592)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.79400  (1.75592)
     | > grad_norm_1: 9.51265  (9.46166)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.98110  (2.07535)
     | > loader_time: 0.03300  (0.04658)


[1m   --> STEP: 47/161 -- GLOBAL_STEP: 3125[0m
     | > loss_gen: 3.33414  (3.09183)
     | > loss_kl: 1.48966  (1.46847)
     | > loss_feat: 6.62021  (5.93729)
     | > loss_mel: 27.21217  (27.04399)
     | > loss_duration: 1.68516  (1.76290)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 40.34134  (39.30450)
     | > grad_norm_0: 124.47305  (81.21177)
     | > loss_disc: 1.60653  (1.81392)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.60653  (1.81392)
     | > grad_norm_1: 7.88355  (10.47086)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.91990  (2.10612)
     | > loader_time: 0.03160  (0.04361)


[1m   --> STEP: 72/161 -- GLOBAL_STEP: 3150[0m
     | > loss_gen: 2.46879  (3.08797)
     | > loss_kl: 1.34521  (1.48258)
     | > loss_feat: 4.69210  (5.91564)
     | > loss_mel: 25.87715  (27.07224)
     | > loss_duration: 1.77594  (1.76257)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 36.15919  (39.32099)
     | > grad_norm_0: 103.58744  (85.94663)
     | > loss_disc: 2.24668  (1.82292)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.24668  (1.82292)
     | > grad_norm_1: 17.28579  (11.06843)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.09390  (2.07381)
     | > loader_time: 0.03300  (0.04767)


[1m   --> STEP: 97/161 -- GLOBAL_STEP: 3175[0m
     | > loss_gen: 3.22148  (3.08118)
     | > loss_kl: 1.44347  (1.47215)
     | > loss_feat: 5.52129  (5.88697)
     | > loss_mel: 26.96788  (27.13049)
     | > loss_duration: 1.78957  (1.76552)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 38.94368  (39.33631)
     | > grad_norm_0: 213.72920  (85.67532)
     | > loss_disc: 1.99857  (1.84306)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.99857  (1.84306)
     | > grad_norm_1: 6.61008  (10.91583)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.37940  (2.10096)
     | > loader_time: 0.04030  (0.04642)


[1m   --> STEP: 122/161 -- GLOBAL_STEP: 3200[0m
     | > loss_gen: 2.96572  (3.07058)
     | > loss_kl: 1.29178  (1.47033)
     | > loss_feat: 5.82845  (5.84551)
     | > loss_mel: 27.55310  (27.12396)
     | > loss_duration: 1.72546  (1.76673)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.36451  (39.27712)
     | > grad_norm_0: 132.97778  (87.59866)
     | > loss_disc: 1.81767  (1.84874)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.81767  (1.84874)
     | > grad_norm_1: 6.09239  (11.33787)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.59770  (2.13806)
     | > loader_time: 0.04900  (0.04798)


[1m   --> STEP: 147/161 -- GLOBAL_STEP: 3225[0m
     | > loss_gen: 2.77592  (3.08023)
     | > loss_kl: 1.48772  (1.46558)
     | > loss_feat: 5.40197  (5.87905)
     | > loss_mel: 28.73410  (27.10425)
     | > loss_duration: 1.74263  (1.76636)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 40.14234  (39.29548)
     | > grad_norm_0: 198.29648  (86.92460)
     | > loss_disc: 2.01810  (1.84330)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.01810  (1.84330)
     | > grad_norm_1: 30.86034  (11.25500)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.37250  (2.15052)
     | > loader_time: 0.04240  (0.04929)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02581 [0m(-0.00129)
     | > avg_loss_gen:[91m 3.53657 [0m(+0.34259)
     | > avg_loss_kl:[91m 1.63905 [0m(+0.10205)
     | > avg_loss_feat:[91m 6.70664 [0m(+0.29916)
     | > avg_loss_mel:[91m 27.37720 [0m(+0.39841)
     | > avg_loss_duration:[92m 1.73376 [0m(-0.00741)
     | > avg_loss_0:[91m 40.99322 [0m(+1.13480)
     | > avg_loss_disc:[92m 1.62958 [0m(-0.08222)
     | > avg_loss_1:[92m 1.62958 [0m(-0.08222)


[4m[1m > EPOCH: 20/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 18:15:43) [0m

[1m   --> STEP: 10/161 -- GLOBAL_STEP: 3250[0m
     | > loss_gen: 2.47160  (2.99401)
     | > loss_kl: 1.48514  (1.44382)
     | > loss_feat: 4.77246  (5.65008)
     | > loss_mel: 25.08236  (26.80799)
     | > loss_duration: 1.77641  (1.75944)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 35.58797  (38.65533)
     | > grad_norm_0: 38.68641  (70.61449)
     | > loss_disc: 2.05157  (1.90254)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.05157  (1.90254)
     | > grad_norm_1: 7.96846  (11.71204)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.17940  (2.10737)
     | > loader_time: 0.03350  (0.04354)


[1m   --> STEP: 35/161 -- GLOBAL_STEP: 3275[0m
     | > loss_gen: 3.51131  (3.03419)
     | > loss_kl: 1.48532  (1.47963)
     | > loss_feat: 5.56488  (5.74492)
     | > loss_mel: 25.69302  (26.95183)
     | > loss_duration: 1.77328  (1.76481)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 38.02782  (38.97538)
     | > grad_norm_0: 72.58685  (83.20584)
     | > loss_disc: 1.94168  (1.88270)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.94168  (1.88270)
     | > grad_norm_1: 13.66396  (11.41806)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.64730  (2.14933)
     | > loader_time: 0.04350  (0.04007)


[1m   --> STEP: 60/161 -- GLOBAL_STEP: 3300[0m
     | > loss_gen: 2.50613  (3.01138)
     | > loss_kl: 1.18110  (1.46680)
     | > loss_feat: 6.15080  (5.68681)
     | > loss_mel: 27.36321  (26.93931)
     | > loss_duration: 1.81264  (1.75817)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.01388  (38.86247)
     | > grad_norm_0: 82.54203  (85.66034)
     | > loss_disc: 1.87202  (1.91123)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.87202  (1.91123)
     | > grad_norm_1: 13.67475  (12.77384)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.94070  (2.11308)
     | > loader_time: 0.12270  (0.04153)


[1m   --> STEP: 85/161 -- GLOBAL_STEP: 3325[0m
     | > loss_gen: 2.95590  (3.00602)
     | > loss_kl: 1.50039  (1.46363)
     | > loss_feat: 5.60956  (5.67600)
     | > loss_mel: 26.62438  (26.89280)
     | > loss_duration: 1.80511  (1.76008)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 38.49535  (38.79853)
     | > grad_norm_0: 52.53399  (82.24754)
     | > loss_disc: 1.95141  (1.90723)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.95141  (1.90723)
     | > grad_norm_1: 12.46780  (11.80264)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.39310  (2.09979)
     | > loader_time: 0.11040  (0.04843)


[1m   --> STEP: 110/161 -- GLOBAL_STEP: 3350[0m
     | > loss_gen: 2.88161  (3.02296)
     | > loss_kl: 1.38040  (1.45872)
     | > loss_feat: 5.41037  (5.75886)
     | > loss_mel: 27.68640  (26.92422)
     | > loss_duration: 1.74415  (1.76156)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.10292  (38.92633)
     | > grad_norm_0: 47.10191  (86.73309)
     | > loss_disc: 2.01093  (1.90037)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.01093  (1.90037)
     | > grad_norm_1: 11.05626  (11.91725)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.91240  (2.11266)
     | > loader_time: 0.03520  (0.05001)


[1m   --> STEP: 135/161 -- GLOBAL_STEP: 3375[0m
     | > loss_gen: 2.98699  (3.01887)
     | > loss_kl: 1.28541  (1.45698)
     | > loss_feat: 5.41490  (5.74151)
     | > loss_mel: 26.16899  (26.95169)
     | > loss_duration: 1.68761  (1.76099)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 37.54389  (38.93003)
     | > grad_norm_0: 91.12957  (86.25397)
     | > loss_disc: 1.87382  (1.91639)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.87382  (1.91639)
     | > grad_norm_1: 7.22271  (11.79535)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.34360  (2.14809)
     | > loader_time: 0.02960  (0.05380)


[1m   --> STEP: 160/161 -- GLOBAL_STEP: 3400[0m
     | > loss_gen: 2.82739  (3.00649)
     | > loss_kl: 1.47937  (1.45550)
     | > loss_feat: 6.57019  (5.71261)
     | > loss_mel: 27.19615  (26.90825)
     | > loss_duration: 1.74250  (1.76165)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.81560  (38.84450)
     | > grad_norm_0: 100.89647  (84.93719)
     | > loss_disc: 1.66537  (1.91757)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.66537  (1.91757)
     | > grad_norm_1: 9.80042  (11.40241)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.11740  (2.14475)
     | > loader_time: 0.03800  (0.05141)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02816 [0m(+0.00235)
     | > avg_loss_gen:[92m 3.16636 [0m(-0.37021)
     | > avg_loss_kl:[91m 1.69381 [0m(+0.05476)
     | > avg_loss_feat:[92m 5.13111 [0m(-1.57553)
     | > avg_loss_mel:[92m 25.34079 [0m(-2.03641)
     | > avg_loss_duration:[91m 1.73394 [0m(+0.00018)
     | > avg_loss_0:[92m 37.06601 [0m(-3.92721)
     | > avg_loss_disc:[91m 1.94429 [0m(+0.31471)
     | > avg_loss_1:[91m 1.94429 [0m(+0.31471)


[4m[1m > EPOCH: 21/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 18:21:53) [0m

[1m   --> STEP: 23/161 -- GLOBAL_STEP: 3425[0m
     | > loss_gen: 2.89902  (3.01911)
     | > loss_kl: 1.38402  (1.47234)
     | > loss_feat: 6.02789  (5.80835)
     | > loss_mel: 28.20916  (26.85491)
     | > loss_duration: 1.70651  (1.76609)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 40.22659  (38.92081)
     | > grad_norm_0: 61.57446  (81.13503)
     | > loss_disc: 1.82867  (1.88852)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.82867  (1.88852)
     | > grad_norm_1: 8.66281  (12.28868)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.19470  (2.05851)
     | > loader_time: 0.11410  (0.04583)


[1m   --> STEP: 48/161 -- GLOBAL_STEP: 3450[0m
     | > loss_gen: 3.49348  (3.03017)
     | > loss_kl: 1.57038  (1.46974)
     | > loss_feat: 6.24686  (5.77413)
     | > loss_mel: 26.21880  (26.86449)
     | > loss_duration: 1.79071  (1.75864)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.32022  (38.89717)
     | > grad_norm_0: 66.01916  (79.37916)
     | > loss_disc: 1.82334  (1.88340)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.82334  (1.88340)
     | > grad_norm_1: 10.92057  (13.31637)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.77590  (2.11257)
     | > loader_time: 0.03040  (0.05139)


[1m   --> STEP: 73/161 -- GLOBAL_STEP: 3475[0m
     | > loss_gen: 3.07107  (3.02110)
     | > loss_kl: 1.56869  (1.46497)
     | > loss_feat: 6.51104  (5.75905)
     | > loss_mel: 26.62775  (26.92615)
     | > loss_duration: 1.75701  (1.74993)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.53556  (38.92120)
     | > grad_norm_0: 58.32318  (76.44646)
     | > loss_disc: 1.77756  (1.88732)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.77756  (1.88732)
     | > grad_norm_1: 6.38646  (12.90843)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.22100  (2.09468)
     | > loader_time: 0.03730  (0.05076)


[1m   --> STEP: 98/161 -- GLOBAL_STEP: 3500[0m
     | > loss_gen: 3.00749  (3.01611)
     | > loss_kl: 1.38166  (1.44220)
     | > loss_feat: 5.73214  (5.74729)
     | > loss_mel: 26.55622  (26.91221)
     | > loss_duration: 1.88518  (1.75627)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 38.56268  (38.87409)
     | > grad_norm_0: 87.84986  (79.96246)
     | > loss_disc: 1.75592  (1.88957)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.75592  (1.88957)
     | > grad_norm_1: 7.90435  (12.36663)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.74390  (2.11309)
     | > loader_time: 0.03390  (0.05150)


[1m   --> STEP: 123/161 -- GLOBAL_STEP: 3525[0m
     | > loss_gen: 3.06974  (3.01551)
     | > loss_kl: 1.44002  (1.43765)
     | > loss_feat: 6.07314  (5.74773)
     | > loss_mel: 26.85845  (26.89866)
     | > loss_duration: 1.83497  (1.75953)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.27633  (38.85907)
     | > grad_norm_0: 52.05130  (81.22917)
     | > loss_disc: 1.73754  (1.88981)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.73754  (1.88981)
     | > grad_norm_1: 7.90527  (11.80655)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.95260  (2.14423)
     | > loader_time: 0.05320  (0.05254)


[1m   --> STEP: 148/161 -- GLOBAL_STEP: 3550[0m
     | > loss_gen: 2.68811  (3.01257)
     | > loss_kl: 1.58140  (1.44991)
     | > loss_feat: 6.41030  (5.73808)
     | > loss_mel: 28.97375  (26.88514)
     | > loss_duration: 1.81312  (1.75878)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 41.46667  (38.84448)
     | > grad_norm_0: 51.74697  (80.51938)
     | > loss_disc: 1.77703  (1.89355)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.77703  (1.89355)
     | > grad_norm_1: 10.94709  (11.58023)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.29780  (2.15480)
     | > loader_time: 0.04030  (0.05238)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02848 [0m(+0.00032)
     | > avg_loss_gen:[92m 3.09070 [0m(-0.07566)
     | > avg_loss_kl:[92m 1.38101 [0m(-0.31280)
     | > avg_loss_feat:[92m 4.84394 [0m(-0.28717)
     | > avg_loss_mel:[91m 27.30321 [0m(+1.96241)
     | > avg_loss_duration:[92m 1.72727 [0m(-0.00667)
     | > avg_loss_0:[91m 38.34613 [0m(+1.28012)
     | > avg_loss_disc:[91m 1.94487 [0m(+0.00058)
     | > avg_loss_1:[91m 1.94487 [0m(+0.00058)


[4m[1m > EPOCH: 22/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 18:28:04) [0m

[1m   --> STEP: 11/161 -- GLOBAL_STEP: 3575[0m
     | > loss_gen: 3.03208  (2.90091)
     | > loss_kl: 1.34386  (1.44857)
     | > loss_feat: 5.87473  (5.51990)
     | > loss_mel: 26.39364  (26.54314)
     | > loss_duration: 1.70977  (1.77461)
     | > amp_scaler: 512.00000  (605.09091)
     | > loss_0: 38.35408  (38.18713)
     | > grad_norm_0: 112.35519  (68.16870)
     | > loss_disc: 1.88041  (1.95668)
     | > amp_scaler-1: 512.00000  (605.09091)
     | > loss_1: 1.88041  (1.95668)
     | > grad_norm_1: 5.94126  (8.32286)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.64160  (1.94843)
     | > loader_time: 0.12090  (0.05738)


[1m   --> STEP: 36/161 -- GLOBAL_STEP: 3600[0m
     | > loss_gen: 3.00811  (2.98374)
     | > loss_kl: 1.39839  (1.46904)
     | > loss_feat: 5.35783  (5.64363)
     | > loss_mel: 26.09053  (26.73725)
     | > loss_duration: 1.81981  (1.77041)
     | > amp_scaler: 512.00000  (540.44444)
     | > loss_0: 37.67467  (38.60407)
     | > grad_norm_0: 83.95741  (77.97003)
     | > loss_disc: 1.98113  (1.91848)
     | > amp_scaler-1: 512.00000  (540.44444)
     | > loss_1: 1.98113  (1.91848)
     | > grad_norm_1: 10.69370  (10.23952)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.21210  (2.10626)
     | > loader_time: 0.10050  (0.05497)


[1m   --> STEP: 61/161 -- GLOBAL_STEP: 3625[0m
     | > loss_gen: 2.67917  (2.95906)
     | > loss_kl: 1.38130  (1.48130)
     | > loss_feat: 4.99074  (5.58664)
     | > loss_mel: 25.94099  (26.69201)
     | > loss_duration: 1.76724  (1.76202)
     | > amp_scaler: 512.00000  (528.78689)
     | > loss_0: 36.75945  (38.48104)
     | > grad_norm_0: 102.34399  (77.81201)
     | > loss_disc: 1.94978  (1.94088)
     | > amp_scaler-1: 512.00000  (528.78689)
     | > loss_1: 1.94978  (1.94088)
     | > grad_norm_1: 13.37236  (10.89497)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.29990  (2.09920)
     | > loader_time: 0.17100  (0.05018)


[1m   --> STEP: 86/161 -- GLOBAL_STEP: 3650[0m
     | > loss_gen: 2.67883  (2.95146)
     | > loss_kl: 1.50920  (1.48719)
     | > loss_feat: 5.42915  (5.56766)
     | > loss_mel: 26.64015  (26.63979)
     | > loss_duration: 1.68791  (1.75690)
     | > amp_scaler: 512.00000  (523.90698)
     | > loss_0: 37.94524  (38.40300)
     | > grad_norm_0: 50.57636  (77.54939)
     | > loss_disc: 1.99215  (1.94461)
     | > amp_scaler-1: 512.00000  (523.90698)
     | > loss_1: 1.99215  (1.94461)
     | > grad_norm_1: 9.68645  (12.66674)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.65060  (2.08853)
     | > loader_time: 0.02710  (0.05702)


[1m   --> STEP: 111/161 -- GLOBAL_STEP: 3675[0m
     | > loss_gen: 2.69210  (2.93654)
     | > loss_kl: 1.32874  (1.48717)
     | > loss_feat: 5.05839  (5.53984)
     | > loss_mel: 25.20559  (26.82775)
     | > loss_duration: 1.76552  (1.75717)
     | > amp_scaler: 512.00000  (521.22523)
     | > loss_0: 36.05034  (38.54846)
     | > grad_norm_0: 48.10888  (77.56574)
     | > loss_disc: 1.99045  (1.95710)
     | > amp_scaler-1: 512.00000  (521.22523)
     | > loss_1: 1.99045  (1.95710)
     | > grad_norm_1: 10.12853  (11.90128)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.51330  (2.10948)
     | > loader_time: 0.04420  (0.05566)


[1m   --> STEP: 136/161 -- GLOBAL_STEP: 3700[0m
     | > loss_gen: 3.06691  (2.91945)
     | > loss_kl: 1.48842  (1.48969)
     | > loss_feat: 5.76969  (5.50255)
     | > loss_mel: 25.68495  (26.79435)
     | > loss_duration: 1.72438  (1.75872)
     | > amp_scaler: 512.00000  (519.52941)
     | > loss_0: 37.73434  (38.46474)
     | > grad_norm_0: 66.26776  (77.73704)
     | > loss_disc: 1.79543  (1.96190)
     | > amp_scaler-1: 512.00000  (519.52941)
     | > loss_1: 1.79543  (1.96190)
     | > grad_norm_1: 10.62203  (11.47757)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.17670  (2.14918)
     | > loader_time: 0.04380  (0.05704)


[1m   --> STEP: 161/161 -- GLOBAL_STEP: 3725[0m
     | > loss_gen: 2.75929  (2.92275)
     | > loss_kl: 1.82939  (1.48782)
     | > loss_feat: 5.24011  (5.51627)
     | > loss_mel: 27.08384  (26.78974)
     | > loss_duration: 1.75098  (1.75690)
     | > amp_scaler: 512.00000  (518.36025)
     | > loss_0: 38.66361  (38.47348)
     | > grad_norm_0: 102.54808  (79.69780)
     | > loss_disc: 2.01960  (1.96690)
     | > amp_scaler-1: 512.00000  (518.36025)
     | > loss_1: 2.01960  (1.96690)
     | > grad_norm_1: 8.70566  (11.45037)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.26170  (2.14252)
     | > loader_time: 0.02570  (0.05505)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.03062 [0m(+0.00214)
     | > avg_loss_gen:[91m 3.25185 [0m(+0.16115)
     | > avg_loss_kl:[92m 1.37592 [0m(-0.00509)
     | > avg_loss_feat:[91m 6.13175 [0m(+1.28781)
     | > avg_loss_mel:[92m 26.43229 [0m(-0.87091)
     | > avg_loss_duration:[92m 1.72493 [0m(-0.00234)
     | > avg_loss_0:[91m 38.91675 [0m(+0.57062)
     | > avg_loss_disc:[92m 1.75876 [0m(-0.18610)
     | > avg_loss_1:[92m 1.75876 [0m(-0.18610)


[4m[1m > EPOCH: 23/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 18:34:14) [0m

[1m   --> STEP: 24/161 -- GLOBAL_STEP: 3750[0m
     | > loss_gen: 3.14083  (2.89618)
     | > loss_kl: 1.45468  (1.47665)
     | > loss_feat: 5.67324  (5.33698)
     | > loss_mel: 27.14068  (26.46264)
     | > loss_duration: 1.74493  (1.74896)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.15437  (37.92141)
     | > grad_norm_0: 58.31380  (85.89981)
     | > loss_disc: 1.86238  (2.00460)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.86238  (2.00460)
     | > grad_norm_1: 10.37582  (18.48913)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.80280  (2.06717)
     | > loader_time: 0.02910  (0.04005)


[1m   --> STEP: 49/161 -- GLOBAL_STEP: 3775[0m
     | > loss_gen: 2.55380  (2.85338)
     | > loss_kl: 1.37005  (1.48591)
     | > loss_feat: 5.44363  (5.29235)
     | > loss_mel: 28.02371  (26.48991)
     | > loss_duration: 1.67203  (1.74411)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.06322  (37.86565)
     | > grad_norm_0: 43.82956  (88.78134)
     | > loss_disc: 1.93016  (2.01671)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.93016  (2.01671)
     | > grad_norm_1: 16.60516  (15.90510)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.35510  (2.11147)
     | > loader_time: 0.03880  (0.04049)


[1m   --> STEP: 74/161 -- GLOBAL_STEP: 3800[0m
     | > loss_gen: 3.15938  (2.85874)
     | > loss_kl: 1.63339  (1.49771)
     | > loss_feat: 6.10893  (5.29639)
     | > loss_mel: 27.69243  (26.73521)
     | > loss_duration: 1.66659  (1.74552)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 40.26071  (38.13357)
     | > grad_norm_0: 77.03743  (86.69899)
     | > loss_disc: 1.76034  (2.01514)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.76034  (2.01514)
     | > grad_norm_1: 6.92408  (14.91468)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.79170  (2.07735)
     | > loader_time: 0.03060  (0.04440)


[1m   --> STEP: 99/161 -- GLOBAL_STEP: 3825[0m
     | > loss_gen: 2.66395  (2.87264)
     | > loss_kl: 1.26897  (1.48133)
     | > loss_feat: 5.29327  (5.34297)
     | > loss_mel: 27.76715  (26.83004)
     | > loss_duration: 1.77392  (1.74773)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 38.76725  (38.27472)
     | > grad_norm_0: 74.67811  (87.66861)
     | > loss_disc: 2.01274  (2.00843)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.01274  (2.00843)
     | > grad_norm_1: 20.46093  (14.81515)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.74690  (2.08931)
     | > loader_time: 0.03010  (0.04823)


[1m   --> STEP: 124/161 -- GLOBAL_STEP: 3850[0m
     | > loss_gen: 2.80679  (2.87575)
     | > loss_kl: 1.39929  (1.46502)
     | > loss_feat: 5.66618  (5.37329)
     | > loss_mel: 25.80736  (26.84231)
     | > loss_duration: 1.70178  (1.74649)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 37.38140  (38.30286)
     | > grad_norm_0: 53.93176  (87.96284)
     | > loss_disc: 2.03414  (2.00043)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.03414  (2.00043)
     | > grad_norm_1: 8.35850  (13.77347)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.16590  (2.13285)
     | > loader_time: 0.03770  (0.05161)


[1m   --> STEP: 149/161 -- GLOBAL_STEP: 3875[0m
     | > loss_gen: 2.80027  (2.87746)
     | > loss_kl: 1.22159  (1.46587)
     | > loss_feat: 5.31194  (5.38317)
     | > loss_mel: 26.44537  (26.82421)
     | > loss_duration: 1.68437  (1.74839)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 37.46355  (38.29910)
     | > grad_norm_0: 62.35918  (87.93053)
     | > loss_disc: 2.01114  (2.00379)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.01114  (2.00379)
     | > grad_norm_1: 11.34222  (13.38081)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.05880  (2.14125)
     | > loader_time: 0.03410  (0.05109)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02758 [0m(-0.00304)
     | > avg_loss_gen:[92m 2.96552 [0m(-0.28633)
     | > avg_loss_kl:[91m 1.50811 [0m(+0.13218)
     | > avg_loss_feat:[92m 6.11929 [0m(-0.01247)
     | > avg_loss_mel:[92m 26.35728 [0m(-0.07501)
     | > avg_loss_duration:[91m 1.73056 [0m(+0.00562)
     | > avg_loss_0:[92m 38.68076 [0m(-0.23600)
     | > avg_loss_disc:[91m 1.85199 [0m(+0.09323)
     | > avg_loss_1:[91m 1.85199 [0m(+0.09323)


[4m[1m > EPOCH: 24/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 18:40:22) [0m

[1m   --> STEP: 12/161 -- GLOBAL_STEP: 3900[0m
     | > loss_gen: 3.00424  (2.88956)
     | > loss_kl: 1.47364  (1.43189)
     | > loss_feat: 4.96130  (5.27363)
     | > loss_mel: 27.06566  (26.76465)
     | > loss_duration: 1.75514  (1.74782)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.25998  (38.10755)
     | > grad_norm_0: 144.05748  (93.85222)
     | > loss_disc: 2.08146  (2.02125)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.08146  (2.02125)
     | > grad_norm_1: 23.53195  (17.77017)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.88170  (2.03335)
     | > loader_time: 0.02630  (0.06595)


[1m   --> STEP: 37/161 -- GLOBAL_STEP: 3925[0m
     | > loss_gen: 2.90381  (2.90584)
     | > loss_kl: 1.41738  (1.46074)
     | > loss_feat: 5.58331  (5.48781)
     | > loss_mel: 26.13771  (26.76543)
     | > loss_duration: 1.73669  (1.74499)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.77890  (38.36481)
     | > grad_norm_0: 110.33282  (89.67645)
     | > loss_disc: 2.02024  (1.98298)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.02024  (1.98298)
     | > grad_norm_1: 19.47650  (11.92594)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.73230  (2.12325)
     | > loader_time: 0.02590  (0.05022)


[1m   --> STEP: 62/161 -- GLOBAL_STEP: 3950[0m
     | > loss_gen: 2.73079  (2.91141)
     | > loss_kl: 1.46754  (1.44962)
     | > loss_feat: 4.48404  (5.51052)
     | > loss_mel: 25.07581  (26.74725)
     | > loss_duration: 1.81805  (1.74565)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.57623  (38.36445)
     | > grad_norm_0: 103.04053  (95.41792)
     | > loss_disc: 2.03551  (1.97861)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.03551  (1.97861)
     | > grad_norm_1: 18.53479  (10.81787)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.80380  (2.08937)
     | > loader_time: 0.02790  (0.05220)


[1m   --> STEP: 87/161 -- GLOBAL_STEP: 3975[0m
     | > loss_gen: 2.79058  (2.89838)
     | > loss_kl: 1.53544  (1.44842)
     | > loss_feat: 5.27494  (5.47056)
     | > loss_mel: 24.25120  (26.61601)
     | > loss_duration: 1.76523  (1.74461)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.61739  (38.17798)
     | > grad_norm_0: 121.76686  (96.99935)
     | > loss_disc: 2.16831  (1.98362)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.16831  (1.98362)
     | > grad_norm_1: 26.13942  (12.26402)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.38190  (2.09989)
     | > loader_time: 0.04300  (0.05962)


[1m   --> STEP: 112/161 -- GLOBAL_STEP: 4000[0m
     | > loss_gen: 2.74635  (2.90215)
     | > loss_kl: 1.51636  (1.45079)
     | > loss_feat: 5.55472  (5.49441)
     | > loss_mel: 27.29383  (26.66864)
     | > loss_duration: 1.71830  (1.74516)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.82956  (38.26115)
     | > grad_norm_0: 81.17763  (95.40636)
     | > loss_disc: 1.96551  (1.98225)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.96551  (1.98225)
     | > grad_norm_1: 14.48446  (12.05057)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.21160  (2.11822)
     | > loader_time: 0.08980  (0.05565)


[1m   --> STEP: 137/161 -- GLOBAL_STEP: 4025[0m
     | > loss_gen: 3.27208  (2.90643)
     | > loss_kl: 1.78058  (1.45778)
     | > loss_feat: 5.27289  (5.50708)
     | > loss_mel: 25.03059  (26.74256)
     | > loss_duration: 1.74268  (1.74738)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.09882  (38.36122)
     | > grad_norm_0: 59.91167  (94.72112)
     | > loss_disc: 2.04683  (1.98135)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.04683  (1.98135)
     | > grad_norm_1: 19.99831  (11.99256)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.38640  (2.14995)
     | > loader_time: 0.03780  (0.05490)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02783 [0m(+0.00025)
     | > avg_loss_gen:[91m 2.96800 [0m(+0.00248)
     | > avg_loss_kl:[92m 1.42791 [0m(-0.08019)
     | > avg_loss_feat:[92m 5.49503 [0m(-0.62426)
     | > avg_loss_mel:[92m 25.76274 [0m(-0.59455)
     | > avg_loss_duration:[92m 1.71519 [0m(-0.01537)
     | > avg_loss_0:[92m 37.36886 [0m(-1.31189)
     | > avg_loss_disc:[91m 1.96989 [0m(+0.11790)
     | > avg_loss_1:[91m 1.96989 [0m(+0.11790)


[4m[1m > EPOCH: 25/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 18:46:31) [0m

[1m   --> STEP: 0/161 -- GLOBAL_STEP: 4050[0m
     | > loss_gen: 3.25151  (3.25151)
     | > loss_kl: 1.54636  (1.54636)
     | > loss_feat: 6.08394  (6.08394)
     | > loss_mel: 26.18941  (26.18941)
     | > loss_duration: 1.76921  (1.76921)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.84043  (38.84043)
     | > grad_norm_0: 43.36081  (43.36081)
     | > loss_disc: 1.87011  (1.87011)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.87011  (1.87011)
     | > grad_norm_1: 5.04010  (5.04010)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 3.44650  (3.44645)
     | > loader_time: 3.38860  (3.38863)


[1m   --> STEP: 25/161 -- GLOBAL_STEP: 4075[0m
     | > loss_gen: 2.83149  (2.91662)
     | > loss_kl: 1.44630  (1.47601)
     | > loss_feat: 5.19596  (5.49099)
     | > loss_mel: 25.79456  (26.20180)
     | > loss_duration: 1.74212  (1.75098)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.01044  (37.83641)
     | > grad_norm_0: 120.25930  (69.40056)
     | > loss_disc: 2.09920  (1.97368)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.09920  (1.97368)
     | > grad_norm_1: 8.66363  (12.67182)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.23730  (2.07783)
     | > loader_time: 0.05340  (0.06306)


[1m   --> STEP: 50/161 -- GLOBAL_STEP: 4100[0m
     | > loss_gen: 2.80221  (2.89618)
     | > loss_kl: 1.54743  (1.48626)
     | > loss_feat: 5.30644  (5.43149)
     | > loss_mel: 24.44779  (26.06417)
     | > loss_duration: 1.70625  (1.74831)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.81011  (37.62641)
     | > grad_norm_0: 69.29692  (82.64986)
     | > loss_disc: 2.06655  (1.98458)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06655  (1.98458)
     | > grad_norm_1: 8.44780  (12.65322)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.07640  (2.10014)
     | > loader_time: 0.03560  (0.05289)


[1m   --> STEP: 75/161 -- GLOBAL_STEP: 4125[0m
     | > loss_gen: 2.85805  (2.88006)
     | > loss_kl: 1.46462  (1.47360)
     | > loss_feat: 5.43937  (5.42695)
     | > loss_mel: 26.25035  (26.13336)
     | > loss_duration: 1.77702  (1.74296)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.78941  (37.65693)
     | > grad_norm_0: 79.80323  (82.99924)
     | > loss_disc: 1.94189  (1.98632)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.94189  (1.98632)
     | > grad_norm_1: 19.10547  (13.54489)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.77020  (2.08548)
     | > loader_time: 0.04700  (0.05027)


[1m   --> STEP: 100/161 -- GLOBAL_STEP: 4150[0m
     | > loss_gen: 3.01720  (2.86833)
     | > loss_kl: 1.58285  (1.47848)
     | > loss_feat: 6.08736  (5.40942)
     | > loss_mel: 26.89497  (26.31809)
     | > loss_duration: 1.78349  (1.74679)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 39.36587  (37.82111)
     | > grad_norm_0: 81.08008  (88.17970)
     | > loss_disc: 1.87563  (1.99238)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.87563  (1.99238)
     | > grad_norm_1: 10.43682  (13.38218)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.41130  (2.09065)
     | > loader_time: 0.03930  (0.04749)


[1m   --> STEP: 125/161 -- GLOBAL_STEP: 4175[0m
     | > loss_gen: 2.76996  (2.86890)
     | > loss_kl: 1.37155  (1.46566)
     | > loss_feat: 5.45844  (5.40793)
     | > loss_mel: 25.98650  (26.36281)
     | > loss_duration: 1.76053  (1.74517)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.34698  (37.85048)
     | > grad_norm_0: 78.51529  (84.27283)
     | > loss_disc: 1.99442  (1.99847)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.99442  (1.99847)
     | > grad_norm_1: 11.12332  (13.19608)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.26430  (2.13468)
     | > loader_time: 0.03740  (0.04707)


[1m   --> STEP: 150/161 -- GLOBAL_STEP: 4200[0m
     | > loss_gen: 2.76679  (2.87019)
     | > loss_kl: 1.35032  (1.46331)
     | > loss_feat: 5.36679  (5.40569)
     | > loss_mel: 25.68967  (26.37176)
     | > loss_duration: 1.73195  (1.74564)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.90552  (37.85658)
     | > grad_norm_0: 83.34755  (85.00523)
     | > loss_disc: 2.07143  (2.00248)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.07143  (2.00248)
     | > grad_norm_1: 6.54837  (12.68950)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.34470  (2.14121)
     | > loader_time: 0.03710  (0.04680)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02811 [0m(+0.00028)
     | > avg_loss_gen:[92m 2.90347 [0m(-0.06453)
     | > avg_loss_kl:[91m 1.47287 [0m(+0.04495)
     | > avg_loss_feat:[92m 5.17098 [0m(-0.32404)
     | > avg_loss_mel:[92m 25.53852 [0m(-0.22422)
     | > avg_loss_duration:[91m 1.72286 [0m(+0.00767)
     | > avg_loss_0:[92m 36.80869 [0m(-0.56017)
     | > avg_loss_disc:[91m 1.98394 [0m(+0.01405)
     | > avg_loss_1:[91m 1.98394 [0m(+0.01405)


[4m[1m > EPOCH: 26/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 18:52:43) [0m

[1m   --> STEP: 13/161 -- GLOBAL_STEP: 4225[0m
     | > loss_gen: 3.06260  (2.88427)
     | > loss_kl: 1.16884  (1.42109)
     | > loss_feat: 5.61214  (5.45685)
     | > loss_mel: 27.74151  (26.67985)
     | > loss_duration: 1.74306  (1.74207)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 39.32814  (38.18413)
     | > grad_norm_0: 136.94730  (102.38679)
     | > loss_disc: 1.90857  (2.04074)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.90857  (2.04074)
     | > grad_norm_1: 12.41734  (12.15500)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.47610  (2.07097)
     | > loader_time: 0.05480  (0.05386)


[1m   --> STEP: 38/161 -- GLOBAL_STEP: 4250[0m
     | > loss_gen: 3.42438  (2.92517)
     | > loss_kl: 1.44597  (1.44158)
     | > loss_feat: 5.55028  (5.57274)
     | > loss_mel: 26.90552  (26.56793)
     | > loss_duration: 1.67744  (1.74244)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 39.00359  (38.24986)
     | > grad_norm_0: 150.74261  (94.51851)
     | > loss_disc: 1.84907  (1.99245)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.84907  (1.99245)
     | > grad_norm_1: 9.60808  (11.26321)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.50130  (2.14458)
     | > loader_time: 0.03770  (0.05159)


[1m   --> STEP: 63/161 -- GLOBAL_STEP: 4275[0m
     | > loss_gen: 2.87087  (2.89709)
     | > loss_kl: 1.30733  (1.47535)
     | > loss_feat: 6.11505  (5.50855)
     | > loss_mel: 27.32392  (26.46171)
     | > loss_duration: 1.71359  (1.74243)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 39.33076  (38.08514)
     | > grad_norm_0: 102.50822  (91.51593)
     | > loss_disc: 1.76521  (2.00409)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.76521  (2.00409)
     | > grad_norm_1: 5.63916  (11.89805)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.98550  (2.10664)
     | > loader_time: 0.08370  (0.04878)


[1m   --> STEP: 88/161 -- GLOBAL_STEP: 4300[0m
     | > loss_gen: 2.70904  (2.89124)
     | > loss_kl: 1.37976  (1.46560)
     | > loss_feat: 5.11557  (5.46809)
     | > loss_mel: 27.23542  (26.38402)
     | > loss_duration: 1.70848  (1.74233)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.14827  (37.95128)
     | > grad_norm_0: 149.86487  (89.63412)
     | > loss_disc: 2.04696  (2.00561)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.04696  (2.00561)
     | > grad_norm_1: 9.04755  (13.08261)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.24520  (2.09704)
     | > loader_time: 0.03970  (0.05041)


[1m   --> STEP: 113/161 -- GLOBAL_STEP: 4325[0m
     | > loss_gen: 2.32854  (2.87347)
     | > loss_kl: 1.74640  (1.47368)
     | > loss_feat: 5.66060  (5.44225)
     | > loss_mel: 28.06959  (26.37366)
     | > loss_duration: 1.76986  (1.74349)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 39.57498  (37.90655)
     | > grad_norm_0: 159.58772  (89.89690)
     | > loss_disc: 2.21095  (2.01055)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.21095  (2.01055)
     | > grad_norm_1: 11.94019  (13.26550)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.96310  (2.11405)
     | > loader_time: 0.04070  (0.05204)


[1m   --> STEP: 138/161 -- GLOBAL_STEP: 4350[0m
     | > loss_gen: 3.07596  (2.88272)
     | > loss_kl: 1.58513  (1.47910)
     | > loss_feat: 5.69096  (5.44414)
     | > loss_mel: 27.26806  (26.40544)
     | > loss_duration: 1.74353  (1.74440)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 39.36365  (37.95580)
     | > grad_norm_0: 109.33919  (91.79386)
     | > loss_disc: 1.79895  (2.00591)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.79895  (2.00591)
     | > grad_norm_1: 8.34609  (13.24455)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.68600  (2.14414)
     | > loader_time: 0.03840  (0.05384)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02822 [0m(+0.00012)
     | > avg_loss_gen:[91m 3.13010 [0m(+0.22663)
     | > avg_loss_kl:[91m 1.53165 [0m(+0.05879)
     | > avg_loss_feat:[92m 4.93297 [0m(-0.23802)
     | > avg_loss_mel:[92m 24.95856 [0m(-0.57996)
     | > avg_loss_duration:[91m 1.74100 [0m(+0.01815)
     | > avg_loss_0:[92m 36.29428 [0m(-0.51441)
     | > avg_loss_disc:[91m 2.18365 [0m(+0.19970)
     | > avg_loss_1:[91m 2.18365 [0m(+0.19970)


[4m[1m > EPOCH: 27/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 18:58:55) [0m

[1m   --> STEP: 1/161 -- GLOBAL_STEP: 4375[0m
     | > loss_gen: 2.59340  (2.59340)
     | > loss_kl: 1.60257  (1.60257)
     | > loss_feat: 5.65237  (5.65237)
     | > loss_mel: 25.48979  (25.48979)
     | > loss_duration: 1.76904  (1.76904)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.10717  (37.10717)
     | > grad_norm_0: 74.66216  (74.66216)
     | > loss_disc: 2.02857  (2.02857)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.02857  (2.02857)
     | > grad_norm_1: 9.43557  (9.43557)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.00760  (2.00756)
     | > loader_time: 0.12370  (0.12372)


[1m   --> STEP: 26/161 -- GLOBAL_STEP: 4400[0m
     | > loss_gen: 3.09544  (2.87359)
     | > loss_kl: 1.42705  (1.48063)
     | > loss_feat: 5.92973  (5.48974)
     | > loss_mel: 27.14072  (26.41937)
     | > loss_duration: 1.84500  (1.74597)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 39.43793  (38.00930)
     | > grad_norm_0: 105.42548  (89.10193)
     | > loss_disc: 1.83600  (1.98841)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.83600  (1.98841)
     | > grad_norm_1: 5.24192  (10.87321)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.51180  (2.10059)
     | > loader_time: 0.04810  (0.04465)


[1m   --> STEP: 51/161 -- GLOBAL_STEP: 4425[0m
     | > loss_gen: 3.15416  (2.87400)
     | > loss_kl: 1.50191  (1.49070)
     | > loss_feat: 5.92205  (5.51058)
     | > loss_mel: 26.35073  (26.47012)
     | > loss_duration: 1.73059  (1.73573)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.65944  (38.08113)
     | > grad_norm_0: 72.73870  (90.42681)
     | > loss_disc: 1.93229  (2.00171)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.93229  (2.00171)
     | > grad_norm_1: 11.54132  (11.35731)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.13650  (2.11541)
     | > loader_time: 0.03650  (0.04172)


[1m   --> STEP: 76/161 -- GLOBAL_STEP: 4450[0m
     | > loss_gen: 3.35386  (2.85684)
     | > loss_kl: 1.54062  (1.47157)
     | > loss_feat: 5.48381  (5.44692)
     | > loss_mel: 26.58986  (26.52043)
     | > loss_duration: 1.70963  (1.73079)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.67777  (38.02655)
     | > grad_norm_0: 123.65946  (87.21786)
     | > loss_disc: 2.03413  (2.01731)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.03413  (2.01731)
     | > grad_norm_1: 9.22081  (14.37252)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.27490  (2.09619)
     | > loader_time: 0.07060  (0.04745)


[1m   --> STEP: 101/161 -- GLOBAL_STEP: 4475[0m
     | > loss_gen: 2.94107  (2.86790)
     | > loss_kl: 1.24755  (1.46808)
     | > loss_feat: 5.88558  (5.50043)
     | > loss_mel: 25.15801  (26.44106)
     | > loss_duration: 1.70317  (1.73272)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.93538  (38.01020)
     | > grad_norm_0: 70.51822  (85.86509)
     | > loss_disc: 1.90115  (2.00243)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.90115  (2.00243)
     | > grad_norm_1: 7.16731  (13.22222)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.04950  (2.10499)
     | > loader_time: 0.03230  (0.05030)


[1m   --> STEP: 126/161 -- GLOBAL_STEP: 4500[0m
     | > loss_gen: 2.88360  (2.86887)
     | > loss_kl: 1.47630  (1.47309)
     | > loss_feat: 5.06441  (5.50597)
     | > loss_mel: 26.61338  (26.45468)
     | > loss_duration: 1.77143  (1.73690)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.80912  (38.03951)
     | > grad_norm_0: 106.72588  (88.92771)
     | > loss_disc: 2.07927  (1.99671)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.07927  (1.99671)
     | > grad_norm_1: 18.08020  (12.65225)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.93610  (2.14564)
     | > loader_time: 0.15190  (0.05272)


[1m   --> STEP: 151/161 -- GLOBAL_STEP: 4525[0m
     | > loss_gen: 3.14940  (2.87410)
     | > loss_kl: 1.48848  (1.47104)
     | > loss_feat: 5.54114  (5.50901)
     | > loss_mel: 26.08377  (26.44372)
     | > loss_duration: 1.65475  (1.73407)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.91754  (38.03194)
     | > grad_norm_0: 78.84936  (88.30910)
     | > loss_disc: 2.06579  (1.99797)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06579  (1.99797)
     | > grad_norm_1: 8.64212  (12.14943)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.98660  (2.16144)
     | > loader_time: 0.04390  (0.05131)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02957 [0m(+0.00135)
     | > avg_loss_gen:[92m 2.93442 [0m(-0.19568)
     | > avg_loss_kl:[92m 1.29301 [0m(-0.23864)
     | > avg_loss_feat:[91m 5.65547 [0m(+0.72251)
     | > avg_loss_mel:[91m 25.70514 [0m(+0.74658)
     | > avg_loss_duration:[92m 1.72058 [0m(-0.02043)
     | > avg_loss_0:[91m 37.30862 [0m(+1.01434)
     | > avg_loss_disc:[92m 1.93720 [0m(-0.24645)
     | > avg_loss_1:[92m 1.93720 [0m(-0.24645)


[4m[1m > EPOCH: 28/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 19:05:03) [0m

[1m   --> STEP: 14/161 -- GLOBAL_STEP: 4550[0m
     | > loss_gen: 2.82423  (2.74909)
     | > loss_kl: 1.26435  (1.45619)
     | > loss_feat: 5.79520  (5.26698)
     | > loss_mel: 26.30484  (25.96487)
     | > loss_duration: 1.69115  (1.73321)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.87977  (37.17033)
     | > grad_norm_0: 94.55862  (83.86230)
     | > loss_disc: 2.07343  (2.06962)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.07343  (2.06962)
     | > grad_norm_1: 9.19145  (12.87812)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.08350  (2.09272)
     | > loader_time: 0.03560  (0.05964)


[1m   --> STEP: 39/161 -- GLOBAL_STEP: 4575[0m
     | > loss_gen: 2.96278  (2.81081)
     | > loss_kl: 1.33356  (1.52350)
     | > loss_feat: 5.07896  (5.24147)
     | > loss_mel: 27.13490  (26.24643)
     | > loss_duration: 1.72025  (1.73682)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.23045  (37.55902)
     | > grad_norm_0: 149.75357  (91.59296)
     | > loss_disc: 2.01027  (2.05691)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.01027  (2.05691)
     | > grad_norm_1: 6.76147  (14.67243)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.04890  (2.11660)
     | > loader_time: 0.03500  (0.04506)


[1m   --> STEP: 64/161 -- GLOBAL_STEP: 4600[0m
     | > loss_gen: 3.01773  (2.82158)
     | > loss_kl: 1.42785  (1.50854)
     | > loss_feat: 5.91812  (5.31525)
     | > loss_mel: 27.61120  (26.32013)
     | > loss_duration: 1.76265  (1.73732)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 39.73754  (37.70281)
     | > grad_norm_0: 64.75645  (97.09505)
     | > loss_disc: 1.82283  (2.04401)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.82283  (2.04401)
     | > grad_norm_1: 12.40654  (15.85128)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.05290  (2.08882)
     | > loader_time: 0.03510  (0.04747)


[1m   --> STEP: 89/161 -- GLOBAL_STEP: 4625[0m
     | > loss_gen: 3.02081  (2.83370)
     | > loss_kl: 1.53550  (1.50587)
     | > loss_feat: 5.62187  (5.34487)
     | > loss_mel: 25.54542  (26.32367)
     | > loss_duration: 1.74011  (1.73618)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.46371  (37.74429)
     | > grad_norm_0: 90.83865  (92.84167)
     | > loss_disc: 1.98982  (2.03437)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.98982  (2.03437)
     | > grad_norm_1: 5.89633  (14.09267)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.60670  (2.07656)
     | > loader_time: 0.02530  (0.05505)


[1m   --> STEP: 114/161 -- GLOBAL_STEP: 4650[0m
     | > loss_gen: 2.79561  (2.83739)
     | > loss_kl: 1.56539  (1.51088)
     | > loss_feat: 5.35079  (5.36439)
     | > loss_mel: 27.04731  (26.29591)
     | > loss_duration: 1.74738  (1.73876)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.50648  (37.74732)
     | > grad_norm_0: 100.22795  (94.39240)
     | > loss_disc: 1.88120  (2.02618)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.88120  (2.02618)
     | > grad_norm_1: 7.08135  (13.94730)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.03210  (2.09967)
     | > loader_time: 0.05270  (0.05595)


[1m   --> STEP: 139/161 -- GLOBAL_STEP: 4675[0m
     | > loss_gen: 2.90376  (2.84083)
     | > loss_kl: 1.36029  (1.50558)
     | > loss_feat: 5.08592  (5.37647)
     | > loss_mel: 26.37068  (26.24433)
     | > loss_duration: 1.76729  (1.73860)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.48794  (37.70581)
     | > grad_norm_0: 56.85844  (92.83295)
     | > loss_disc: 2.24960  (2.02897)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.24960  (2.02897)
     | > grad_norm_1: 7.91926  (13.35144)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.98550  (2.13324)
     | > loader_time: 0.03140  (0.05805)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02842 [0m(-0.00115)
     | > avg_loss_gen:[91m 3.28702 [0m(+0.35261)
     | > avg_loss_kl:[91m 1.61648 [0m(+0.32348)
     | > avg_loss_feat:[92m 5.36127 [0m(-0.29420)
     | > avg_loss_mel:[91m 26.37707 [0m(+0.67193)
     | > avg_loss_duration:[91m 1.72637 [0m(+0.00579)
     | > avg_loss_0:[91m 38.36822 [0m(+1.05960)
     | > avg_loss_disc:[91m 1.98518 [0m(+0.04798)
     | > avg_loss_1:[91m 1.98518 [0m(+0.04798)


[4m[1m > EPOCH: 29/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 19:11:12) [0m

[1m   --> STEP: 2/161 -- GLOBAL_STEP: 4700[0m
     | > loss_gen: 2.60034  (2.83833)
     | > loss_kl: 1.48349  (1.45560)
     | > loss_feat: 5.40313  (5.48955)
     | > loss_mel: 26.88559  (26.76716)
     | > loss_duration: 1.73160  (1.76070)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.10415  (38.31135)
     | > grad_norm_0: 112.72435  (117.00574)
     | > loss_disc: 2.04277  (1.94441)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.04277  (1.94441)
     | > grad_norm_1: 8.41868  (8.20109)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.57450  (1.84507)
     | > loader_time: 0.02150  (0.10036)


[1m   --> STEP: 27/161 -- GLOBAL_STEP: 4725[0m
     | > loss_gen: 3.04008  (2.89689)
     | > loss_kl: 1.44787  (1.44682)
     | > loss_feat: 5.29678  (5.57717)
     | > loss_mel: 26.90431  (26.48336)
     | > loss_duration: 1.68245  (1.75038)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.37148  (38.15462)
     | > grad_norm_0: 116.88990  (85.26923)
     | > loss_disc: 2.06177  (2.00616)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06177  (2.00616)
     | > grad_norm_1: 7.48268  (11.90853)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.84390  (2.06843)
     | > loader_time: 0.03590  (0.04733)


[1m   --> STEP: 52/161 -- GLOBAL_STEP: 4750[0m
     | > loss_gen: 2.69906  (2.86323)
     | > loss_kl: 1.50103  (1.46008)
     | > loss_feat: 4.36494  (5.43071)
     | > loss_mel: 26.51615  (26.37637)
     | > loss_duration: 1.73317  (1.73910)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.81433  (37.86949)
     | > grad_norm_0: 86.90271  (84.62353)
     | > loss_disc: 2.20147  (2.03893)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.20147  (2.03893)
     | > grad_norm_1: 13.30907  (11.50698)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.65000  (2.08416)
     | > loader_time: 0.02700  (0.04718)


[1m   --> STEP: 77/161 -- GLOBAL_STEP: 4775[0m
     | > loss_gen: 2.77483  (2.85178)
     | > loss_kl: 1.43734  (1.46908)
     | > loss_feat: 5.26949  (5.44137)
     | > loss_mel: 25.08638  (26.40295)
     | > loss_duration: 1.79802  (1.73937)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.36607  (37.90456)
     | > grad_norm_0: 70.70888  (90.12301)
     | > loss_disc: 2.00752  (2.03103)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.00752  (2.03103)
     | > grad_norm_1: 18.43399  (12.81541)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.02080  (2.07689)
     | > loader_time: 0.03720  (0.04504)


[1m   --> STEP: 102/161 -- GLOBAL_STEP: 4800[0m
     | > loss_gen: 2.78226  (2.86176)
     | > loss_kl: 1.67916  (1.48260)
     | > loss_feat: 5.05482  (5.46233)
     | > loss_mel: 26.59379  (26.42852)
     | > loss_duration: 1.63068  (1.73508)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.74072  (37.97029)
     | > grad_norm_0: 115.91301  (93.46708)
     | > loss_disc: 2.17605  (2.03035)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.17605  (2.03035)
     | > grad_norm_1: 19.94481  (13.14159)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.68230  (2.08494)
     | > loader_time: 0.02410  (0.04414)


[1m   --> STEP: 127/161 -- GLOBAL_STEP: 4825[0m
     | > loss_gen: 2.70884  (2.85798)
     | > loss_kl: 1.21404  (1.47335)
     | > loss_feat: 5.50950  (5.45551)
     | > loss_mel: 25.27550  (26.44948)
     | > loss_duration: 1.84276  (1.73599)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.55062  (37.97231)
     | > grad_norm_0: 167.90286  (92.53754)
     | > loss_disc: 1.91679  (2.02659)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.91679  (2.02659)
     | > grad_norm_1: 5.34372  (12.74402)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.76740  (2.12883)
     | > loader_time: 0.04810  (0.04545)


[1m   --> STEP: 152/161 -- GLOBAL_STEP: 4850[0m
     | > loss_gen: 2.50645  (2.85524)
     | > loss_kl: 1.41245  (1.47714)
     | > loss_feat: 4.45268  (5.45124)
     | > loss_mel: 25.56132  (26.42657)
     | > loss_duration: 1.69227  (1.73422)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.62517  (37.94442)
     | > grad_norm_0: 63.51910  (94.62309)
     | > loss_disc: 2.16498  (2.02628)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.16498  (2.02628)
     | > grad_norm_1: 27.05445  (12.58237)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.50270  (2.13011)
     | > loader_time: 0.02350  (0.04521)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02735 [0m(-0.00108)
     | > avg_loss_gen:[92m 2.81414 [0m(-0.47288)
     | > avg_loss_kl:[92m 1.26584 [0m(-0.35064)
     | > avg_loss_feat:[91m 6.04808 [0m(+0.68681)
     | > avg_loss_mel:[91m 26.47564 [0m(+0.09856)
     | > avg_loss_duration:[92m 1.72592 [0m(-0.00044)
     | > avg_loss_0:[92m 38.32962 [0m(-0.03859)
     | > avg_loss_disc:[92m 1.84869 [0m(-0.13649)
     | > avg_loss_1:[92m 1.84869 [0m(-0.13649)


[4m[1m > EPOCH: 30/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 19:17:23) [0m

[1m   --> STEP: 15/161 -- GLOBAL_STEP: 4875[0m
     | > loss_gen: 3.00683  (2.79604)
     | > loss_kl: 1.53840  (1.51415)
     | > loss_feat: 5.45488  (5.26925)
     | > loss_mel: 25.71338  (25.93319)
     | > loss_duration: 1.72309  (1.73139)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.43658  (37.24402)
     | > grad_norm_0: 45.62385  (96.41898)
     | > loss_disc: 1.95553  (2.08170)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.95553  (2.08170)
     | > grad_norm_1: 6.72244  (12.95177)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.22150  (2.05077)
     | > loader_time: 0.03750  (0.05271)


[1m   --> STEP: 40/161 -- GLOBAL_STEP: 4900[0m
     | > loss_gen: 2.25898  (2.79631)
     | > loss_kl: 1.38092  (1.44864)
     | > loss_feat: 4.66721  (5.32447)
     | > loss_mel: 27.52210  (25.91733)
     | > loss_duration: 1.63761  (1.73973)
     | > amp_scaler: 512.00000  (352.00000)
     | > loss_0: 37.46681  (37.22648)
     | > grad_norm_0: 160.81949  (95.96497)
     | > loss_disc: 2.35528  (2.06708)
     | > amp_scaler-1: 512.00000  (352.00000)
     | > loss_1: 2.35528  (2.06708)
     | > grad_norm_1: 17.00838  (17.29752)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.17290  (2.11124)
     | > loader_time: 0.03630  (0.05068)


[1m   --> STEP: 65/161 -- GLOBAL_STEP: 4925[0m
     | > loss_gen: 2.65639  (2.81596)
     | > loss_kl: 1.56022  (1.44900)
     | > loss_feat: 5.17264  (5.32955)
     | > loss_mel: 26.80430  (25.92096)
     | > loss_duration: 1.75246  (1.73871)
     | > amp_scaler: 512.00000  (413.53846)
     | > loss_0: 37.94602  (37.25418)
     | > grad_norm_0: 71.93260  (88.65704)
     | > loss_disc: 1.98616  (2.05847)
     | > amp_scaler-1: 512.00000  (413.53846)
     | > loss_1: 1.98616  (2.05847)
     | > grad_norm_1: 5.04907  (14.49464)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.79600  (2.08366)
     | > loader_time: 0.02940  (0.05170)


[1m   --> STEP: 90/161 -- GLOBAL_STEP: 4950[0m
     | > loss_gen: 3.03007  (2.83083)
     | > loss_kl: 1.67613  (1.45668)
     | > loss_feat: 5.85299  (5.40297)
     | > loss_mel: 26.48676  (26.00715)
     | > loss_duration: 1.67986  (1.73696)
     | > amp_scaler: 512.00000  (440.88889)
     | > loss_0: 38.72580  (37.43458)
     | > grad_norm_0: 106.79662  (91.03420)
     | > loss_disc: 1.87534  (2.03865)
     | > amp_scaler-1: 512.00000  (440.88889)
     | > loss_1: 1.87534  (2.03865)
     | > grad_norm_1: 5.57418  (13.37680)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.55550  (2.07461)
     | > loader_time: 0.02150  (0.04950)


[1m   --> STEP: 115/161 -- GLOBAL_STEP: 4975[0m
     | > loss_gen: 2.77506  (2.82686)
     | > loss_kl: 1.48322  (1.45867)
     | > loss_feat: 5.68839  (5.38584)
     | > loss_mel: 24.99422  (25.98245)
     | > loss_duration: 1.71717  (1.73766)
     | > amp_scaler: 512.00000  (456.34783)
     | > loss_0: 36.65805  (37.39147)
     | > grad_norm_0: 32.99448  (92.19430)
     | > loss_disc: 1.99484  (2.04361)
     | > amp_scaler-1: 512.00000  (456.34783)
     | > loss_1: 1.99484  (2.04361)
     | > grad_norm_1: 7.99219  (14.21515)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.16440  (2.10116)
     | > loader_time: 0.03850  (0.04829)


[1m   --> STEP: 140/161 -- GLOBAL_STEP: 5000[0m
     | > loss_gen: 2.60992  (2.83922)
     | > loss_kl: 1.71587  (1.45346)
     | > loss_feat: 5.13716  (5.40780)
     | > loss_mel: 25.91858  (25.99201)
     | > loss_duration: 1.78331  (1.73840)
     | > amp_scaler: 512.00000  (466.28571)
     | > loss_0: 37.16486  (37.43089)
     | > grad_norm_0: 68.86877  (90.59077)
     | > loss_disc: 2.16128  (2.03605)
     | > amp_scaler-1: 512.00000  (466.28571)
     | > loss_1: 2.16128  (2.03605)
     | > grad_norm_1: 9.06842  (13.76925)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.87860  (2.13106)
     | > loader_time: 0.03210  (0.04964)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02825 [0m(+0.00090)
     | > avg_loss_gen:[91m 2.84840 [0m(+0.03426)
     | > avg_loss_kl:[91m 1.52575 [0m(+0.25991)
     | > avg_loss_feat:[91m 6.55213 [0m(+0.50405)
     | > avg_loss_mel:[91m 26.67246 [0m(+0.19683)
     | > avg_loss_duration:[92m 1.71519 [0m(-0.01073)
     | > avg_loss_0:[91m 39.31394 [0m(+0.98431)
     | > avg_loss_disc:[92m 1.75219 [0m(-0.09650)
     | > avg_loss_1:[92m 1.75219 [0m(-0.09650)


[4m[1m > EPOCH: 31/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 19:23:33) [0m

[1m   --> STEP: 3/161 -- GLOBAL_STEP: 5025[0m
     | > loss_gen: 2.87082  (2.95441)
     | > loss_kl: 1.47415  (1.43266)
     | > loss_feat: 5.27656  (5.35567)
     | > loss_mel: 25.75945  (25.93678)
     | > loss_duration: 1.72375  (1.72415)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.10472  (37.40367)
     | > grad_norm_0: 61.99144  (79.44852)
     | > loss_disc: 2.01413  (1.99697)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.01413  (1.99697)
     | > grad_norm_1: 9.40311  (10.34221)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.43090  (1.99495)
     | > loader_time: 0.05730  (0.12298)


[1m   --> STEP: 28/161 -- GLOBAL_STEP: 5050[0m
     | > loss_gen: 2.87738  (2.86782)
     | > loss_kl: 1.51617  (1.51132)
     | > loss_feat: 6.75172  (5.45078)
     | > loss_mel: 29.30256  (26.82013)
     | > loss_duration: 1.77016  (1.72789)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 42.21800  (38.37794)
     | > grad_norm_0: 32.96028  (94.33354)
     | > loss_disc: 1.69703  (2.02506)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.69703  (2.02506)
     | > grad_norm_1: 12.64082  (11.83211)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.00160  (2.06571)
     | > loader_time: 0.03120  (0.04629)


[1m   --> STEP: 53/161 -- GLOBAL_STEP: 5075[0m
     | > loss_gen: 2.88444  (2.86226)
     | > loss_kl: 1.23936  (1.48910)
     | > loss_feat: 4.99751  (5.46854)
     | > loss_mel: 23.07008  (26.60633)
     | > loss_duration: 1.70740  (1.72856)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.89880  (38.15479)
     | > grad_norm_0: 43.77045  (97.58316)
     | > loss_disc: 2.04315  (2.01186)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.04315  (2.01186)
     | > grad_norm_1: 9.26108  (12.19609)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.68430  (2.08328)
     | > loader_time: 0.02470  (0.04779)


[1m   --> STEP: 78/161 -- GLOBAL_STEP: 5100[0m
     | > loss_gen: 2.96944  (2.85936)
     | > loss_kl: 1.42438  (1.48582)
     | > loss_feat: 5.76990  (5.48795)
     | > loss_mel: 24.98542  (26.35233)
     | > loss_duration: 1.65349  (1.72455)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.80263  (37.91001)
     | > grad_norm_0: 55.55098  (94.47378)
     | > loss_disc: 1.89246  (2.00337)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.89246  (2.00337)
     | > grad_norm_1: 8.78543  (12.59068)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.64280  (2.07756)
     | > loader_time: 0.12340  (0.04886)


[1m   --> STEP: 103/161 -- GLOBAL_STEP: 5125[0m
     | > loss_gen: 2.63030  (2.86011)
     | > loss_kl: 1.40529  (1.47589)
     | > loss_feat: 4.88950  (5.47578)
     | > loss_mel: 26.51282  (26.28635)
     | > loss_duration: 1.76084  (1.72758)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.19876  (37.82570)
     | > grad_norm_0: 95.55412  (94.11864)
     | > loss_disc: 2.09332  (2.00643)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.09332  (2.00643)
     | > grad_norm_1: 20.02664  (14.41445)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.31620  (2.09165)
     | > loader_time: 0.03820  (0.05106)


[1m   --> STEP: 128/161 -- GLOBAL_STEP: 5150[0m
     | > loss_gen: 3.06051  (2.85868)
     | > loss_kl: 1.55431  (1.47904)
     | > loss_feat: 5.33202  (5.48132)
     | > loss_mel: 24.54028  (26.18896)
     | > loss_duration: 1.79950  (1.73148)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.28662  (37.73947)
     | > grad_norm_0: 110.50596  (90.98902)
     | > loss_disc: 2.10365  (2.00892)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.10365  (2.00892)
     | > grad_norm_1: 51.08528  (14.46921)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.67090  (2.13786)
     | > loader_time: 0.05070  (0.05228)


[1m   --> STEP: 153/161 -- GLOBAL_STEP: 5175[0m
     | > loss_gen: 2.81268  (2.84750)
     | > loss_kl: 1.39521  (1.48120)
     | > loss_feat: 4.65081  (5.44982)
     | > loss_mel: 24.80474  (26.12510)
     | > loss_duration: 1.77345  (1.73284)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.43689  (37.63645)
     | > grad_norm_0: 96.92357  (91.21775)
     | > loss_disc: 2.22655  (2.02239)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.22655  (2.02239)
     | > grad_norm_1: 14.71037  (14.78550)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.46330  (2.13508)
     | > loader_time: 0.03760  (0.05089)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02779 [0m(-0.00045)
     | > avg_loss_gen:[91m 3.45500 [0m(+0.60660)
     | > avg_loss_kl:[92m 1.38757 [0m(-0.13819)
     | > avg_loss_feat:[92m 5.35592 [0m(-1.19621)
     | > avg_loss_mel:[92m 24.41434 [0m(-2.25813)
     | > avg_loss_duration:[92m 1.70332 [0m(-0.01187)
     | > avg_loss_0:[92m 36.31615 [0m(-2.99779)
     | > avg_loss_disc:[91m 2.02203 [0m(+0.26984)
     | > avg_loss_1:[91m 2.02203 [0m(+0.26984)

 > BEST MODEL : /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000/best_model_5184.pth.tar

[4m[1m > EPOCH: 32/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 19:29:46) [0m

[1m   --> STEP: 16/161 -- GLOBAL_STEP: 5200[0m
     | > loss_gen: 2.94215  (2.88959)
     | > loss_kl: 1.56542  (1.50743)
     | > loss_feat: 5.33889  (5.54909)
     | > loss_mel: 25.68921  (26.28434)
     | > loss_duration: 1.82392  (1.72136)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.35959  (37.95180)
     | > grad_norm_0: 31.98228  (102.03855)
     | > loss_disc: 2.00206  (1.95958)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.00206  (1.95958)
     | > grad_norm_1: 11.73681  (12.72159)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.19550  (2.09004)
     | > loader_time: 0.03690  (0.05064)


[1m   --> STEP: 41/161 -- GLOBAL_STEP: 5225[0m
     | > loss_gen: 2.98425  (2.83839)
     | > loss_kl: 1.50083  (1.51974)
     | > loss_feat: 5.21212  (5.43495)
     | > loss_mel: 26.89161  (26.13515)
     | > loss_duration: 1.69332  (1.71732)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.28215  (37.64554)
     | > grad_norm_0: 124.47800  (107.19048)
     | > loss_disc: 1.99617  (2.02493)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.99617  (2.02493)
     | > grad_norm_1: 12.18778  (12.18514)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.65440  (2.14686)
     | > loader_time: 0.02560  (0.05077)


[1m   --> STEP: 66/161 -- GLOBAL_STEP: 5250[0m
     | > loss_gen: 2.78264  (2.84425)
     | > loss_kl: 1.48292  (1.50667)
     | > loss_feat: 5.48091  (5.44676)
     | > loss_mel: 26.11154  (26.13040)
     | > loss_duration: 1.77079  (1.72474)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.62880  (37.65283)
     | > grad_norm_0: 77.96574  (106.73504)
     | > loss_disc: 1.97815  (2.02608)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.97815  (2.02608)
     | > grad_norm_1: 6.55865  (13.86117)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.99990  (2.10886)
     | > loader_time: 0.03060  (0.04656)


[1m   --> STEP: 91/161 -- GLOBAL_STEP: 5275[0m
     | > loss_gen: 3.04484  (2.85356)
     | > loss_kl: 1.31071  (1.50454)
     | > loss_feat: 4.54130  (5.50244)
     | > loss_mel: 24.16261  (26.14581)
     | > loss_duration: 1.73951  (1.72475)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.79897  (37.73110)
     | > grad_norm_0: 111.87943  (101.26577)
     | > loss_disc: 2.34795  (2.02107)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.34795  (2.02107)
     | > grad_norm_1: 11.88239  (12.84084)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.15330  (2.10335)
     | > loader_time: 0.03110  (0.04651)


[1m   --> STEP: 116/161 -- GLOBAL_STEP: 5300[0m
     | > loss_gen: 2.66837  (2.84930)
     | > loss_kl: 1.60131  (1.49115)
     | > loss_feat: 6.67555  (5.49196)
     | > loss_mel: 25.15204  (26.04673)
     | > loss_duration: 1.81191  (1.73143)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.90917  (37.61058)
     | > grad_norm_0: 46.77478  (97.70894)
     | > loss_disc: 1.87997  (2.02415)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.87997  (2.02415)
     | > grad_norm_1: 10.13373  (12.68842)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.27030  (2.13477)
     | > loader_time: 0.04330  (0.04669)


[1m   --> STEP: 141/161 -- GLOBAL_STEP: 5325[0m
     | > loss_gen: 3.01854  (2.84417)
     | > loss_kl: 1.62403  (1.48962)
     | > loss_feat: 5.73250  (5.46006)
     | > loss_mel: 25.24050  (26.03620)
     | > loss_duration: 1.66619  (1.73253)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.28175  (37.56258)
     | > grad_norm_0: 101.50383  (96.35128)
     | > loss_disc: 2.05979  (2.03048)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.05979  (2.03048)
     | > grad_norm_1: 7.23698  (12.80314)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.70520  (2.15231)
     | > loader_time: 0.09910  (0.05006)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02726 [0m(-0.00054)
     | > avg_loss_gen:[92m 3.25539 [0m(-0.19961)
     | > avg_loss_kl:[91m 1.52533 [0m(+0.13776)
     | > avg_loss_feat:[92m 5.34353 [0m(-0.01240)
     | > avg_loss_mel:[91m 25.27990 [0m(+0.86556)
     | > avg_loss_duration:[91m 1.72719 [0m(+0.02387)
     | > avg_loss_0:[91m 37.13133 [0m(+0.81519)
     | > avg_loss_disc:[91m 2.12877 [0m(+0.10674)
     | > avg_loss_1:[91m 2.12877 [0m(+0.10674)


[4m[1m > EPOCH: 33/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 19:35:57) [0m

[1m   --> STEP: 4/161 -- GLOBAL_STEP: 5350[0m
     | > loss_gen: 3.02580  (2.70570)
     | > loss_kl: 1.68720  (1.54242)
     | > loss_feat: 5.22492  (5.35668)
     | > loss_mel: 25.78745  (25.74926)
     | > loss_duration: 1.62805  (1.70108)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.35342  (37.05514)
     | > grad_norm_0: 152.25504  (123.78659)
     | > loss_disc: 2.02468  (2.02747)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.02468  (2.02747)
     | > grad_norm_1: 35.97284  (27.47019)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.78080  (2.09332)
     | > loader_time: 0.03300  (0.08260)


[1m   --> STEP: 29/161 -- GLOBAL_STEP: 5375[0m
     | > loss_gen: 3.17682  (2.85528)
     | > loss_kl: 1.45570  (1.49428)
     | > loss_feat: 5.22414  (5.57733)
     | > loss_mel: 25.41427  (25.69712)
     | > loss_duration: 1.63613  (1.71781)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.90705  (37.34183)
     | > grad_norm_0: 101.88372  (94.38387)
     | > loss_disc: 2.04624  (2.03382)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.04624  (2.03382)
     | > grad_norm_1: 15.91005  (19.09477)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.12900  (2.08785)
     | > loader_time: 0.03180  (0.04082)


[1m   --> STEP: 54/161 -- GLOBAL_STEP: 5400[0m
     | > loss_gen: 2.52094  (2.82001)
     | > loss_kl: 1.34231  (1.50292)
     | > loss_feat: 5.66388  (5.43727)
     | > loss_mel: 27.36085  (25.74689)
     | > loss_duration: 1.69723  (1.72939)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.58521  (37.23649)
     | > grad_norm_0: 110.13316  (95.73295)
     | > loss_disc: 1.95059  (2.04308)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.95059  (2.04308)
     | > grad_norm_1: 7.97023  (16.40907)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.46560  (2.09233)
     | > loader_time: 0.03420  (0.03862)


[1m   --> STEP: 79/161 -- GLOBAL_STEP: 5425[0m
     | > loss_gen: 2.89810  (2.83523)
     | > loss_kl: 1.46944  (1.51416)
     | > loss_feat: 5.64557  (5.48522)
     | > loss_mel: 27.26206  (25.78111)
     | > loss_duration: 1.78531  (1.72658)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 39.06049  (37.34230)
     | > grad_norm_0: 64.45361  (94.67422)
     | > loss_disc: 1.97115  (2.02736)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.97115  (2.02736)
     | > grad_norm_1: 7.27470  (16.63082)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.98550  (2.08191)
     | > loader_time: 0.02980  (0.04600)


[1m   --> STEP: 104/161 -- GLOBAL_STEP: 5450[0m
     | > loss_gen: 2.96651  (2.84712)
     | > loss_kl: 1.49814  (1.50039)
     | > loss_feat: 5.76377  (5.50187)
     | > loss_mel: 26.31004  (25.84223)
     | > loss_duration: 1.76853  (1.72805)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.30699  (37.41966)
     | > grad_norm_0: 54.70928  (93.22231)
     | > loss_disc: 1.96607  (2.02099)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.96607  (2.02099)
     | > grad_norm_1: 12.86347  (15.48993)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.53020  (2.09098)
     | > loader_time: 0.02740  (0.04730)


[1m   --> STEP: 129/161 -- GLOBAL_STEP: 5475[0m
     | > loss_gen: 2.91084  (2.85740)
     | > loss_kl: 1.54708  (1.49053)
     | > loss_feat: 5.25024  (5.52853)
     | > loss_mel: 25.82488  (25.81915)
     | > loss_duration: 1.74018  (1.72722)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.27321  (37.42283)
     | > grad_norm_0: 129.37743  (96.30518)
     | > loss_disc: 2.03082  (2.01438)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.03082  (2.01438)
     | > grad_norm_1: 34.85830  (15.40043)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.46010  (2.14265)
     | > loader_time: 0.04470  (0.04759)


[1m   --> STEP: 154/161 -- GLOBAL_STEP: 5500[0m
     | > loss_gen: 2.97404  (2.85319)
     | > loss_kl: 1.57451  (1.48945)
     | > loss_feat: 5.13977  (5.49672)
     | > loss_mel: 24.24089  (25.75500)
     | > loss_duration: 1.74460  (1.72864)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.67381  (37.32299)
     | > grad_norm_0: 82.41036  (97.04881)
     | > loss_disc: 2.07803  (2.01916)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.07803  (2.01916)
     | > grad_norm_1: 28.75722  (15.63067)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.70850  (2.14463)
     | > loader_time: 0.02990  (0.04659)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02713 [0m(-0.00012)
     | > avg_loss_gen:[92m 3.14471 [0m(-0.11068)
     | > avg_loss_kl:[91m 1.57573 [0m(+0.05040)
     | > avg_loss_feat:[91m 5.38670 [0m(+0.04318)
     | > avg_loss_mel:[92m 24.02575 [0m(-1.25414)
     | > avg_loss_duration:[92m 1.70789 [0m(-0.01930)
     | > avg_loss_0:[92m 35.84078 [0m(-1.29055)
     | > avg_loss_disc:[92m 1.89791 [0m(-0.23085)
     | > avg_loss_1:[92m 1.89791 [0m(-0.23085)

 > BEST MODEL : /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000/best_model_5508.pth.tar

[4m[1m > EPOCH: 34/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 19:42:08) [0m

[1m   --> STEP: 17/161 -- GLOBAL_STEP: 5525[0m
     | > loss_gen: 2.87337  (2.88080)
     | > loss_kl: 1.72451  (1.52339)
     | > loss_feat: 5.45849  (5.64446)
     | > loss_mel: 23.69081  (25.46785)
     | > loss_duration: 1.71168  (1.74288)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.45886  (37.25939)
     | > grad_norm_0: 83.40878  (66.80315)
     | > loss_disc: 2.09051  (1.96067)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.09051  (1.96067)
     | > grad_norm_1: 9.74314  (8.47770)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.13870  (2.08274)
     | > loader_time: 0.03580  (0.03888)


[1m   --> STEP: 42/161 -- GLOBAL_STEP: 5550[0m
     | > loss_gen: 2.68233  (2.84019)
     | > loss_kl: 1.45801  (1.49080)
     | > loss_feat: 6.00896  (5.51508)
     | > loss_mel: 26.62478  (25.66077)
     | > loss_duration: 1.70325  (1.73667)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.47734  (37.24352)
     | > grad_norm_0: 97.93299  (82.23046)
     | > loss_disc: 1.97928  (2.00575)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.97928  (2.00575)
     | > grad_norm_1: 9.61230  (10.19190)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.80880  (2.09955)
     | > loader_time: 0.02760  (0.04246)


[1m   --> STEP: 67/161 -- GLOBAL_STEP: 5575[0m
     | > loss_gen: 2.48173  (2.86083)
     | > loss_kl: 1.26877  (1.50265)
     | > loss_feat: 4.91857  (5.54033)
     | > loss_mel: 25.62782  (25.71349)
     | > loss_duration: 1.71614  (1.73170)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.01303  (37.34900)
     | > grad_norm_0: 166.64610  (91.25023)
     | > loss_disc: 2.08242  (2.01412)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.08242  (2.01412)
     | > grad_norm_1: 16.98965  (11.94226)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.11970  (2.08173)
     | > loader_time: 0.09710  (0.04479)


[1m   --> STEP: 92/161 -- GLOBAL_STEP: 5600[0m
     | > loss_gen: 2.92227  (2.85653)
     | > loss_kl: 1.60815  (1.50105)
     | > loss_feat: 5.53610  (5.52952)
     | > loss_mel: 24.48738  (25.70213)
     | > loss_duration: 1.75330  (1.73008)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.30720  (37.31931)
     | > grad_norm_0: 82.82017  (93.61497)
     | > loss_disc: 1.96345  (2.01623)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.96345  (2.01623)
     | > grad_norm_1: 9.48137  (11.79468)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.18810  (2.08263)
     | > loader_time: 0.03180  (0.04650)


[1m   --> STEP: 117/161 -- GLOBAL_STEP: 5625[0m
     | > loss_gen: 2.75803  (2.86035)
     | > loss_kl: 1.53216  (1.50336)
     | > loss_feat: 5.55911  (5.55154)
     | > loss_mel: 24.92451  (25.66830)
     | > loss_duration: 1.73595  (1.73052)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.50976  (37.31407)
     | > grad_norm_0: 92.34793  (94.75838)
     | > loss_disc: 2.02754  (2.01446)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.02754  (2.01446)
     | > grad_norm_1: 16.36539  (11.64836)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.44110  (2.11498)
     | > loader_time: 0.04490  (0.04630)


[1m   --> STEP: 142/161 -- GLOBAL_STEP: 5650[0m
     | > loss_gen: 2.83153  (2.85491)
     | > loss_kl: 1.59024  (1.50680)
     | > loss_feat: 5.75018  (5.53637)
     | > loss_mel: 25.27054  (25.63562)
     | > loss_duration: 1.80822  (1.73013)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.25071  (37.26383)
     | > grad_norm_0: 131.91695  (93.97962)
     | > loss_disc: 1.95191  (2.01435)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.95191  (2.01435)
     | > grad_norm_1: 9.07978  (11.62637)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.18760  (2.13163)
     | > loader_time: 0.03450  (0.04728)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02715 [0m(+0.00002)
     | > avg_loss_gen:[91m 3.24264 [0m(+0.09793)
     | > avg_loss_kl:[91m 1.76170 [0m(+0.18597)
     | > avg_loss_feat:[91m 5.55524 [0m(+0.16854)
     | > avg_loss_mel:[91m 25.58339 [0m(+1.55763)
     | > avg_loss_duration:[92m 1.70464 [0m(-0.00325)
     | > avg_loss_0:[91m 37.84760 [0m(+2.00682)
     | > avg_loss_disc:[91m 1.98816 [0m(+0.09025)
     | > avg_loss_1:[91m 1.98816 [0m(+0.09025)


[4m[1m > EPOCH: 35/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 19:48:19) [0m

[1m   --> STEP: 5/161 -- GLOBAL_STEP: 5675[0m
     | > loss_gen: 2.54953  (2.73887)
     | > loss_kl: 1.42497  (1.53377)
     | > loss_feat: 4.91931  (5.53540)
     | > loss_mel: 26.10921  (25.75341)
     | > loss_duration: 1.70850  (1.73395)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.71152  (37.29540)
     | > grad_norm_0: 122.97346  (119.93733)
     | > loss_disc: 2.18633  (2.00603)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.18633  (2.00603)
     | > grad_norm_1: 23.43161  (22.90546)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.42290  (2.07177)
     | > loader_time: 0.03400  (0.07937)


[1m   --> STEP: 30/161 -- GLOBAL_STEP: 5700[0m
     | > loss_gen: 2.92951  (2.78383)
     | > loss_kl: 1.52561  (1.52750)
     | > loss_feat: 5.22081  (5.37229)
     | > loss_mel: 26.22379  (25.72760)
     | > loss_duration: 1.73970  (1.71533)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.63941  (37.12654)
     | > grad_norm_0: 148.46239  (105.10464)
     | > loss_disc: 2.09352  (2.04492)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.09352  (2.04492)
     | > grad_norm_1: 20.44810  (15.90860)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.63590  (2.11727)
     | > loader_time: 0.04150  (0.04171)


[1m   --> STEP: 55/161 -- GLOBAL_STEP: 5725[0m
     | > loss_gen: 2.96412  (2.83309)
     | > loss_kl: 1.41404  (1.53993)
     | > loss_feat: 5.23454  (5.50127)
     | > loss_mel: 27.34430  (25.94693)
     | > loss_duration: 1.66322  (1.71675)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.62022  (37.53798)
     | > grad_norm_0: 105.46305  (93.99406)
     | > loss_disc: 2.15530  (2.04193)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.15530  (2.04193)
     | > grad_norm_1: 10.45372  (13.84977)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.27370  (2.11256)
     | > loader_time: 0.03980  (0.04622)


[1m   --> STEP: 80/161 -- GLOBAL_STEP: 5750[0m
     | > loss_gen: 2.54265  (2.83950)
     | > loss_kl: 1.53988  (1.52663)
     | > loss_feat: 4.93029  (5.51039)
     | > loss_mel: 24.84617  (25.89306)
     | > loss_duration: 1.75247  (1.72165)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.61146  (37.49124)
     | > grad_norm_0: 77.99166  (96.27003)
     | > loss_disc: 2.16839  (2.03070)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.16839  (2.03070)
     | > grad_norm_1: 28.10932  (14.38838)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.01470  (2.10489)
     | > loader_time: 0.14600  (0.04761)


[1m   --> STEP: 105/161 -- GLOBAL_STEP: 5775[0m
     | > loss_gen: 2.86236  (2.83543)
     | > loss_kl: 1.33526  (1.51551)
     | > loss_feat: 5.28338  (5.49080)
     | > loss_mel: 25.51754  (25.75782)
     | > loss_duration: 1.68614  (1.72398)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.68468  (37.32354)
     | > grad_norm_0: 138.55687  (93.83922)
     | > loss_disc: 2.04074  (2.03319)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.04074  (2.03319)
     | > grad_norm_1: 10.29822  (14.61156)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.18820  (2.11295)
     | > loader_time: 0.03320  (0.04772)


[1m   --> STEP: 130/161 -- GLOBAL_STEP: 5800[0m
     | > loss_gen: 2.96196  (2.83764)
     | > loss_kl: 1.45800  (1.50485)
     | > loss_feat: 5.24586  (5.49016)
     | > loss_mel: 25.72943  (25.72723)
     | > loss_duration: 1.75139  (1.72492)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.14664  (37.28480)
     | > grad_norm_0: 80.15964  (93.84591)
     | > loss_disc: 1.97646  (2.03033)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.97646  (2.03033)
     | > grad_norm_1: 23.01722  (14.47946)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.47920  (2.16220)
     | > loader_time: 0.04180  (0.04842)


[1m   --> STEP: 155/161 -- GLOBAL_STEP: 5825[0m
     | > loss_gen: 2.88890  (2.83894)
     | > loss_kl: 1.84152  (1.51221)
     | > loss_feat: 5.64943  (5.50021)
     | > loss_mel: 25.26496  (25.68904)
     | > loss_duration: 1.62558  (1.72639)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.27039  (37.26679)
     | > grad_norm_0: 123.03508  (92.50910)
     | > loss_disc: 2.06553  (2.03074)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06553  (2.03074)
     | > grad_norm_1: 8.78989  (13.97182)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.85180  (2.15219)
     | > loader_time: 0.09090  (0.04947)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02770 [0m(+0.00055)
     | > avg_loss_gen:[92m 3.13142 [0m(-0.11122)
     | > avg_loss_kl:[92m 1.35882 [0m(-0.40288)
     | > avg_loss_feat:[91m 5.87224 [0m(+0.31700)
     | > avg_loss_mel:[91m 26.21020 [0m(+0.62681)
     | > avg_loss_duration:[92m 1.69765 [0m(-0.00699)
     | > avg_loss_0:[91m 38.27032 [0m(+0.42272)
     | > avg_loss_disc:[92m 1.87226 [0m(-0.11591)
     | > avg_loss_1:[92m 1.87226 [0m(-0.11591)


[4m[1m > EPOCH: 36/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 19:54:30) [0m

[1m   --> STEP: 18/161 -- GLOBAL_STEP: 5850[0m
     | > loss_gen: 2.65211  (2.87202)
     | > loss_kl: 1.58646  (1.50163)
     | > loss_feat: 5.49146  (5.45574)
     | > loss_mel: 24.51995  (25.63464)
     | > loss_duration: 1.72239  (1.71911)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.97236  (37.18313)
     | > grad_norm_0: 96.04585  (107.59287)
     | > loss_disc: 2.05471  (2.03266)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.05471  (2.03266)
     | > grad_norm_1: 10.44096  (14.06860)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.14310  (2.11995)
     | > loader_time: 0.03580  (0.04300)


[1m   --> STEP: 43/161 -- GLOBAL_STEP: 5875[0m
     | > loss_gen: 2.49884  (2.84704)
     | > loss_kl: 1.45904  (1.47955)
     | > loss_feat: 4.83061  (5.48812)
     | > loss_mel: 25.47444  (25.55053)
     | > loss_duration: 1.77605  (1.72162)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.03899  (37.08686)
     | > grad_norm_0: 109.61487  (96.49821)
     | > loss_disc: 2.25706  (2.03860)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.25706  (2.03860)
     | > grad_norm_1: 10.06183  (12.70128)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.64120  (2.11834)
     | > loader_time: 0.05640  (0.04391)


[1m   --> STEP: 68/161 -- GLOBAL_STEP: 5900[0m
     | > loss_gen: 3.03637  (2.84361)
     | > loss_kl: 1.40724  (1.48648)
     | > loss_feat: 6.04671  (5.49322)
     | > loss_mel: 27.57628  (25.56589)
     | > loss_duration: 1.68643  (1.71827)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 39.75304  (37.10747)
     | > grad_norm_0: 161.69206  (98.65147)
     | > loss_disc: 1.84623  (2.03239)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.84623  (2.03239)
     | > grad_norm_1: 30.10104  (14.13082)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.13450  (2.09243)
     | > loader_time: 0.08960  (0.04920)


[1m   --> STEP: 93/161 -- GLOBAL_STEP: 5925[0m
     | > loss_gen: 3.05099  (2.84882)
     | > loss_kl: 1.54335  (1.49952)
     | > loss_feat: 4.89157  (5.51142)
     | > loss_mel: 24.34165  (25.66453)
     | > loss_duration: 1.85215  (1.72357)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.67970  (37.24786)
     | > grad_norm_0: 123.00600  (97.82263)
     | > loss_disc: 2.06829  (2.02871)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06829  (2.02871)
     | > grad_norm_1: 12.08373  (14.21145)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.77050  (2.08799)
     | > loader_time: 0.06300  (0.05293)


[1m   --> STEP: 118/161 -- GLOBAL_STEP: 5950[0m
     | > loss_gen: 2.58513  (2.84509)
     | > loss_kl: 1.57355  (1.50055)
     | > loss_feat: 5.44509  (5.49230)
     | > loss_mel: 26.14779  (25.59420)
     | > loss_duration: 1.76337  (1.72319)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.51493  (37.15532)
     | > grad_norm_0: 76.76362  (97.49082)
     | > loss_disc: 1.99647  (2.02443)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.99647  (2.02443)
     | > grad_norm_1: 14.14226  (14.18131)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.45570  (2.12041)
     | > loader_time: 0.04460  (0.05174)


[1m   --> STEP: 143/161 -- GLOBAL_STEP: 5975[0m
     | > loss_gen: 2.56983  (2.84599)
     | > loss_kl: 1.66272  (1.50223)
     | > loss_feat: 5.71919  (5.50499)
     | > loss_mel: 26.70595  (25.56444)
     | > loss_duration: 1.77829  (1.72333)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.43599  (37.14098)
     | > grad_norm_0: 46.02165  (99.11161)
     | > loss_disc: 1.92819  (2.02179)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.92819  (2.02179)
     | > grad_norm_1: 10.22276  (14.89472)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.34140  (2.13852)
     | > loader_time: 0.03780  (0.05244)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02753 [0m(-0.00017)
     | > avg_loss_gen:[92m 2.89318 [0m(-0.23824)
     | > avg_loss_kl:[91m 1.48348 [0m(+0.12466)
     | > avg_loss_feat:[92m 4.55036 [0m(-1.32187)
     | > avg_loss_mel:[92m 23.93046 [0m(-2.27974)
     | > avg_loss_duration:[92m 1.69688 [0m(-0.00077)
     | > avg_loss_0:[92m 34.55436 [0m(-3.71596)
     | > avg_loss_disc:[91m 2.18697 [0m(+0.31471)
     | > avg_loss_1:[91m 2.18697 [0m(+0.31471)

 > BEST MODEL : /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000/best_model_5994.pth.tar

[4m[1m > EPOCH: 37/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 20:00:40) [0m

[1m   --> STEP: 6/161 -- GLOBAL_STEP: 6000[0m
     | > loss_gen: 2.99455  (2.90184)
     | > loss_kl: 1.54406  (1.52483)
     | > loss_feat: 5.78756  (5.59021)
     | > loss_mel: 27.33546  (26.06341)
     | > loss_duration: 1.64102  (1.68988)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 39.30265  (37.77017)
     | > grad_norm_0: 79.89227  (92.48171)
     | > loss_disc: 1.86514  (1.98001)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.86514  (1.98001)
     | > grad_norm_1: 9.37478  (13.45633)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.39430  (2.07727)
     | > loader_time: 0.03650  (0.05376)


[1m   --> STEP: 31/161 -- GLOBAL_STEP: 6025[0m
     | > loss_gen: 2.90068  (2.87599)
     | > loss_kl: 1.51818  (1.51724)
     | > loss_feat: 6.54374  (5.59185)
     | > loss_mel: 25.84432  (25.77845)
     | > loss_duration: 1.71727  (1.71618)
     | > amp_scaler: 512.00000  (355.09677)
     | > loss_0: 38.52418  (37.47972)
     | > grad_norm_0: 34.74822  (88.41156)
     | > loss_disc: 1.69909  (1.98526)
     | > amp_scaler-1: 512.00000  (355.09677)
     | > loss_1: 1.69909  (1.98526)
     | > grad_norm_1: 12.48081  (13.82874)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.91250  (2.12158)
     | > loader_time: 0.04650  (0.04575)


[1m   --> STEP: 56/161 -- GLOBAL_STEP: 6050[0m
     | > loss_gen: 2.74471  (2.86973)
     | > loss_kl: 1.49213  (1.50792)
     | > loss_feat: 5.46511  (5.59910)
     | > loss_mel: 25.90468  (25.72093)
     | > loss_duration: 1.80896  (1.71768)
     | > amp_scaler: 512.00000  (425.14286)
     | > loss_0: 37.41559  (37.41536)
     | > grad_norm_0: 71.10175  (87.72686)
     | > loss_disc: 2.14773  (2.01020)
     | > amp_scaler-1: 512.00000  (425.14286)
     | > loss_1: 2.14773  (2.01020)
     | > grad_norm_1: 27.27757  (12.38059)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.92940  (2.09991)
     | > loader_time: 0.03780  (0.04511)


[1m   --> STEP: 81/161 -- GLOBAL_STEP: 6075[0m
     | > loss_gen: 2.95159  (2.86485)
     | > loss_kl: 1.39499  (1.50453)
     | > loss_feat: 5.77606  (5.61512)
     | > loss_mel: 24.68733  (25.59849)
     | > loss_duration: 1.68891  (1.71931)
     | > amp_scaler: 512.00000  (451.95062)
     | > loss_0: 36.49888  (37.30230)
     | > grad_norm_0: 105.31794  (86.18893)
     | > loss_disc: 1.97173  (2.00709)
     | > amp_scaler-1: 512.00000  (451.95062)
     | > loss_1: 1.97173  (2.00709)
     | > grad_norm_1: 7.92699  (11.70750)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.03900  (2.08181)
     | > loader_time: 0.18170  (0.05034)


[1m   --> STEP: 106/161 -- GLOBAL_STEP: 6100[0m
     | > loss_gen: 2.65031  (2.86455)
     | > loss_kl: 1.30527  (1.50619)
     | > loss_feat: 5.43544  (5.61204)
     | > loss_mel: 24.91174  (25.55440)
     | > loss_duration: 1.72995  (1.72426)
     | > amp_scaler: 512.00000  (466.11321)
     | > loss_0: 36.03272  (37.26145)
     | > grad_norm_0: 80.77242  (83.95511)
     | > loss_disc: 2.07297  (2.00956)
     | > amp_scaler-1: 512.00000  (466.11321)
     | > loss_1: 2.07297  (2.00956)
     | > grad_norm_1: 9.19025  (12.76106)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.10730  (2.08825)
     | > loader_time: 0.15470  (0.05058)


[1m   --> STEP: 131/161 -- GLOBAL_STEP: 6125[0m
     | > loss_gen: 3.08406  (2.86577)
     | > loss_kl: 1.43421  (1.50506)
     | > loss_feat: 5.77865  (5.61039)
     | > loss_mel: 24.40756  (25.59581)
     | > loss_duration: 1.72953  (1.72487)
     | > amp_scaler: 512.00000  (474.87023)
     | > loss_0: 36.43401  (37.30190)
     | > grad_norm_0: 71.69742  (86.06641)
     | > loss_disc: 1.94102  (2.00894)
     | > amp_scaler-1: 512.00000  (474.87023)
     | > loss_1: 1.94102  (2.00894)
     | > grad_norm_1: 12.07491  (12.69627)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.15840  (2.15486)
     | > loader_time: 0.03820  (0.05422)


[1m   --> STEP: 156/161 -- GLOBAL_STEP: 6150[0m
     | > loss_gen: 2.86574  (2.86421)
     | > loss_kl: 1.25273  (1.49651)
     | > loss_feat: 5.60021  (5.60161)
     | > loss_mel: 24.71755  (25.53396)
     | > loss_duration: 1.75301  (1.72639)
     | > amp_scaler: 512.00000  (480.82051)
     | > loss_0: 36.18924  (37.22269)
     | > grad_norm_0: 110.57431  (85.01041)
     | > loss_disc: 2.05213  (2.00920)
     | > amp_scaler-1: 512.00000  (480.82051)
     | > loss_1: 2.05213  (2.00920)
     | > grad_norm_1: 7.18294  (12.42744)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.00640  (2.14521)
     | > loader_time: 0.04340  (0.05351)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02843 [0m(+0.00090)
     | > avg_loss_gen:[92m 2.65048 [0m(-0.24271)
     | > avg_loss_kl:[91m 1.67113 [0m(+0.18765)
     | > avg_loss_feat:[91m 5.64555 [0m(+1.09519)
     | > avg_loss_mel:[91m 26.13764 [0m(+2.20718)
     | > avg_loss_duration:[92m 1.68670 [0m(-0.01018)
     | > avg_loss_0:[91m 37.79150 [0m(+3.23713)
     | > avg_loss_disc:[92m 1.95344 [0m(-0.23353)
     | > avg_loss_1:[92m 1.95344 [0m(-0.23353)


[4m[1m > EPOCH: 38/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 20:06:51) [0m

[1m   --> STEP: 19/161 -- GLOBAL_STEP: 6175[0m
     | > loss_gen: 2.76447  (2.88532)
     | > loss_kl: 1.27537  (1.51831)
     | > loss_feat: 5.44983  (5.56697)
     | > loss_mel: 27.33437  (25.96067)
     | > loss_duration: 1.75610  (1.72036)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 38.58015  (37.65163)
     | > grad_norm_0: 162.41452  (109.32273)
     | > loss_disc: 1.95224  (1.98943)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.95224  (1.98943)
     | > grad_norm_1: 38.85290  (15.72575)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.06560  (2.06842)
     | > loader_time: 0.03650  (0.06419)


[1m   --> STEP: 44/161 -- GLOBAL_STEP: 6200[0m
     | > loss_gen: 3.03994  (2.86886)
     | > loss_kl: 1.64742  (1.52815)
     | > loss_feat: 5.49126  (5.58761)
     | > loss_mel: 25.80503  (25.77761)
     | > loss_duration: 1.67371  (1.72802)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 37.65735  (37.49025)
     | > grad_norm_0: 127.55256  (96.68493)
     | > loss_disc: 2.03875  (2.01900)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.03875  (2.01900)
     | > grad_norm_1: 11.41506  (14.80256)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.89210  (2.11629)
     | > loader_time: 0.03200  (0.05190)


[1m   --> STEP: 69/161 -- GLOBAL_STEP: 6225[0m
     | > loss_gen: 2.84998  (2.86413)
     | > loss_kl: 1.27668  (1.51463)
     | > loss_feat: 5.50827  (5.57556)
     | > loss_mel: 24.70602  (25.67412)
     | > loss_duration: 1.72516  (1.72161)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 36.06611  (37.35005)
     | > grad_norm_0: 108.37108  (94.31404)
     | > loss_disc: 1.94995  (2.02568)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.94995  (2.02568)
     | > grad_norm_1: 11.11660  (14.66184)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.86200  (2.09199)
     | > loader_time: 0.15580  (0.05080)


[1m   --> STEP: 94/161 -- GLOBAL_STEP: 6250[0m
     | > loss_gen: 2.83890  (2.86838)
     | > loss_kl: 1.24218  (1.51056)
     | > loss_feat: 5.40437  (5.59538)
     | > loss_mel: 25.19994  (25.64077)
     | > loss_duration: 1.75485  (1.72315)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 36.44024  (37.33824)
     | > grad_norm_0: 44.69969  (96.63349)
     | > loss_disc: 2.07389  (2.01437)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.07389  (2.01437)
     | > grad_norm_1: 6.42069  (14.06542)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.27570  (2.09608)
     | > loader_time: 0.04540  (0.05270)


[1m   --> STEP: 119/161 -- GLOBAL_STEP: 6275[0m
     | > loss_gen: 2.86231  (2.87488)
     | > loss_kl: 1.51804  (1.51633)
     | > loss_feat: 6.18532  (5.63070)
     | > loss_mel: 25.73272  (25.59068)
     | > loss_duration: 1.61973  (1.72564)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 37.91811  (37.33823)
     | > grad_norm_0: 85.02406  (93.81187)
     | > loss_disc: 1.92055  (2.00709)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.92055  (2.00709)
     | > grad_norm_1: 7.03348  (13.30193)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.35830  (2.13326)
     | > loader_time: 0.03760  (0.05371)


[1m   --> STEP: 144/161 -- GLOBAL_STEP: 6300[0m
     | > loss_gen: 3.16323  (2.87157)
     | > loss_kl: 1.40783  (1.51450)
     | > loss_feat: 5.60207  (5.62375)
     | > loss_mel: 24.66268  (25.55509)
     | > loss_duration: 1.73176  (1.72587)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 36.56757  (37.29078)
     | > grad_norm_0: 76.11158  (97.02340)
     | > loss_disc: 2.01897  (2.01495)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.01897  (2.01495)
     | > grad_norm_1: 11.42799  (13.17229)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.09300  (2.14724)
     | > loader_time: 0.03910  (0.05262)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02858 [0m(+0.00014)
     | > avg_loss_gen:[91m 2.86705 [0m(+0.21657)
     | > avg_loss_kl:[91m 1.67155 [0m(+0.00042)
     | > avg_loss_feat:[92m 5.55558 [0m(-0.08998)
     | > avg_loss_mel:[92m 25.04822 [0m(-1.08942)
     | > avg_loss_duration:[91m 1.69733 [0m(+0.01063)
     | > avg_loss_0:[92m 36.83972 [0m(-0.95177)
     | > avg_loss_disc:[91m 2.62342 [0m(+0.66998)
     | > avg_loss_1:[91m 2.62342 [0m(+0.66998)


[4m[1m > EPOCH: 39/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 20:13:01) [0m

[1m   --> STEP: 7/161 -- GLOBAL_STEP: 6325[0m
     | > loss_gen: 3.39342  (2.90182)
     | > loss_kl: 1.38305  (1.50963)
     | > loss_feat: 6.29444  (5.73370)
     | > loss_mel: 24.84402  (25.72499)
     | > loss_duration: 1.73359  (1.75056)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 37.64851  (37.62069)
     | > grad_norm_0: 161.48508  (98.31516)
     | > loss_disc: 1.74059  (1.99209)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.74059  (1.99209)
     | > grad_norm_1: 22.17701  (17.60842)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.04560  (2.10820)
     | > loader_time: 0.04610  (0.05259)


[1m   --> STEP: 32/161 -- GLOBAL_STEP: 6350[0m
     | > loss_gen: 2.75890  (2.85136)
     | > loss_kl: 1.42614  (1.51061)
     | > loss_feat: 4.98606  (5.44990)
     | > loss_mel: 24.88548  (25.75661)
     | > loss_duration: 1.78397  (1.71615)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 35.84055  (37.28463)
     | > grad_norm_0: 118.13403  (98.84453)
     | > loss_disc: 2.01422  (2.16423)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.01422  (2.16423)
     | > grad_norm_1: 8.18056  (18.52782)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.34720  (2.10055)
     | > loader_time: 0.04260  (0.04140)


[1m   --> STEP: 57/161 -- GLOBAL_STEP: 6375[0m
     | > loss_gen: 2.61293  (2.78876)
     | > loss_kl: 1.46218  (1.52773)
     | > loss_feat: 5.56511  (5.28518)
     | > loss_mel: 25.13653  (25.59676)
     | > loss_duration: 1.68570  (1.71160)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 36.46245  (36.91002)
     | > grad_norm_0: 90.44313  (93.30917)
     | > loss_disc: 2.01334  (2.16642)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.01334  (2.16642)
     | > grad_norm_1: 5.95987  (15.57669)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.04370  (2.08249)
     | > loader_time: 0.03600  (0.04171)


[1m   --> STEP: 82/161 -- GLOBAL_STEP: 6400[0m
     | > loss_gen: 3.02514  (2.78648)
     | > loss_kl: 1.47154  (1.51974)
     | > loss_feat: 5.13889  (5.29257)
     | > loss_mel: 24.47812  (25.49757)
     | > loss_duration: 1.76422  (1.71553)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 35.87790  (36.81188)
     | > grad_norm_0: 52.09810  (92.17054)
     | > loss_disc: 2.14200  (2.13192)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.14200  (2.13192)
     | > grad_norm_1: 17.14226  (14.82291)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.58630  (2.05787)
     | > loader_time: 0.03060  (0.04827)


[1m   --> STEP: 107/161 -- GLOBAL_STEP: 6425[0m
     | > loss_gen: 2.79673  (2.80086)
     | > loss_kl: 1.53640  (1.52349)
     | > loss_feat: 5.70071  (5.36889)
     | > loss_mel: 25.30917  (25.44410)
     | > loss_duration: 1.72809  (1.72090)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 37.07109  (36.85824)
     | > grad_norm_0: 80.65465  (86.93023)
     | > loss_disc: 2.07792  (2.10085)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.07792  (2.10085)
     | > grad_norm_1: 15.57629  (13.94267)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.46780  (2.09418)
     | > loader_time: 0.03660  (0.04809)


[1m   --> STEP: 132/161 -- GLOBAL_STEP: 6450[0m
     | > loss_gen: 2.94432  (2.81348)
     | > loss_kl: 1.68175  (1.52209)
     | > loss_feat: 4.95332  (5.39920)
     | > loss_mel: 22.94237  (25.34637)
     | > loss_duration: 1.63407  (1.72144)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 34.15583  (36.80258)
     | > grad_norm_0: 56.27580  (85.87062)
     | > loss_disc: 2.28813  (2.08426)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.28813  (2.08426)
     | > grad_norm_1: 11.99571  (13.23325)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.79670  (2.15064)
     | > loader_time: 0.02950  (0.04960)


[1m   --> STEP: 157/161 -- GLOBAL_STEP: 6475[0m
     | > loss_gen: 2.84326  (2.81497)
     | > loss_kl: 1.55249  (1.53196)
     | > loss_feat: 6.28070  (5.43115)
     | > loss_mel: 24.76419  (25.36248)
     | > loss_duration: 1.69384  (1.71889)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 37.13448  (36.85944)
     | > grad_norm_0: 46.08335  (87.74813)
     | > loss_disc: 1.93735  (2.07244)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.93735  (2.07244)
     | > grad_norm_1: 8.79867  (13.81044)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.13830  (2.14415)
     | > loader_time: 0.03720  (0.05114)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02957 [0m(+0.00099)
     | > avg_loss_gen:[91m 2.87397 [0m(+0.00692)
     | > avg_loss_kl:[92m 1.49044 [0m(-0.18110)
     | > avg_loss_feat:[91m 6.12069 [0m(+0.56511)
     | > avg_loss_mel:[91m 26.66131 [0m(+1.61309)
     | > avg_loss_duration:[91m 1.72304 [0m(+0.02571)
     | > avg_loss_0:[91m 38.86946 [0m(+2.02974)
     | > avg_loss_disc:[92m 1.83075 [0m(-0.79267)
     | > avg_loss_1:[92m 1.83075 [0m(-0.79267)


[4m[1m > EPOCH: 40/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 20:19:09) [0m

[1m   --> STEP: 20/161 -- GLOBAL_STEP: 6500[0m
     | > loss_gen: 2.79838  (2.84792)
     | > loss_kl: 1.68827  (1.56017)
     | > loss_feat: 5.40346  (5.62523)
     | > loss_mel: 24.87564  (25.58580)
     | > loss_duration: 1.69125  (1.71959)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 36.45700  (37.33871)
     | > grad_norm_0: 87.49352  (134.90976)
     | > loss_disc: 1.98081  (2.01286)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.98081  (2.01286)
     | > grad_norm_1: 9.15526  (12.62715)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.88910  (2.07021)
     | > loader_time: 0.03170  (0.05011)


[1m   --> STEP: 45/161 -- GLOBAL_STEP: 6525[0m
     | > loss_gen: 2.68115  (2.85092)
     | > loss_kl: 1.60475  (1.53079)
     | > loss_feat: 5.83996  (5.58949)
     | > loss_mel: 26.63928  (25.39989)
     | > loss_duration: 1.76691  (1.71654)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 38.53204  (37.08763)
     | > grad_norm_0: 114.29556  (114.84829)
     | > loss_disc: 1.96015  (2.05376)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.96015  (2.05376)
     | > grad_norm_1: 7.00403  (11.60578)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.77250  (2.10133)
     | > loader_time: 0.04500  (0.04262)


[1m   --> STEP: 70/161 -- GLOBAL_STEP: 6550[0m
     | > loss_gen: 2.32965  (2.85059)
     | > loss_kl: 1.57904  (1.53135)
     | > loss_feat: 5.57776  (5.60653)
     | > loss_mel: 24.91848  (25.45348)
     | > loss_duration: 1.70932  (1.71760)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 36.11424  (37.15955)
     | > grad_norm_0: 136.90236  (103.55654)
     | > loss_disc: 2.19617  (2.03810)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.19617  (2.03810)
     | > grad_norm_1: 18.77111  (11.34372)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.81110  (2.07820)
     | > loader_time: 0.07770  (0.04525)


[1m   --> STEP: 95/161 -- GLOBAL_STEP: 6575[0m
     | > loss_gen: 3.06080  (2.85511)
     | > loss_kl: 1.60497  (1.53140)
     | > loss_feat: 6.02767  (5.59602)
     | > loss_mel: 27.05740  (25.35310)
     | > loss_duration: 1.74722  (1.71690)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 39.49805  (37.05253)
     | > grad_norm_0: 92.41279  (101.81615)
     | > loss_disc: 2.01214  (2.03191)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.01214  (2.03191)
     | > grad_norm_1: 7.89890  (12.57896)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.05240  (2.09717)
     | > loader_time: 0.03730  (0.04694)


[1m   --> STEP: 120/161 -- GLOBAL_STEP: 6600[0m
     | > loss_gen: 2.66439  (2.84872)
     | > loss_kl: 1.51432  (1.53768)
     | > loss_feat: 5.43815  (5.57790)
     | > loss_mel: 25.11870  (25.40761)
     | > loss_duration: 1.68346  (1.72002)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 36.41903  (37.09192)
     | > grad_norm_0: 124.39148  (98.99432)
     | > loss_disc: 2.18400  (2.03612)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.18400  (2.03612)
     | > grad_norm_1: 16.90538  (12.85158)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.26240  (2.12005)
     | > loader_time: 0.03900  (0.04585)


[1m   --> STEP: 145/161 -- GLOBAL_STEP: 6625[0m
     | > loss_gen: 2.79108  (2.84234)
     | > loss_kl: 1.71421  (1.54112)
     | > loss_feat: 5.68848  (5.55324)
     | > loss_mel: 24.13531  (25.31599)
     | > loss_duration: 1.78601  (1.71869)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 36.11509  (36.97137)
     | > grad_norm_0: 42.11241  (100.33408)
     | > loss_disc: 2.00359  (2.04143)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.00359  (2.04143)
     | > grad_norm_1: 7.08155  (12.87846)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.53660  (2.13848)
     | > loader_time: 0.17040  (0.04888)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02980 [0m(+0.00023)
     | > avg_loss_gen:[92m 2.85227 [0m(-0.02170)
     | > avg_loss_kl:[92m 1.46105 [0m(-0.02939)
     | > avg_loss_feat:[91m 6.15181 [0m(+0.03112)
     | > avg_loss_mel:[92m 24.10358 [0m(-2.55773)
     | > avg_loss_duration:[92m 1.69599 [0m(-0.02705)
     | > avg_loss_0:[92m 36.26470 [0m(-2.60476)
     | > avg_loss_disc:[91m 1.87711 [0m(+0.04637)
     | > avg_loss_1:[91m 1.87711 [0m(+0.04637)


[4m[1m > EPOCH: 41/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 20:25:22) [0m

[1m   --> STEP: 8/161 -- GLOBAL_STEP: 6650[0m
     | > loss_gen: 2.68725  (2.75960)
     | > loss_kl: 1.78371  (1.50612)
     | > loss_feat: 5.21861  (5.19432)
     | > loss_mel: 26.42471  (25.02554)
     | > loss_duration: 1.63800  (1.67500)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 37.75229  (36.16059)
     | > grad_norm_0: 131.84688  (101.10173)
     | > loss_disc: 2.19361  (2.13279)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.19361  (2.13279)
     | > grad_norm_1: 15.15807  (12.63747)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.97840  (2.03441)
     | > loader_time: 0.03080  (0.04333)


[1m   --> STEP: 33/161 -- GLOBAL_STEP: 6675[0m
     | > loss_gen: 3.13902  (2.79716)
     | > loss_kl: 1.56830  (1.53039)
     | > loss_feat: 6.27208  (5.38340)
     | > loss_mel: 28.59618  (25.13284)
     | > loss_duration: 1.65156  (1.71041)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 41.22715  (36.55420)
     | > grad_norm_0: 49.40637  (101.72440)
     | > loss_disc: 1.86688  (2.05118)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.86688  (2.05118)
     | > grad_norm_1: 21.31376  (17.12495)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.10830  (2.10285)
     | > loader_time: 0.03310  (0.04283)


[1m   --> STEP: 58/161 -- GLOBAL_STEP: 6700[0m
     | > loss_gen: 2.91740  (2.81381)
     | > loss_kl: 1.52581  (1.53415)
     | > loss_feat: 6.12599  (5.46549)
     | > loss_mel: 25.18462  (25.34586)
     | > loss_duration: 1.74186  (1.70817)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 37.49569  (36.86747)
     | > grad_norm_0: 47.00792  (101.92056)
     | > loss_disc: 1.82876  (2.05016)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.82876  (2.05016)
     | > grad_norm_1: 10.57784  (16.15019)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.86010  (2.09981)
     | > loader_time: 0.03350  (0.03961)


[1m   --> STEP: 83/161 -- GLOBAL_STEP: 6725[0m
     | > loss_gen: 2.71044  (2.83074)
     | > loss_kl: 1.47869  (1.54731)
     | > loss_feat: 5.95571  (5.50967)
     | > loss_mel: 24.30893  (25.33511)
     | > loss_duration: 1.76135  (1.70974)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 36.21511  (36.93257)
     | > grad_norm_0: 114.40768  (100.90388)
     | > loss_disc: 2.10069  (2.04072)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.10069  (2.04072)
     | > grad_norm_1: 22.84266  (16.33220)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.35410  (2.07292)
     | > loader_time: 0.03910  (0.04220)


[1m   --> STEP: 108/161 -- GLOBAL_STEP: 6750[0m
     | > loss_gen: 2.80157  (2.83335)
     | > loss_kl: 1.77793  (1.55102)
     | > loss_feat: 6.18032  (5.53782)
     | > loss_mel: 26.00266  (25.25604)
     | > loss_duration: 1.65419  (1.71117)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 38.41668  (36.88940)
     | > grad_norm_0: 157.02592  (96.50417)
     | > loss_disc: 1.97281  (2.03415)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.97281  (2.03415)
     | > grad_norm_1: 11.45768  (15.33279)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.28590  (2.08693)
     | > loader_time: 0.04000  (0.04433)


[1m   --> STEP: 133/161 -- GLOBAL_STEP: 6775[0m
     | > loss_gen: 2.99523  (2.84129)
     | > loss_kl: 1.54677  (1.54467)
     | > loss_feat: 5.54428  (5.54252)
     | > loss_mel: 24.66186  (25.24403)
     | > loss_duration: 1.77157  (1.71630)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 36.51971  (36.88882)
     | > grad_norm_0: 63.86676  (95.66087)
     | > loss_disc: 2.07368  (2.03861)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.07368  (2.03861)
     | > grad_norm_1: 14.24535  (15.86167)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.94560  (2.13534)
     | > loader_time: 0.03060  (0.04730)


[1m   --> STEP: 158/161 -- GLOBAL_STEP: 6800[0m
     | > loss_gen: 2.70218  (2.83780)
     | > loss_kl: 1.40268  (1.53837)
     | > loss_feat: 5.76497  (5.53361)
     | > loss_mel: 23.97593  (25.20783)
     | > loss_duration: 1.70079  (1.71536)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 35.54656  (36.83297)
     | > grad_norm_0: 184.91908  (95.61845)
     | > loss_disc: 2.04686  (2.03454)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.04686  (2.03454)
     | > grad_norm_1: 10.13088  (14.97493)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.25900  (2.13163)
     | > loader_time: 0.11980  (0.04696)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02816 [0m(-0.00164)
     | > avg_loss_gen:[91m 3.13584 [0m(+0.28357)
     | > avg_loss_kl:[91m 1.54729 [0m(+0.08624)
     | > avg_loss_feat:[92m 5.82230 [0m(-0.32951)
     | > avg_loss_mel:[91m 25.78978 [0m(+1.68620)
     | > avg_loss_duration:[91m 1.70382 [0m(+0.00783)
     | > avg_loss_0:[91m 37.99903 [0m(+1.73433)
     | > avg_loss_disc:[91m 1.93076 [0m(+0.05365)
     | > avg_loss_1:[91m 1.93076 [0m(+0.05365)


[4m[1m > EPOCH: 42/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 20:31:34) [0m

[1m   --> STEP: 21/161 -- GLOBAL_STEP: 6825[0m
     | > loss_gen: 2.84752  (2.83807)
     | > loss_kl: 1.59480  (1.49737)
     | > loss_feat: 5.63602  (5.46651)
     | > loss_mel: 25.93319  (24.79630)
     | > loss_duration: 1.78960  (1.71311)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 37.80113  (36.31136)
     | > grad_norm_0: 94.49714  (95.34740)
     | > loss_disc: 2.04616  (2.01924)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.04616  (2.01924)
     | > grad_norm_1: 6.19919  (14.94566)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.05760  (2.11300)
     | > loader_time: 0.03470  (0.04561)


[1m   --> STEP: 46/161 -- GLOBAL_STEP: 6850[0m
     | > loss_gen: 3.01163  (2.82323)
     | > loss_kl: 1.88726  (1.53701)
     | > loss_feat: 5.18450  (5.51179)
     | > loss_mel: 24.66447  (24.88848)
     | > loss_duration: 1.73906  (1.72002)
     | > amp_scaler: 256.00000  (395.13043)
     | > loss_0: 36.48692  (36.48053)
     | > grad_norm_0: 73.37023  (88.07383)
     | > loss_disc: 2.11255  (2.03471)
     | > amp_scaler-1: 256.00000  (395.13043)
     | > loss_1: 2.11255  (2.03471)
     | > grad_norm_1: 13.61411  (12.57245)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.31960  (2.12894)
     | > loader_time: 0.17190  (0.04368)


[1m   --> STEP: 71/161 -- GLOBAL_STEP: 6875[0m
     | > loss_gen: 2.88885  (2.83903)
     | > loss_kl: 1.63457  (1.53687)
     | > loss_feat: 5.60278  (5.55138)
     | > loss_mel: 25.55556  (25.00451)
     | > loss_duration: 1.63200  (1.71348)
     | > amp_scaler: 256.00000  (346.14085)
     | > loss_0: 37.31377  (36.64527)
     | > grad_norm_0: 113.42418  (89.37994)
     | > loss_disc: 1.92926  (2.02113)
     | > amp_scaler-1: 256.00000  (346.14085)
     | > loss_1: 1.92926  (2.02113)
     | > grad_norm_1: 18.55070  (12.80240)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.42270  (2.08960)
     | > loader_time: 0.03550  (0.05489)


[1m   --> STEP: 96/161 -- GLOBAL_STEP: 6900[0m
     | > loss_gen: 2.56283  (2.83806)
     | > loss_kl: 1.38083  (1.55278)
     | > loss_feat: 5.97284  (5.56054)
     | > loss_mel: 23.41615  (25.02105)
     | > loss_duration: 1.81639  (1.71331)
     | > amp_scaler: 256.00000  (322.66667)
     | > loss_0: 35.14903  (36.68573)
     | > grad_norm_0: 67.26247  (89.75902)
     | > loss_disc: 1.95832  (2.02397)
     | > amp_scaler-1: 256.00000  (322.66667)
     | > loss_1: 1.95832  (2.02397)
     | > grad_norm_1: 10.98402  (13.87950)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.85050  (2.11935)
     | > loader_time: 0.04490  (0.05073)


[1m   --> STEP: 121/161 -- GLOBAL_STEP: 6925[0m
     | > loss_gen: 2.94340  (2.83934)
     | > loss_kl: 1.58246  (1.55215)
     | > loss_feat: 5.09572  (5.56810)
     | > loss_mel: 25.99372  (25.04266)
     | > loss_duration: 1.70708  (1.71135)
     | > amp_scaler: 256.00000  (308.89256)
     | > loss_0: 37.32238  (36.71360)
     | > grad_norm_0: 108.28686  (88.11694)
     | > loss_disc: 2.16144  (2.02229)
     | > amp_scaler-1: 256.00000  (308.89256)
     | > loss_1: 2.16144  (2.02229)
     | > grad_norm_1: 9.00421  (13.65367)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.52080  (2.14544)
     | > loader_time: 0.06810  (0.05024)


[1m   --> STEP: 146/161 -- GLOBAL_STEP: 6950[0m
     | > loss_gen: 3.00235  (2.84671)
     | > loss_kl: 1.50899  (1.54625)
     | > loss_feat: 5.98178  (5.58954)
     | > loss_mel: 25.44445  (25.07921)
     | > loss_duration: 1.75017  (1.71241)
     | > amp_scaler: 256.00000  (299.83562)
     | > loss_0: 37.68774  (36.77412)
     | > grad_norm_0: 55.12667  (90.42606)
     | > loss_disc: 2.02020  (2.01602)
     | > amp_scaler-1: 256.00000  (299.83562)
     | > loss_1: 2.02020  (2.01602)
     | > grad_norm_1: 6.62057  (13.25173)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.33210  (2.15799)
     | > loader_time: 0.16700  (0.04972)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02907 [0m(+0.00091)
     | > avg_loss_gen:[91m 3.20200 [0m(+0.06616)
     | > avg_loss_kl:[91m 1.66554 [0m(+0.11825)
     | > avg_loss_feat:[91m 6.18861 [0m(+0.36631)
     | > avg_loss_mel:[91m 26.41519 [0m(+0.62541)
     | > avg_loss_duration:[92m 1.69637 [0m(-0.00745)
     | > avg_loss_0:[91m 39.16771 [0m(+1.16868)
     | > avg_loss_disc:[92m 1.78465 [0m(-0.14611)
     | > avg_loss_1:[92m 1.78465 [0m(-0.14611)


[4m[1m > EPOCH: 43/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 20:37:42) [0m

[1m   --> STEP: 9/161 -- GLOBAL_STEP: 6975[0m
     | > loss_gen: 3.03286  (2.83811)
     | > loss_kl: 1.56707  (1.58607)
     | > loss_feat: 5.53043  (5.77973)
     | > loss_mel: 25.17565  (25.69696)
     | > loss_duration: 1.69523  (1.71451)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.00124  (37.61538)
     | > grad_norm_0: 151.80389  (118.04644)
     | > loss_disc: 1.85709  (2.02716)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.85709  (2.02716)
     | > grad_norm_1: 11.88129  (12.04874)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.70370  (2.02516)
     | > loader_time: 0.09410  (0.05740)


[1m   --> STEP: 34/161 -- GLOBAL_STEP: 7000[0m
     | > loss_gen: 3.00027  (2.84105)
     | > loss_kl: 1.39572  (1.58081)
     | > loss_feat: 5.42754  (5.63436)
     | > loss_mel: 23.79957  (25.40350)
     | > loss_duration: 1.83286  (1.71566)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.45595  (37.17539)
     | > grad_norm_0: 133.35905  (102.30333)
     | > loss_disc: 1.93884  (2.03571)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.93884  (2.03571)
     | > grad_norm_1: 9.62009  (14.91895)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.85520  (2.10380)
     | > loader_time: 0.04250  (0.04237)


[1m   --> STEP: 59/161 -- GLOBAL_STEP: 7025[0m
     | > loss_gen: 3.00465  (2.83405)
     | > loss_kl: 1.63835  (1.56756)
     | > loss_feat: 5.51080  (5.59065)
     | > loss_mel: 26.48040  (25.36013)
     | > loss_duration: 1.66730  (1.71169)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.30150  (37.06408)
     | > grad_norm_0: 84.19299  (95.67685)
     | > loss_disc: 2.12768  (2.04410)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.12768  (2.04410)
     | > grad_norm_1: 25.04657  (15.86957)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.85370  (2.09629)
     | > loader_time: 0.02960  (0.04126)


[1m   --> STEP: 84/161 -- GLOBAL_STEP: 7050[0m
     | > loss_gen: 2.74389  (2.82722)
     | > loss_kl: 1.41083  (1.55327)
     | > loss_feat: 5.35894  (5.55396)
     | > loss_mel: 24.59274  (25.26496)
     | > loss_duration: 1.70661  (1.70947)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.81300  (36.90888)
     | > grad_norm_0: 139.24619  (91.82194)
     | > loss_disc: 2.17771  (2.04494)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.17771  (2.04494)
     | > grad_norm_1: 16.01365  (15.81061)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.98310  (2.07249)
     | > loader_time: 0.03720  (0.04735)


[1m   --> STEP: 109/161 -- GLOBAL_STEP: 7075[0m
     | > loss_gen: 2.75682  (2.82862)
     | > loss_kl: 1.49334  (1.55947)
     | > loss_feat: 6.06234  (5.53631)
     | > loss_mel: 24.94803  (25.26325)
     | > loss_duration: 1.75753  (1.71382)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.01805  (36.90147)
     | > grad_norm_0: 84.42148  (91.13020)
     | > loss_disc: 2.03600  (2.04552)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.03600  (2.04552)
     | > grad_norm_1: 8.46140  (15.21033)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.54470  (2.08439)
     | > loader_time: 0.04790  (0.04889)


[1m   --> STEP: 134/161 -- GLOBAL_STEP: 7100[0m
     | > loss_gen: 3.06674  (2.83348)
     | > loss_kl: 1.61491  (1.56746)
     | > loss_feat: 5.53351  (5.55557)
     | > loss_mel: 25.90230  (25.28655)
     | > loss_duration: 1.67004  (1.71519)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.78751  (36.95824)
     | > grad_norm_0: 82.32298  (92.67796)
     | > loss_disc: 2.07036  (2.04174)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.07036  (2.04174)
     | > grad_norm_1: 11.80526  (15.37346)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.68150  (2.12770)
     | > loader_time: 0.10870  (0.05229)


[1m   --> STEP: 159/161 -- GLOBAL_STEP: 7125[0m
     | > loss_gen: 2.91415  (2.83753)
     | > loss_kl: 1.48181  (1.56562)
     | > loss_feat: 5.01259  (5.56912)
     | > loss_mel: 25.19234  (25.26518)
     | > loss_duration: 1.71680  (1.71332)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.31768  (36.95077)
     | > grad_norm_0: 203.35965  (92.16259)
     | > loss_disc: 2.09605  (2.03735)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.09605  (2.03735)
     | > grad_norm_1: 33.48734  (15.07803)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.90980  (2.12342)
     | > loader_time: 0.03630  (0.05208)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02718 [0m(-0.00189)
     | > avg_loss_gen:[92m 2.53925 [0m(-0.66275)
     | > avg_loss_kl:[92m 1.58967 [0m(-0.07587)
     | > avg_loss_feat:[92m 5.90765 [0m(-0.28096)
     | > avg_loss_mel:[92m 24.10729 [0m(-2.30790)
     | > avg_loss_duration:[92m 1.69070 [0m(-0.00567)
     | > avg_loss_0:[92m 35.83456 [0m(-3.33316)
     | > avg_loss_disc:[91m 1.99500 [0m(+0.21036)
     | > avg_loss_1:[91m 1.99500 [0m(+0.21036)


[4m[1m > EPOCH: 44/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 20:43:51) [0m

[1m   --> STEP: 22/161 -- GLOBAL_STEP: 7150[0m
     | > loss_gen: 2.46006  (2.82900)
     | > loss_kl: 1.59240  (1.56494)
     | > loss_feat: 4.78222  (5.45990)
     | > loss_mel: 26.27066  (24.86445)
     | > loss_duration: 1.66961  (1.72159)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.77496  (36.43990)
     | > grad_norm_0: 165.61040  (101.63345)
     | > loss_disc: 2.24209  (2.04163)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.24209  (2.04163)
     | > grad_norm_1: 31.40375  (16.33738)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.93350  (2.06072)
     | > loader_time: 0.03130  (0.05259)


[1m   --> STEP: 47/161 -- GLOBAL_STEP: 7175[0m
     | > loss_gen: 2.92164  (2.83411)
     | > loss_kl: 1.62047  (1.57113)
     | > loss_feat: 6.77282  (5.55002)
     | > loss_mel: 25.62018  (25.04110)
     | > loss_duration: 1.68961  (1.71956)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.62471  (36.71592)
     | > grad_norm_0: 137.15695  (98.56776)
     | > loss_disc: 1.88208  (2.02611)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.88208  (2.02611)
     | > grad_norm_1: 22.04214  (15.50958)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.05930  (2.08956)
     | > loader_time: 0.03260  (0.04799)


[1m   --> STEP: 72/161 -- GLOBAL_STEP: 7200[0m
     | > loss_gen: 2.75692  (2.83898)
     | > loss_kl: 1.48595  (1.56499)
     | > loss_feat: 4.93127  (5.56749)
     | > loss_mel: 25.74872  (25.03899)
     | > loss_duration: 1.70979  (1.71604)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.63265  (36.72649)
     | > grad_norm_0: 85.81472  (104.48672)
     | > loss_disc: 2.66737  (2.07582)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.66737  (2.07582)
     | > grad_norm_1: 14.11803  (16.86884)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.79600  (2.07018)
     | > loader_time: 0.03290  (0.04692)


[1m   --> STEP: 97/161 -- GLOBAL_STEP: 7225[0m
     | > loss_gen: 2.92760  (2.82621)
     | > loss_kl: 1.63546  (1.56375)
     | > loss_feat: 5.14248  (5.51816)
     | > loss_mel: 24.22696  (25.10226)
     | > loss_duration: 1.83212  (1.72076)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.76461  (36.73115)
     | > grad_norm_0: 66.87841  (100.92153)
     | > loss_disc: 2.26214  (2.09677)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.26214  (2.09677)
     | > grad_norm_1: 12.58268  (16.01223)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.32960  (2.09778)
     | > loader_time: 0.04360  (0.05107)


[1m   --> STEP: 122/161 -- GLOBAL_STEP: 7250[0m
     | > loss_gen: 2.55691  (2.82270)
     | > loss_kl: 1.36471  (1.56247)
     | > loss_feat: 4.93845  (5.50651)
     | > loss_mel: 23.71033  (25.08947)
     | > loss_duration: 1.67185  (1.72045)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.24224  (36.70160)
     | > grad_norm_0: 87.13070  (100.72259)
     | > loss_disc: 2.12618  (2.08249)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.12618  (2.08249)
     | > grad_norm_1: 12.60453  (14.74033)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.56600  (2.13032)
     | > loader_time: 0.04640  (0.05230)


[1m   --> STEP: 147/161 -- GLOBAL_STEP: 7275[0m
     | > loss_gen: 2.81832  (2.83473)
     | > loss_kl: 1.52390  (1.56225)
     | > loss_feat: 5.99100  (5.54139)
     | > loss_mel: 25.41111  (25.10399)
     | > loss_duration: 1.70153  (1.72128)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.44586  (36.76364)
     | > grad_norm_0: 79.67334  (100.24899)
     | > loss_disc: 1.89459  (2.06423)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.89459  (2.06423)
     | > grad_norm_1: 12.39976  (14.46694)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.41590  (2.14083)
     | > loader_time: 0.03960  (0.05208)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02812 [0m(+0.00093)
     | > avg_loss_gen:[91m 3.20709 [0m(+0.66784)
     | > avg_loss_kl:[91m 1.79841 [0m(+0.20874)
     | > avg_loss_feat:[91m 6.29310 [0m(+0.38546)
     | > avg_loss_mel:[91m 24.22515 [0m(+0.11786)
     | > avg_loss_duration:[91m 1.69096 [0m(+0.00026)
     | > avg_loss_0:[91m 37.21473 [0m(+1.38017)
     | > avg_loss_disc:[92m 1.87789 [0m(-0.11711)
     | > avg_loss_1:[92m 1.87789 [0m(-0.11711)


[4m[1m > EPOCH: 45/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 20:49:59) [0m

[1m   --> STEP: 10/161 -- GLOBAL_STEP: 7300[0m
     | > loss_gen: 2.72865  (2.90574)
     | > loss_kl: 1.62155  (1.61062)
     | > loss_feat: 5.55534  (5.82753)
     | > loss_mel: 24.02703  (24.33656)
     | > loss_duration: 1.72441  (1.70444)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.65698  (36.38489)
     | > grad_norm_0: 122.78181  (91.06766)
     | > loss_disc: 1.92267  (1.95384)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.92267  (1.95384)
     | > grad_norm_1: 11.91605  (17.82055)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.13310  (2.08975)
     | > loader_time: 0.02940  (0.05957)


[1m   --> STEP: 35/161 -- GLOBAL_STEP: 7325[0m
     | > loss_gen: 2.93171  (2.84803)
     | > loss_kl: 1.52627  (1.57591)
     | > loss_feat: 5.68880  (5.61036)
     | > loss_mel: 25.03848  (24.80774)
     | > loss_duration: 1.68177  (1.70888)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.86703  (36.55092)
     | > grad_norm_0: 135.05692  (99.89769)
     | > loss_disc: 2.06272  (2.01587)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06272  (2.01587)
     | > grad_norm_1: 8.63041  (16.02859)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.74370  (2.14398)
     | > loader_time: 0.04440  (0.04261)


[1m   --> STEP: 60/161 -- GLOBAL_STEP: 7350[0m
     | > loss_gen: 2.84584  (2.84305)
     | > loss_kl: 1.48181  (1.56288)
     | > loss_feat: 5.63807  (5.59032)
     | > loss_mel: 25.26877  (24.78551)
     | > loss_duration: 1.71592  (1.70643)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.95041  (36.48820)
     | > grad_norm_0: 30.62055  (99.87643)
     | > loss_disc: 2.04754  (2.01291)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.04754  (2.01291)
     | > grad_norm_1: 7.28561  (14.11354)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.10550  (2.11366)
     | > loader_time: 0.03510  (0.04108)


[1m   --> STEP: 85/161 -- GLOBAL_STEP: 7375[0m
     | > loss_gen: 3.03217  (2.86272)
     | > loss_kl: 1.55948  (1.58793)
     | > loss_feat: 5.08641  (5.62849)
     | > loss_mel: 25.38035  (24.86452)
     | > loss_duration: 1.78576  (1.70602)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.84416  (36.64967)
     | > grad_norm_0: 91.60983  (97.61858)
     | > loss_disc: 2.09812  (2.01632)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.09812  (2.01632)
     | > grad_norm_1: 10.70778  (14.11878)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.55100  (2.10424)
     | > loader_time: 0.03890  (0.04833)


[1m   --> STEP: 110/161 -- GLOBAL_STEP: 7400[0m
     | > loss_gen: 2.75545  (2.85569)
     | > loss_kl: 1.70347  (1.57968)
     | > loss_feat: 5.90339  (5.63312)
     | > loss_mel: 26.10297  (24.86798)
     | > loss_duration: 1.63691  (1.70685)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.10219  (36.64331)
     | > grad_norm_0: 56.46582  (95.25268)
     | > loss_disc: 1.90721  (2.02077)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.90721  (2.02077)
     | > grad_norm_1: 8.42653  (13.56201)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.93420  (2.10571)
     | > loader_time: 0.04780  (0.04977)


[1m   --> STEP: 135/161 -- GLOBAL_STEP: 7425[0m
     | > loss_gen: 2.75729  (2.84909)
     | > loss_kl: 1.40129  (1.57526)
     | > loss_feat: 5.59461  (5.61260)
     | > loss_mel: 25.83899  (24.81335)
     | > loss_duration: 1.68824  (1.70811)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.28042  (36.55841)
     | > grad_norm_0: 134.40082  (92.78017)
     | > loss_disc: 1.98598  (2.02158)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.98598  (2.02158)
     | > grad_norm_1: 18.15549  (13.89369)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.13620  (2.14217)
     | > loader_time: 0.03320  (0.05368)


[1m   --> STEP: 160/161 -- GLOBAL_STEP: 7450[0m
     | > loss_gen: 2.58877  (2.84563)
     | > loss_kl: 1.43870  (1.57285)
     | > loss_feat: 5.63796  (5.59699)
     | > loss_mel: 22.65468  (24.75497)
     | > loss_duration: 1.77247  (1.71055)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.09258  (36.48100)
     | > grad_norm_0: 134.33195  (91.21601)
     | > loss_disc: 1.92730  (2.02106)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.92730  (2.02106)
     | > grad_norm_1: 9.39579  (13.74622)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.10250  (2.14228)
     | > loader_time: 0.03800  (0.05225)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02796 [0m(-0.00016)
     | > avg_loss_gen:[92m 3.03218 [0m(-0.17491)
     | > avg_loss_kl:[92m 1.59117 [0m(-0.20725)
     | > avg_loss_feat:[92m 5.19588 [0m(-1.09722)
     | > avg_loss_mel:[92m 22.88186 [0m(-1.34330)
     | > avg_loss_duration:[91m 1.69516 [0m(+0.00420)
     | > avg_loss_0:[92m 34.39625 [0m(-2.81847)
     | > avg_loss_disc:[91m 2.09248 [0m(+0.21459)
     | > avg_loss_1:[91m 2.09248 [0m(+0.21459)

 > BEST MODEL : /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000/best_model_7452.pth.tar

[4m[1m > EPOCH: 46/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 20:56:13) [0m

[1m   --> STEP: 23/161 -- GLOBAL_STEP: 7475[0m
     | > loss_gen: 2.67216  (2.85395)
     | > loss_kl: 1.68828  (1.61610)
     | > loss_feat: 5.98144  (5.70106)
     | > loss_mel: 26.10309  (24.75686)
     | > loss_duration: 1.65750  (1.69894)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.10247  (36.62691)
     | > grad_norm_0: 136.12828  (102.75431)
     | > loss_disc: 2.00470  (2.04103)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.00470  (2.04103)
     | > grad_norm_1: 12.60545  (14.34788)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.00770  (2.10731)
     | > loader_time: 0.03170  (0.06592)


[1m   --> STEP: 48/161 -- GLOBAL_STEP: 7500[0m
     | > loss_gen: 2.69860  (2.83410)
     | > loss_kl: 1.54400  (1.59644)
     | > loss_feat: 5.19644  (5.58800)
     | > loss_mel: 24.69118  (24.81436)
     | > loss_duration: 1.65786  (1.70665)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.78808  (36.53954)
     | > grad_norm_0: 258.33670  (95.09518)
     | > loss_disc: 1.99058  (2.03279)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.99058  (2.03279)
     | > grad_norm_1: 8.77269  (15.36462)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.82300  (2.11391)
     | > loader_time: 0.03340  (0.05333)


[1m   --> STEP: 73/161 -- GLOBAL_STEP: 7525[0m
     | > loss_gen: 2.72653  (2.83714)
     | > loss_kl: 1.59613  (1.60778)
     | > loss_feat: 5.10396  (5.59065)
     | > loss_mel: 24.28187  (24.68600)
     | > loss_duration: 1.73646  (1.70342)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.44495  (36.42500)
     | > grad_norm_0: 49.31829  (99.05589)
     | > loss_disc: 2.04364  (2.02742)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.04364  (2.02742)
     | > grad_norm_1: 8.18769  (14.93775)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.30650  (2.09536)
     | > loader_time: 0.03570  (0.05320)


[1m   --> STEP: 98/161 -- GLOBAL_STEP: 7550[0m
     | > loss_gen: 2.97885  (2.84185)
     | > loss_kl: 1.83701  (1.60876)
     | > loss_feat: 6.21271  (5.61914)
     | > loss_mel: 26.52793  (24.71012)
     | > loss_duration: 1.68024  (1.70543)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 39.23674  (36.48530)
     | > grad_norm_0: 131.26749  (98.57827)
     | > loss_disc: 1.78222  (2.02496)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.78222  (2.02496)
     | > grad_norm_1: 7.72476  (14.76154)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.96850  (2.10908)
     | > loader_time: 0.16100  (0.05510)


[1m   --> STEP: 123/161 -- GLOBAL_STEP: 7575[0m
     | > loss_gen: 2.45732  (2.83581)
     | > loss_kl: 1.37424  (1.60587)
     | > loss_feat: 5.26787  (5.59619)
     | > loss_mel: 23.53502  (24.70977)
     | > loss_duration: 1.78974  (1.70979)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.42419  (36.45742)
     | > grad_norm_0: 38.04266  (97.45415)
     | > loss_disc: 2.23024  (2.02897)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.23024  (2.02897)
     | > grad_norm_1: 14.54397  (14.96632)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.93990  (2.14274)
     | > loader_time: 0.05460  (0.05650)


[1m   --> STEP: 148/161 -- GLOBAL_STEP: 7600[0m
     | > loss_gen: 3.01872  (2.84670)
     | > loss_kl: 1.65987  (1.60067)
     | > loss_feat: 5.37994  (5.62034)
     | > loss_mel: 23.06079  (24.69088)
     | > loss_duration: 1.77520  (1.71182)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.89452  (36.47041)
     | > grad_norm_0: 52.01344  (92.38573)
     | > loss_disc: 2.04292  (2.01961)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.04292  (2.01961)
     | > grad_norm_1: 9.89511  (14.90972)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.37990  (2.14666)
     | > loader_time: 0.04060  (0.05450)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02942 [0m(+0.00146)
     | > avg_loss_gen:[92m 2.86431 [0m(-0.16787)
     | > avg_loss_kl:[91m 1.71771 [0m(+0.12654)
     | > avg_loss_feat:[91m 6.13724 [0m(+0.94136)
     | > avg_loss_mel:[91m 24.07162 [0m(+1.18976)
     | > avg_loss_duration:[91m 1.70784 [0m(+0.01268)
     | > avg_loss_0:[91m 36.49871 [0m(+2.10246)
     | > avg_loss_disc:[92m 1.98315 [0m(-0.10933)
     | > avg_loss_1:[92m 1.98315 [0m(-0.10933)


[4m[1m > EPOCH: 47/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 21:02:22) [0m

[1m   --> STEP: 11/161 -- GLOBAL_STEP: 7625[0m
     | > loss_gen: 3.17744  (2.93339)
     | > loss_kl: 1.78584  (1.60622)
     | > loss_feat: 5.50430  (5.74984)
     | > loss_mel: 23.26989  (24.74485)
     | > loss_duration: 1.66597  (1.70406)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.40345  (36.73837)
     | > grad_norm_0: 61.96339  (75.51415)
     | > loss_disc: 2.04598  (2.03607)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.04598  (2.03607)
     | > grad_norm_1: 8.81757  (10.83685)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.64000  (2.01784)
     | > loader_time: 0.02750  (0.06214)


[1m   --> STEP: 36/161 -- GLOBAL_STEP: 7650[0m
     | > loss_gen: 2.90999  (2.89386)
     | > loss_kl: 1.48426  (1.60042)
     | > loss_feat: 5.81901  (5.74565)
     | > loss_mel: 24.53679  (24.40674)
     | > loss_duration: 1.74245  (1.71309)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.49250  (36.35976)
     | > grad_norm_0: 123.59763  (79.05125)
     | > loss_disc: 1.86747  (1.98300)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.86747  (1.98300)
     | > grad_norm_1: 14.65573  (12.54675)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.31340  (2.13229)
     | > loader_time: 0.04380  (0.05432)


[1m   --> STEP: 61/161 -- GLOBAL_STEP: 7675[0m
     | > loss_gen: 2.93017  (2.87929)
     | > loss_kl: 1.55576  (1.58823)
     | > loss_feat: 4.95273  (5.70113)
     | > loss_mel: 23.29209  (24.58698)
     | > loss_duration: 1.68605  (1.70442)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.41681  (36.46005)
     | > grad_norm_0: 209.22919  (91.25873)
     | > loss_disc: 2.06533  (1.99236)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06533  (1.99236)
     | > grad_norm_1: 24.57541  (13.76641)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.63680  (2.10637)
     | > loader_time: 0.03530  (0.04901)


[1m   --> STEP: 86/161 -- GLOBAL_STEP: 7700[0m
     | > loss_gen: 2.75538  (2.87229)
     | > loss_kl: 1.70723  (1.58806)
     | > loss_feat: 5.85460  (5.68255)
     | > loss_mel: 24.76161  (24.51060)
     | > loss_duration: 1.67120  (1.70425)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.75002  (36.35774)
     | > grad_norm_0: 121.67066  (87.75315)
     | > loss_disc: 2.03753  (1.99834)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.03753  (1.99834)
     | > grad_norm_1: 15.65472  (13.79388)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.53720  (2.09375)
     | > loader_time: 0.03160  (0.04913)


[1m   --> STEP: 111/161 -- GLOBAL_STEP: 7725[0m
     | > loss_gen: 2.79660  (2.86996)
     | > loss_kl: 1.43711  (1.58426)
     | > loss_feat: 5.07312  (5.68924)
     | > loss_mel: 23.25479  (24.54254)
     | > loss_duration: 1.75192  (1.70946)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.31353  (36.39545)
     | > grad_norm_0: 72.34953  (85.85644)
     | > loss_disc: 2.10520  (2.01034)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.10520  (2.01034)
     | > grad_norm_1: 12.31098  (13.95817)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.48810  (2.11616)
     | > loader_time: 0.05690  (0.04768)


[1m   --> STEP: 136/161 -- GLOBAL_STEP: 7750[0m
     | > loss_gen: 2.47773  (2.86407)
     | > loss_kl: 1.56363  (1.58895)
     | > loss_feat: 5.27840  (5.65268)
     | > loss_mel: 23.77892  (24.55679)
     | > loss_duration: 1.61366  (1.70906)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.71234  (36.37155)
     | > grad_norm_0: 44.99064  (87.01046)
     | > loss_disc: 1.98404  (2.01537)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.98404  (2.01537)
     | > grad_norm_1: 15.50797  (13.90194)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.13470  (2.14966)
     | > loader_time: 0.03780  (0.05238)


[1m   --> STEP: 161/161 -- GLOBAL_STEP: 7775[0m
     | > loss_gen: 3.03211  (2.86961)
     | > loss_kl: 1.78090  (1.58949)
     | > loss_feat: 5.93741  (5.67839)
     | > loss_mel: 23.76539  (24.54688)
     | > loss_duration: 1.78165  (1.71069)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.29747  (36.39505)
     | > grad_norm_0: 49.97520  (88.94177)
     | > loss_disc: 1.93995  (2.01290)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.93995  (2.01290)
     | > grad_norm_1: 11.68443  (13.49493)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.18220  (2.14264)
     | > loader_time: 0.02420  (0.05180)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02820 [0m(-0.00122)
     | > avg_loss_gen:[92m 2.80714 [0m(-0.05716)
     | > avg_loss_kl:[92m 1.68564 [0m(-0.03207)
     | > avg_loss_feat:[92m 5.79428 [0m(-0.34296)
     | > avg_loss_mel:[92m 23.78249 [0m(-0.28913)
     | > avg_loss_duration:[92m 1.68293 [0m(-0.02491)
     | > avg_loss_0:[92m 35.75248 [0m(-0.74623)
     | > avg_loss_disc:[91m 2.04702 [0m(+0.06387)
     | > avg_loss_1:[91m 2.04702 [0m(+0.06387)


[4m[1m > EPOCH: 48/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 21:08:32) [0m

[1m   --> STEP: 24/161 -- GLOBAL_STEP: 7800[0m
     | > loss_gen: 2.67560  (2.83268)
     | > loss_kl: 1.77102  (1.59843)
     | > loss_feat: 5.69846  (5.58745)
     | > loss_mel: 24.35854  (24.53507)
     | > loss_duration: 1.74617  (1.71194)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.24978  (36.26557)
     | > grad_norm_0: 50.33796  (87.74754)
     | > loss_disc: 2.01251  (2.03023)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.01251  (2.03023)
     | > grad_norm_1: 22.38960  (12.03351)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.97580  (2.04232)
     | > loader_time: 0.02940  (0.03946)


[1m   --> STEP: 49/161 -- GLOBAL_STEP: 7825[0m
     | > loss_gen: 2.65764  (2.83081)
     | > loss_kl: 1.67205  (1.58875)
     | > loss_feat: 5.29815  (5.54894)
     | > loss_mel: 23.81066  (24.55879)
     | > loss_duration: 1.70137  (1.71332)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.13986  (36.24061)
     | > grad_norm_0: 165.58104  (84.73218)
     | > loss_disc: 2.01591  (2.03098)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.01591  (2.03098)
     | > grad_norm_1: 16.87896  (12.82012)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.19190  (2.10747)
     | > loader_time: 0.03570  (0.04897)


[1m   --> STEP: 74/161 -- GLOBAL_STEP: 7850[0m
     | > loss_gen: 3.23515  (2.85007)
     | > loss_kl: 1.50334  (1.57628)
     | > loss_feat: 5.96159  (5.60680)
     | > loss_mel: 23.99253  (24.57080)
     | > loss_duration: 1.68551  (1.71059)
     | > amp_scaler: 512.00000  (328.64865)
     | > loss_0: 36.37812  (36.31454)
     | > grad_norm_0: 40.25926  (82.49218)
     | > loss_disc: 1.97168  (2.01764)
     | > amp_scaler-1: 512.00000  (328.64865)
     | > loss_1: 1.97168  (2.01764)
     | > grad_norm_1: 13.01730  (13.29193)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.81590  (2.06980)
     | > loader_time: 0.03170  (0.04849)


[1m   --> STEP: 99/161 -- GLOBAL_STEP: 7875[0m
     | > loss_gen: 2.74793  (2.85480)
     | > loss_kl: 1.84467  (1.57677)
     | > loss_feat: 5.71706  (5.63058)
     | > loss_mel: 26.64439  (24.52044)
     | > loss_duration: 1.68736  (1.70852)
     | > amp_scaler: 512.00000  (374.94949)
     | > loss_0: 38.64141  (36.29112)
     | > grad_norm_0: 156.66801  (83.35638)
     | > loss_disc: 2.05271  (2.01076)
     | > amp_scaler-1: 512.00000  (374.94949)
     | > loss_1: 2.05271  (2.01076)
     | > grad_norm_1: 10.09273  (13.51939)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.84340  (2.09224)
     | > loader_time: 0.02990  (0.04678)


[1m   --> STEP: 124/161 -- GLOBAL_STEP: 7900[0m
     | > loss_gen: 2.85396  (2.85706)
     | > loss_kl: 1.66991  (1.57898)
     | > loss_feat: 5.57102  (5.65295)
     | > loss_mel: 24.53100  (24.56261)
     | > loss_duration: 1.66348  (1.70850)
     | > amp_scaler: 512.00000  (402.58065)
     | > loss_0: 36.28938  (36.36009)
     | > grad_norm_0: 80.07189  (88.75794)
     | > loss_disc: 2.01297  (2.01492)
     | > amp_scaler-1: 512.00000  (402.58065)
     | > loss_1: 2.01297  (2.01492)
     | > grad_norm_1: 16.53071  (13.14523)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.01430  (2.12953)
     | > loader_time: 0.03710  (0.04794)


[1m   --> STEP: 149/161 -- GLOBAL_STEP: 7925[0m
     | > loss_gen: 2.99436  (2.85442)
     | > loss_kl: 1.75976  (1.58512)
     | > loss_feat: 6.21558  (5.63486)
     | > loss_mel: 24.36301  (24.54062)
     | > loss_duration: 1.70452  (1.71004)
     | > amp_scaler: 512.00000  (420.93960)
     | > loss_0: 37.03724  (36.32507)
     | > grad_norm_0: 32.75727  (89.82299)
     | > loss_disc: 1.89707  (2.01701)
     | > amp_scaler-1: 512.00000  (420.93960)
     | > loss_1: 1.89707  (2.01701)
     | > grad_norm_1: 8.40152  (13.43502)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.93920  (2.13587)
     | > loader_time: 0.03510  (0.04788)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02824 [0m(+0.00003)
     | > avg_loss_gen:[92m 2.72460 [0m(-0.08255)
     | > avg_loss_kl:[91m 1.81250 [0m(+0.12687)
     | > avg_loss_feat:[92m 5.31925 [0m(-0.47503)
     | > avg_loss_mel:[92m 22.82290 [0m(-0.95959)
     | > avg_loss_duration:[91m 1.68520 [0m(+0.00227)
     | > avg_loss_0:[92m 34.36446 [0m(-1.38803)
     | > avg_loss_disc:[92m 1.93745 [0m(-0.10957)
     | > avg_loss_1:[92m 1.93745 [0m(-0.10957)

 > BEST MODEL : /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000/best_model_7938.pth.tar

[4m[1m > EPOCH: 49/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 21:14:47) [0m

[1m   --> STEP: 12/161 -- GLOBAL_STEP: 7950[0m
     | > loss_gen: 2.98876  (2.82206)
     | > loss_kl: 1.60683  (1.59794)
     | > loss_feat: 5.50311  (5.59596)
     | > loss_mel: 24.70189  (25.07675)
     | > loss_duration: 1.78592  (1.73070)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 36.58650  (36.82342)
     | > grad_norm_0: 54.62096  (87.74847)
     | > loss_disc: 2.09290  (2.03270)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.09290  (2.03270)
     | > grad_norm_1: 9.01433  (19.41229)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.68920  (1.99046)
     | > loader_time: 0.02830  (0.04857)


[1m   --> STEP: 37/161 -- GLOBAL_STEP: 7975[0m
     | > loss_gen: 3.00400  (2.86099)
     | > loss_kl: 1.79427  (1.61853)
     | > loss_feat: 5.97430  (5.65483)
     | > loss_mel: 23.90388  (24.79509)
     | > loss_duration: 1.68750  (1.70696)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 36.36395  (36.63640)
     | > grad_norm_0: 98.34778  (86.62392)
     | > loss_disc: 1.98835  (2.01675)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.98835  (2.01675)
     | > grad_norm_1: 10.41072  (16.76502)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.91710  (2.13958)
     | > loader_time: 0.02930  (0.04790)


[1m   --> STEP: 62/161 -- GLOBAL_STEP: 8000[0m
     | > loss_gen: 2.77895  (2.84706)
     | > loss_kl: 1.50987  (1.61562)
     | > loss_feat: 4.87744  (5.60373)
     | > loss_mel: 24.29657  (24.69864)
     | > loss_duration: 1.73219  (1.70937)
     | > amp_scaler: 256.00000  (417.03226)
     | > loss_0: 35.19503  (36.47442)
     | > grad_norm_0: 64.21032  (79.07008)
     | > loss_disc: 1.97651  (2.02557)
     | > amp_scaler-1: 256.00000  (417.03226)
     | > loss_1: 1.97651  (2.02557)
     | > grad_norm_1: 23.33763  (15.39839)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.74430  (2.09150)
     | > loader_time: 0.15040  (0.04874)


[1m   --> STEP: 87/161 -- GLOBAL_STEP: 8025[0m
     | > loss_gen: 2.88859  (2.84185)
     | > loss_kl: 1.56783  (1.61577)
     | > loss_feat: 5.48688  (5.60184)
     | > loss_mel: 25.70773  (24.61293)
     | > loss_duration: 1.67779  (1.70909)
     | > amp_scaler: 256.00000  (370.75862)
     | > loss_0: 37.32882  (36.38148)
     | > grad_norm_0: 62.90410  (84.08822)
     | > loss_disc: 2.02087  (2.02799)
     | > amp_scaler-1: 256.00000  (370.75862)
     | > loss_1: 2.02087  (2.02799)
     | > grad_norm_1: 10.11205  (15.07056)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.63360  (2.09207)
     | > loader_time: 0.03670  (0.05413)


[1m   --> STEP: 112/161 -- GLOBAL_STEP: 8050[0m
     | > loss_gen: 2.99736  (2.84192)
     | > loss_kl: 1.45957  (1.62343)
     | > loss_feat: 5.55160  (5.59792)
     | > loss_mel: 23.43739  (24.61288)
     | > loss_duration: 1.70502  (1.71042)
     | > amp_scaler: 256.00000  (345.14286)
     | > loss_0: 35.15092  (36.38656)
     | > grad_norm_0: 102.03804  (87.06635)
     | > loss_disc: 2.07903  (2.03126)
     | > amp_scaler-1: 256.00000  (345.14286)
     | > loss_1: 2.07903  (2.03126)
     | > grad_norm_1: 8.00518  (15.01697)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.18960  (2.10643)
     | > loader_time: 0.11830  (0.05539)


[1m   --> STEP: 137/161 -- GLOBAL_STEP: 8075[0m
     | > loss_gen: 2.70913  (2.83756)
     | > loss_kl: 1.67805  (1.62050)
     | > loss_feat: 5.54577  (5.59796)
     | > loss_mel: 24.20398  (24.62269)
     | > loss_duration: 1.68833  (1.71118)
     | > amp_scaler: 256.00000  (328.87591)
     | > loss_0: 35.82526  (36.38990)
     | > grad_norm_0: 87.72501  (86.96971)
     | > loss_disc: 1.96657  (2.03128)
     | > amp_scaler-1: 256.00000  (328.87591)
     | > loss_1: 1.96657  (2.03128)
     | > grad_norm_1: 8.64883  (14.89981)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.41380  (2.14153)
     | > loader_time: 0.03970  (0.05542)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02750 [0m(-0.00073)
     | > avg_loss_gen:[92m 2.45830 [0m(-0.26630)
     | > avg_loss_kl:[92m 1.59400 [0m(-0.21850)
     | > avg_loss_feat:[92m 5.05305 [0m(-0.26620)
     | > avg_loss_mel:[92m 22.48695 [0m(-0.33595)
     | > avg_loss_duration:[91m 1.68668 [0m(+0.00148)
     | > avg_loss_0:[92m 33.27899 [0m(-1.08546)
     | > avg_loss_disc:[91m 2.25014 [0m(+0.31269)
     | > avg_loss_1:[91m 2.25014 [0m(+0.31269)

 > BEST MODEL : /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000/best_model_8100.pth.tar

[4m[1m > EPOCH: 50/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 21:20:59) [0m

[1m   --> STEP: 0/161 -- GLOBAL_STEP: 8100[0m
     | > loss_gen: 2.31007  (2.31007)
     | > loss_kl: 1.67597  (1.67597)
     | > loss_feat: 5.47325  (5.47325)
     | > loss_mel: 22.81301  (22.81301)
     | > loss_duration: 1.74518  (1.74518)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.01748  (34.01748)
     | > grad_norm_0: 117.42428  (117.42428)
     | > loss_disc: 2.21514  (2.21514)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.21514  (2.21514)
     | > grad_norm_1: 17.62912  (17.62912)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 3.72270  (3.72266)
     | > loader_time: 3.46040  (3.46041)


[1m   --> STEP: 25/161 -- GLOBAL_STEP: 8125[0m
     | > loss_gen: 2.84783  (2.83505)
     | > loss_kl: 1.57474  (1.58321)
     | > loss_feat: 5.13529  (5.55382)
     | > loss_mel: 23.44506  (24.07800)
     | > loss_duration: 1.66559  (1.71222)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.66851  (35.76229)
     | > grad_norm_0: 130.86015  (93.54616)
     | > loss_disc: 2.07920  (2.06183)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.07920  (2.06183)
     | > grad_norm_1: 21.33467  (13.30735)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.08200  (2.10445)
     | > loader_time: 0.03130  (0.05017)


[1m   --> STEP: 50/161 -- GLOBAL_STEP: 8150[0m
     | > loss_gen: 2.59616  (2.83901)
     | > loss_kl: 1.72794  (1.61460)
     | > loss_feat: 4.99738  (5.58095)
     | > loss_mel: 23.23140  (24.29834)
     | > loss_duration: 1.65570  (1.70656)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.20858  (36.03946)
     | > grad_norm_0: 174.48840  (101.69162)
     | > loss_disc: 2.18520  (2.04355)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.18520  (2.04355)
     | > grad_norm_1: 15.88520  (13.46680)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.17040  (2.14514)
     | > loader_time: 0.03830  (0.04604)


[1m   --> STEP: 75/161 -- GLOBAL_STEP: 8175[0m
     | > loss_gen: 2.63176  (2.83877)
     | > loss_kl: 1.47660  (1.61001)
     | > loss_feat: 5.85799  (5.60480)
     | > loss_mel: 23.31351  (24.45332)
     | > loss_duration: 1.72342  (1.70908)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.00328  (36.21598)
     | > grad_norm_0: 105.27251  (98.72833)
     | > loss_disc: 2.12773  (2.03930)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.12773  (2.03930)
     | > grad_norm_1: 11.01118  (13.48739)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.64610  (2.10093)
     | > loader_time: 0.16570  (0.05565)


[1m   --> STEP: 100/161 -- GLOBAL_STEP: 8200[0m
     | > loss_gen: 2.79388  (2.83948)
     | > loss_kl: 1.60560  (1.61631)
     | > loss_feat: 5.49259  (5.59455)
     | > loss_mel: 23.83081  (24.38097)
     | > loss_duration: 1.69721  (1.71135)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.42008  (36.14266)
     | > grad_norm_0: 84.70908  (96.58524)
     | > loss_disc: 2.11674  (2.03877)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.11674  (2.03877)
     | > grad_norm_1: 15.59882  (13.07681)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.66230  (2.10424)
     | > loader_time: 0.03570  (0.05263)


[1m   --> STEP: 125/161 -- GLOBAL_STEP: 8225[0m
     | > loss_gen: 3.20427  (2.84078)
     | > loss_kl: 1.62459  (1.61008)
     | > loss_feat: 6.11765  (5.58582)
     | > loss_mel: 23.94834  (24.37272)
     | > loss_duration: 1.67266  (1.71017)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.56751  (36.11958)
     | > grad_norm_0: 56.53032  (100.69963)
     | > loss_disc: 1.81926  (2.03611)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.81926  (2.03611)
     | > grad_norm_1: 13.39337  (13.42064)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.29210  (2.14685)
     | > loader_time: 0.03690  (0.05434)


[1m   --> STEP: 150/161 -- GLOBAL_STEP: 8250[0m
     | > loss_gen: 2.82621  (2.84993)
     | > loss_kl: 1.67967  (1.60790)
     | > loss_feat: 6.22633  (5.63567)
     | > loss_mel: 25.71568  (24.42286)
     | > loss_duration: 1.72901  (1.71003)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.17691  (36.22639)
     | > grad_norm_0: 62.24480  (97.40877)
     | > loss_disc: 1.96398  (2.02862)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.96398  (2.02862)
     | > grad_norm_1: 12.31198  (13.54706)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.29740  (2.15296)
     | > loader_time: 0.03770  (0.05464)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02769 [0m(+0.00018)
     | > avg_loss_gen:[91m 2.89523 [0m(+0.43693)
     | > avg_loss_kl:[91m 1.67173 [0m(+0.07773)
     | > avg_loss_feat:[92m 4.85497 [0m(-0.19808)
     | > avg_loss_mel:[92m 21.96935 [0m(-0.51760)
     | > avg_loss_duration:[91m 1.68983 [0m(+0.00315)
     | > avg_loss_0:[92m 33.08111 [0m(-0.19788)
     | > avg_loss_disc:[92m 2.05144 [0m(-0.19870)
     | > avg_loss_1:[92m 2.05144 [0m(-0.19870)

 > BEST MODEL : /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000/best_model_8262.pth.tar

[4m[1m > EPOCH: 51/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 21:27:15) [0m

[1m   --> STEP: 13/161 -- GLOBAL_STEP: 8275[0m
     | > loss_gen: 2.88111  (2.86146)
     | > loss_kl: 1.55127  (1.62895)
     | > loss_feat: 6.58232  (5.69835)
     | > loss_mel: 25.32126  (24.22074)
     | > loss_duration: 1.65317  (1.70989)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.98913  (36.11938)
     | > grad_norm_0: 127.11851  (91.02582)
     | > loss_disc: 1.84189  (1.99346)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.84189  (1.99346)
     | > grad_norm_1: 18.21588  (16.22035)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.70120  (2.06507)
     | > loader_time: 0.19070  (0.07244)


[1m   --> STEP: 38/161 -- GLOBAL_STEP: 8300[0m
     | > loss_gen: 2.93670  (2.88640)
     | > loss_kl: 1.57426  (1.62558)
     | > loss_feat: 6.27798  (5.74936)
     | > loss_mel: 24.41239  (24.49773)
     | > loss_duration: 1.71857  (1.70903)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.91990  (36.46812)
     | > grad_norm_0: 121.99053  (105.41884)
     | > loss_disc: 1.93607  (1.98663)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.93607  (1.98663)
     | > grad_norm_1: 30.61608  (18.39741)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.33040  (2.13722)
     | > loader_time: 0.03490  (0.04969)


[1m   --> STEP: 63/161 -- GLOBAL_STEP: 8325[0m
     | > loss_gen: 2.80005  (2.87054)
     | > loss_kl: 1.83507  (1.63484)
     | > loss_feat: 5.72524  (5.70695)
     | > loss_mel: 24.65393  (24.60333)
     | > loss_duration: 1.62927  (1.69553)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.64355  (36.51119)
     | > grad_norm_0: 146.69128  (100.80328)
     | > loss_disc: 1.92889  (2.01676)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.92889  (2.01676)
     | > grad_norm_1: 15.88700  (15.76142)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.88400  (2.09507)
     | > loader_time: 0.02940  (0.04990)


[1m   --> STEP: 88/161 -- GLOBAL_STEP: 8350[0m
     | > loss_gen: 2.99082  (2.85269)
     | > loss_kl: 1.65419  (1.62822)
     | > loss_feat: 5.48939  (5.68472)
     | > loss_mel: 23.77784  (24.58850)
     | > loss_duration: 1.65005  (1.69608)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.56228  (36.45022)
     | > grad_norm_0: 122.32522  (99.48975)
     | > loss_disc: 2.11751  (2.02492)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.11751  (2.02492)
     | > grad_norm_1: 26.91561  (15.47874)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.23030  (2.08842)
     | > loader_time: 0.03750  (0.04783)


[1m   --> STEP: 113/161 -- GLOBAL_STEP: 8375[0m
     | > loss_gen: 3.00355  (2.85211)
     | > loss_kl: 1.58641  (1.62221)
     | > loss_feat: 5.89927  (5.67797)
     | > loss_mel: 24.35003  (24.51516)
     | > loss_duration: 1.77765  (1.70433)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.61692  (36.37178)
     | > grad_norm_0: 70.67647  (99.86131)
     | > loss_disc: 1.98664  (2.02795)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.98664  (2.02795)
     | > grad_norm_1: 31.00587  (16.65852)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.55090  (2.10653)
     | > loader_time: 0.04160  (0.04989)


[1m   --> STEP: 138/161 -- GLOBAL_STEP: 8400[0m
     | > loss_gen: 2.83058  (2.85174)
     | > loss_kl: 1.61095  (1.61032)
     | > loss_feat: 6.80055  (5.68939)
     | > loss_mel: 25.69013  (24.46234)
     | > loss_duration: 1.73116  (1.70437)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.66338  (36.31816)
     | > grad_norm_0: 116.90093  (97.89542)
     | > loss_disc: 1.89855  (2.02642)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.89855  (2.02642)
     | > grad_norm_1: 8.83457  (15.99697)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.18420  (2.13970)
     | > loader_time: 0.03820  (0.04933)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02685 [0m(-0.00084)
     | > avg_loss_gen:[92m 2.80817 [0m(-0.08706)
     | > avg_loss_kl:[91m 1.88262 [0m(+0.21089)
     | > avg_loss_feat:[91m 5.66548 [0m(+0.81051)
     | > avg_loss_mel:[91m 23.95243 [0m(+1.98308)
     | > avg_loss_duration:[91m 1.70397 [0m(+0.01413)
     | > avg_loss_0:[91m 36.01266 [0m(+2.93155)
     | > avg_loss_disc:[92m 2.00959 [0m(-0.04185)
     | > avg_loss_1:[92m 2.00959 [0m(-0.04185)


[4m[1m > EPOCH: 52/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 21:33:23) [0m

[1m   --> STEP: 1/161 -- GLOBAL_STEP: 8425[0m
     | > loss_gen: 2.71732  (2.71732)
     | > loss_kl: 1.61910  (1.61910)
     | > loss_feat: 5.98700  (5.98700)
     | > loss_mel: 26.50573  (26.50573)
     | > loss_duration: 1.78214  (1.78214)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.61128  (38.61128)
     | > grad_norm_0: 79.61222  (79.61222)
     | > loss_disc: 1.96226  (1.96226)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.96226  (1.96226)
     | > grad_norm_1: 7.97584  (7.97584)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.92760  (1.92764)
     | > loader_time: 0.16680  (0.16679)


[1m   --> STEP: 26/161 -- GLOBAL_STEP: 8450[0m
     | > loss_gen: 3.00884  (2.89610)
     | > loss_kl: 1.64871  (1.63666)
     | > loss_feat: 5.26834  (5.76094)
     | > loss_mel: 22.88723  (24.63798)
     | > loss_duration: 1.79200  (1.70359)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.60511  (36.63528)
     | > grad_norm_0: 123.47363  (108.22414)
     | > loss_disc: 2.03505  (2.00758)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.03505  (2.00758)
     | > grad_norm_1: 8.70201  (11.24591)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.56240  (2.07758)
     | > loader_time: 0.04750  (0.05679)


[1m   --> STEP: 51/161 -- GLOBAL_STEP: 8475[0m
     | > loss_gen: 2.43168  (2.86157)
     | > loss_kl: 1.41618  (1.63285)
     | > loss_feat: 5.39232  (5.69376)
     | > loss_mel: 24.61930  (24.49769)
     | > loss_duration: 1.68803  (1.70196)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.54751  (36.38782)
     | > grad_norm_0: 67.11406  (102.61793)
     | > loss_disc: 2.02786  (2.01801)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.02786  (2.01801)
     | > grad_norm_1: 11.88697  (12.02093)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.05130  (2.08500)
     | > loader_time: 0.03600  (0.04771)


[1m   --> STEP: 76/161 -- GLOBAL_STEP: 8500[0m
     | > loss_gen: 2.92213  (2.86008)
     | > loss_kl: 1.42930  (1.62354)
     | > loss_feat: 5.86537  (5.66955)
     | > loss_mel: 24.41973  (24.38364)
     | > loss_duration: 1.71344  (1.69973)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.34997  (36.23654)
     | > grad_norm_0: 67.01354  (98.74635)
     | > loss_disc: 1.97894  (2.01717)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.97894  (2.01717)
     | > grad_norm_1: 9.48101  (12.14235)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.20500  (2.07994)
     | > loader_time: 0.04700  (0.04663)


[1m   --> STEP: 101/161 -- GLOBAL_STEP: 8525[0m
     | > loss_gen: 2.59927  (2.86349)
     | > loss_kl: 1.65812  (1.60998)
     | > loss_feat: 4.50270  (5.69312)
     | > loss_mel: 24.79146  (24.40307)
     | > loss_duration: 1.65527  (1.70113)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.20682  (36.27079)
     | > grad_norm_0: 82.74287  (100.79045)
     | > loss_disc: 2.22987  (2.01787)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.22987  (2.01787)
     | > grad_norm_1: 24.13567  (12.92498)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.83310  (2.07780)
     | > loader_time: 0.03430  (0.04734)


[1m   --> STEP: 126/161 -- GLOBAL_STEP: 8550[0m
     | > loss_gen: 3.05196  (2.86656)
     | > loss_kl: 1.41957  (1.59901)
     | > loss_feat: 5.51691  (5.68788)
     | > loss_mel: 24.44329  (24.32207)
     | > loss_duration: 1.74660  (1.70279)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.17833  (36.17832)
     | > grad_norm_0: 76.32787  (97.01624)
     | > loss_disc: 2.05440  (2.01560)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.05440  (2.01560)
     | > grad_norm_1: 5.49183  (12.82575)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.85230  (2.11836)
     | > loader_time: 0.03380  (0.04766)


[1m   --> STEP: 151/161 -- GLOBAL_STEP: 8575[0m
     | > loss_gen: 2.73910  (2.86096)
     | > loss_kl: 1.93660  (1.60065)
     | > loss_feat: 5.94655  (5.68665)
     | > loss_mel: 23.51566  (24.30352)
     | > loss_duration: 1.59052  (1.70511)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.72842  (36.15689)
     | > grad_norm_0: 37.95005  (97.96883)
     | > loss_disc: 2.07122  (2.02129)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.07122  (2.02129)
     | > grad_norm_1: 11.86679  (12.77364)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.25330  (2.13464)
     | > loader_time: 0.03440  (0.04763)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02747 [0m(+0.00062)
     | > avg_loss_gen:[91m 3.12447 [0m(+0.31629)
     | > avg_loss_kl:[92m 1.71307 [0m(-0.16955)
     | > avg_loss_feat:[91m 5.97163 [0m(+0.30615)
     | > avg_loss_mel:[91m 24.26006 [0m(+0.30763)
     | > avg_loss_duration:[92m 1.68554 [0m(-0.01843)
     | > avg_loss_0:[91m 36.75476 [0m(+0.74210)
     | > avg_loss_disc:[92m 1.89744 [0m(-0.11215)
     | > avg_loss_1:[92m 1.89744 [0m(-0.11215)


[4m[1m > EPOCH: 53/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 21:39:29) [0m

[1m   --> STEP: 14/161 -- GLOBAL_STEP: 8600[0m
     | > loss_gen: 2.92565  (2.82614)
     | > loss_kl: 1.63340  (1.57923)
     | > loss_feat: 5.27689  (5.49054)
     | > loss_mel: 24.57495  (23.77934)
     | > loss_duration: 1.68891  (1.69709)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.09980  (35.37234)
     | > grad_norm_0: 77.50494  (96.37218)
     | > loss_disc: 2.11358  (2.01484)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.11358  (2.01484)
     | > grad_norm_1: 12.36509  (15.03917)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.18810  (2.08278)
     | > loader_time: 0.03530  (0.06706)


[1m   --> STEP: 39/161 -- GLOBAL_STEP: 8625[0m
     | > loss_gen: 2.60495  (2.82593)
     | > loss_kl: 1.79413  (1.60626)
     | > loss_feat: 5.85396  (5.61383)
     | > loss_mel: 25.08255  (24.23736)
     | > loss_duration: 1.70432  (1.70220)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.03991  (35.98558)
     | > grad_norm_0: 77.51434  (101.47774)
     | > loss_disc: 2.05502  (2.03361)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.05502  (2.03361)
     | > grad_norm_1: 7.70689  (14.09794)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.01700  (2.11872)
     | > loader_time: 0.03340  (0.05064)


[1m   --> STEP: 64/161 -- GLOBAL_STEP: 8650[0m
     | > loss_gen: 2.78328  (2.82536)
     | > loss_kl: 1.48106  (1.61309)
     | > loss_feat: 5.89837  (5.57479)
     | > loss_mel: 23.71214  (24.29437)
     | > loss_duration: 1.74392  (1.69947)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.61877  (36.00708)
     | > grad_norm_0: 113.12186  (100.35702)
     | > loss_disc: 1.92314  (2.02976)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.92314  (2.02976)
     | > grad_norm_1: 15.20400  (13.99004)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.01810  (2.08285)
     | > loader_time: 0.03530  (0.04633)


[1m   --> STEP: 89/161 -- GLOBAL_STEP: 8675[0m
     | > loss_gen: 3.02918  (2.83768)
     | > loss_kl: 1.50197  (1.60475)
     | > loss_feat: 6.08525  (5.62771)
     | > loss_mel: 24.16099  (24.31295)
     | > loss_duration: 1.69851  (1.70360)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.47589  (36.08669)
     | > grad_norm_0: 67.86092  (97.11175)
     | > loss_disc: 2.00763  (2.02516)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.00763  (2.02516)
     | > grad_norm_1: 11.53087  (13.29030)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.69010  (2.07451)
     | > loader_time: 0.02510  (0.04835)


[1m   --> STEP: 114/161 -- GLOBAL_STEP: 8700[0m
     | > loss_gen: 2.86055  (2.83979)
     | > loss_kl: 1.63046  (1.60274)
     | > loss_feat: 5.37454  (5.62419)
     | > loss_mel: 25.18423  (24.33615)
     | > loss_duration: 1.65338  (1.70382)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.70316  (36.10669)
     | > grad_norm_0: 182.47331  (97.49665)
     | > loss_disc: 2.06441  (2.02476)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06441  (2.02476)
     | > grad_norm_1: 43.55821  (13.90767)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.98470  (2.10027)
     | > loader_time: 0.03520  (0.04953)


[1m   --> STEP: 139/161 -- GLOBAL_STEP: 8725[0m
     | > loss_gen: 2.78244  (2.83608)
     | > loss_kl: 1.73680  (1.61307)
     | > loss_feat: 5.29160  (5.61596)
     | > loss_mel: 24.86003  (24.39225)
     | > loss_duration: 1.68631  (1.70364)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.35718  (36.16100)
     | > grad_norm_0: 153.35074  (95.21173)
     | > loss_disc: 2.07954  (2.02815)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.07954  (2.02815)
     | > grad_norm_1: 11.71480  (14.94635)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.86370  (2.12836)
     | > loader_time: 0.02930  (0.05179)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02695 [0m(-0.00051)
     | > avg_loss_gen:[92m 2.89918 [0m(-0.22528)
     | > avg_loss_kl:[92m 1.56918 [0m(-0.14389)
     | > avg_loss_feat:[92m 4.90644 [0m(-1.06519)
     | > avg_loss_mel:[92m 22.86450 [0m(-1.39556)
     | > avg_loss_duration:[92m 1.67341 [0m(-0.01212)
     | > avg_loss_0:[92m 33.91271 [0m(-2.84204)
     | > avg_loss_disc:[91m 2.12685 [0m(+0.22941)
     | > avg_loss_1:[91m 2.12685 [0m(+0.22941)


[4m[1m > EPOCH: 54/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 21:45:34) [0m

[1m   --> STEP: 2/161 -- GLOBAL_STEP: 8750[0m
     | > loss_gen: 3.08803  (2.90104)
     | > loss_kl: 1.58053  (1.73172)
     | > loss_feat: 6.09026  (6.07390)
     | > loss_mel: 23.56648  (24.17799)
     | > loss_duration: 1.70682  (1.67374)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.03212  (36.55840)
     | > grad_norm_0: 86.51454  (68.16427)
     | > loss_disc: 1.92690  (1.97930)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.92690  (1.97930)
     | > grad_norm_1: 8.56661  (9.50364)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.67960  (1.78048)
     | > loader_time: 0.14010  (0.15936)


[1m   --> STEP: 27/161 -- GLOBAL_STEP: 8775[0m
     | > loss_gen: 2.52611  (2.83870)
     | > loss_kl: 1.76781  (1.63438)
     | > loss_feat: 5.82735  (5.63956)
     | > loss_mel: 23.88948  (24.35388)
     | > loss_duration: 1.67076  (1.70807)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.68151  (36.17460)
     | > grad_norm_0: 172.31702  (94.19197)
     | > loss_disc: 2.04265  (2.04197)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.04265  (2.04197)
     | > grad_norm_1: 13.67893  (14.76822)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.72210  (2.04788)
     | > loader_time: 0.03450  (0.05334)


[1m   --> STEP: 52/161 -- GLOBAL_STEP: 8800[0m
     | > loss_gen: 2.68418  (2.84852)
     | > loss_kl: 1.62148  (1.62784)
     | > loss_feat: 5.56792  (5.65305)
     | > loss_mel: 24.54586  (24.35591)
     | > loss_duration: 1.68622  (1.70229)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.10566  (36.18763)
     | > grad_norm_0: 87.98500  (91.23619)
     | > loss_disc: 2.04043  (2.03426)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.04043  (2.03426)
     | > grad_norm_1: 8.40479  (14.61208)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.66210  (2.07787)
     | > loader_time: 0.02660  (0.04511)


[1m   --> STEP: 77/161 -- GLOBAL_STEP: 8825[0m
     | > loss_gen: 2.55525  (2.84348)
     | > loss_kl: 1.65776  (1.63234)
     | > loss_feat: 5.84175  (5.65996)
     | > loss_mel: 24.11952  (24.25463)
     | > loss_duration: 1.64742  (1.69667)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.82170  (36.08707)
     | > grad_norm_0: 167.51456  (95.36108)
     | > loss_disc: 2.03742  (2.02698)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.03742  (2.02698)
     | > grad_norm_1: 15.17690  (14.45610)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.38470  (2.07745)
     | > loader_time: 0.03770  (0.04687)


[1m   --> STEP: 102/161 -- GLOBAL_STEP: 8850[0m
     | > loss_gen: 2.71922  (2.84705)
     | > loss_kl: 1.63979  (1.63548)
     | > loss_feat: 4.88467  (5.63596)
     | > loss_mel: 24.79109  (24.27333)
     | > loss_duration: 1.61874  (1.69872)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.65351  (36.09055)
     | > grad_norm_0: 112.36251  (95.66537)
     | > loss_disc: 2.22921  (2.02822)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.22921  (2.02822)
     | > grad_norm_1: 58.47724  (15.80064)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.64320  (2.07383)
     | > loader_time: 0.12390  (0.04666)


[1m   --> STEP: 127/161 -- GLOBAL_STEP: 8875[0m
     | > loss_gen: 2.95504  (2.84798)
     | > loss_kl: 1.64010  (1.63009)
     | > loss_feat: 6.72338  (5.64292)
     | > loss_mel: 24.95805  (24.26145)
     | > loss_duration: 1.77084  (1.70212)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.04743  (36.08455)
     | > grad_norm_0: 50.14256  (98.44764)
     | > loss_disc: 1.89172  (2.02477)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.89172  (2.02477)
     | > grad_norm_1: 8.94485  (15.93007)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 3.01490  (2.12186)
     | > loader_time: 0.09930  (0.04639)


[1m   --> STEP: 152/161 -- GLOBAL_STEP: 8900[0m
     | > loss_gen: 2.60792  (2.84867)
     | > loss_kl: 1.60014  (1.62889)
     | > loss_feat: 5.21977  (5.64718)
     | > loss_mel: 24.07072  (24.21728)
     | > loss_duration: 1.66510  (1.70295)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.16366  (36.04497)
     | > grad_norm_0: 131.40503  (96.98784)
     | > loss_disc: 2.06014  (2.02607)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06014  (2.02607)
     | > grad_norm_1: 19.55547  (15.52440)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.54000  (2.12101)
     | > loader_time: 0.02500  (0.04476)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02797 [0m(+0.00102)
     | > avg_loss_gen:[92m 2.62120 [0m(-0.27798)
     | > avg_loss_kl:[91m 1.75336 [0m(+0.18418)
     | > avg_loss_feat:[91m 5.07413 [0m(+0.16769)
     | > avg_loss_mel:[91m 23.08762 [0m(+0.22312)
     | > avg_loss_duration:[91m 1.69403 [0m(+0.02062)
     | > avg_loss_0:[91m 34.23034 [0m(+0.31763)
     | > avg_loss_disc:[91m 2.16298 [0m(+0.03612)
     | > avg_loss_1:[91m 2.16298 [0m(+0.03612)


[4m[1m > EPOCH: 55/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 21:51:45) [0m

[1m   --> STEP: 15/161 -- GLOBAL_STEP: 8925[0m
     | > loss_gen: 2.76178  (2.82429)
     | > loss_kl: 1.51327  (1.58952)
     | > loss_feat: 5.43943  (5.55310)
     | > loss_mel: 24.09631  (24.21558)
     | > loss_duration: 1.76226  (1.71404)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.57305  (35.89653)
     | > grad_norm_0: 108.08797  (123.92252)
     | > loss_disc: 2.06702  (2.07930)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06702  (2.07930)
     | > grad_norm_1: 27.79298  (17.00193)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.01280  (2.02131)
     | > loader_time: 0.03750  (0.03849)


[1m   --> STEP: 40/161 -- GLOBAL_STEP: 8950[0m
     | > loss_gen: 2.86790  (2.84227)
     | > loss_kl: 1.69915  (1.61208)
     | > loss_feat: 6.23568  (5.62101)
     | > loss_mel: 24.63162  (24.17827)
     | > loss_duration: 1.74796  (1.70468)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.18232  (35.95831)
     | > grad_norm_0: 60.75460  (110.95856)
     | > loss_disc: 1.93726  (2.03932)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.93726  (2.03932)
     | > grad_norm_1: 11.13512  (16.16039)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.41830  (2.10169)
     | > loader_time: 0.03480  (0.04243)


[1m   --> STEP: 65/161 -- GLOBAL_STEP: 8975[0m
     | > loss_gen: 2.85058  (2.84992)
     | > loss_kl: 1.65404  (1.60931)
     | > loss_feat: 5.10852  (5.65529)
     | > loss_mel: 23.21514  (24.20143)
     | > loss_duration: 1.70551  (1.70371)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.53379  (36.01966)
     | > grad_norm_0: 47.62880  (93.91024)
     | > loss_disc: 2.14114  (2.02913)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.14114  (2.02913)
     | > grad_norm_1: 5.29320  (13.87486)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.82370  (2.04753)
     | > loader_time: 0.02830  (0.04521)


[1m   --> STEP: 90/161 -- GLOBAL_STEP: 9000[0m
     | > loss_gen: 3.03042  (2.84687)
     | > loss_kl: 1.63416  (1.61932)
     | > loss_feat: 5.42658  (5.67195)
     | > loss_mel: 23.07907  (24.20674)
     | > loss_duration: 1.65564  (1.70431)
     | > amp_scaler: 512.00000  (321.42222)
     | > loss_0: 34.82588  (36.04919)
     | > grad_norm_0: 82.25248  (97.66059)
     | > loss_disc: 2.09066  (2.02652)
     | > amp_scaler-1: 512.00000  (321.42222)
     | > loss_1: 2.09066  (2.02652)
     | > grad_norm_1: 10.63252  (13.82470)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.57680  (2.03507)
     | > loader_time: 0.11950  (0.04824)


[1m   --> STEP: 115/161 -- GLOBAL_STEP: 9025[0m
     | > loss_gen: 2.82358  (2.86142)
     | > loss_kl: 1.46447  (1.61799)
     | > loss_feat: 6.27819  (5.71704)
     | > loss_mel: 23.67215  (24.22429)
     | > loss_duration: 1.70851  (1.70739)
     | > amp_scaler: 512.00000  (362.85217)
     | > loss_0: 35.94690  (36.12813)
     | > grad_norm_0: 47.05826  (93.11475)
     | > loss_disc: 1.90629  (2.01609)
     | > amp_scaler-1: 512.00000  (362.85217)
     | > loss_1: 1.90629  (2.01609)
     | > grad_norm_1: 9.03629  (13.15083)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.40930  (2.06578)
     | > loader_time: 0.04060  (0.04744)


[1m   --> STEP: 140/161 -- GLOBAL_STEP: 9050[0m
     | > loss_gen: 2.98925  (2.85426)
     | > loss_kl: 1.70343  (1.61481)
     | > loss_feat: 5.85715  (5.67591)
     | > loss_mel: 24.42619  (24.14497)
     | > loss_duration: 1.67515  (1.70380)
     | > amp_scaler: 256.00000  (376.68571)
     | > loss_0: 36.65117  (35.99375)
     | > grad_norm_0: 116.89137  (98.38367)
     | > loss_disc: 1.99799  (2.02465)
     | > amp_scaler-1: 256.00000  (376.68571)
     | > loss_1: 1.99799  (2.02465)
     | > grad_norm_1: 10.91995  (13.39313)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.89460  (2.09504)
     | > loader_time: 0.06100  (0.04773)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02795 [0m(-0.00002)
     | > avg_loss_gen:[91m 2.92617 [0m(+0.30497)
     | > avg_loss_kl:[92m 1.70751 [0m(-0.04586)
     | > avg_loss_feat:[91m 6.22950 [0m(+1.15537)
     | > avg_loss_mel:[91m 24.64304 [0m(+1.55542)
     | > avg_loss_duration:[92m 1.69075 [0m(-0.00328)
     | > avg_loss_0:[91m 37.19696 [0m(+2.96663)
     | > avg_loss_disc:[92m 1.94506 [0m(-0.21792)
     | > avg_loss_1:[92m 1.94506 [0m(-0.21792)


[4m[1m > EPOCH: 56/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 21:57:52) [0m

[1m   --> STEP: 3/161 -- GLOBAL_STEP: 9075[0m
     | > loss_gen: 3.10228  (2.87937)
     | > loss_kl: 1.77148  (1.78091)
     | > loss_feat: 5.37170  (5.62497)
     | > loss_mel: 23.63362  (25.13359)
     | > loss_duration: 1.73869  (1.71277)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.61777  (37.13161)
     | > grad_norm_0: 40.45002  (115.19501)
     | > loss_disc: 2.06483  (2.01665)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06483  (2.01665)
     | > grad_norm_1: 15.97400  (12.31081)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.75250  (2.01208)
     | > loader_time: 0.04230  (0.05650)


[1m   --> STEP: 28/161 -- GLOBAL_STEP: 9100[0m
     | > loss_gen: 2.52300  (2.86384)
     | > loss_kl: 1.41800  (1.63859)
     | > loss_feat: 5.39408  (5.75234)
     | > loss_mel: 22.78183  (24.43079)
     | > loss_duration: 1.70481  (1.70973)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.82172  (36.39528)
     | > grad_norm_0: 64.19450  (110.11143)
     | > loss_disc: 2.06552  (2.00079)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06552  (2.00079)
     | > grad_norm_1: 18.55612  (14.55458)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.88290  (2.05191)
     | > loader_time: 0.02990  (0.04090)


[1m   --> STEP: 53/161 -- GLOBAL_STEP: 9125[0m
     | > loss_gen: 2.83675  (2.85943)
     | > loss_kl: 1.79262  (1.63879)
     | > loss_feat: 5.50148  (5.69838)
     | > loss_mel: 24.59928  (24.18594)
     | > loss_duration: 1.61563  (1.71023)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.34575  (36.09277)
     | > grad_norm_0: 61.83182  (112.23564)
     | > loss_disc: 2.00116  (2.01417)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.00116  (2.01417)
     | > grad_norm_1: 14.83458  (15.86360)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.51470  (2.06280)
     | > loader_time: 0.02260  (0.03927)


[1m   --> STEP: 78/161 -- GLOBAL_STEP: 9150[0m
     | > loss_gen: 3.00604  (2.85486)
     | > loss_kl: 1.77871  (1.65506)
     | > loss_feat: 5.73887  (5.68413)
     | > loss_mel: 23.48351  (24.17524)
     | > loss_duration: 1.71196  (1.70931)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.71909  (36.07860)
     | > grad_norm_0: 88.77327  (112.38090)
     | > loss_disc: 1.98506  (2.01727)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.98506  (2.01727)
     | > grad_norm_1: 13.94084  (16.00826)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.74120  (2.06820)
     | > loader_time: 0.02610  (0.04523)


[1m   --> STEP: 103/161 -- GLOBAL_STEP: 9175[0m
     | > loss_gen: 2.69806  (2.86201)
     | > loss_kl: 1.78959  (1.64783)
     | > loss_feat: 5.78645  (5.71273)
     | > loss_mel: 23.55357  (24.17255)
     | > loss_duration: 1.72988  (1.71193)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.55755  (36.10706)
     | > grad_norm_0: 64.44456  (110.62207)
     | > loss_disc: 2.07454  (2.01624)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.07454  (2.01624)
     | > grad_norm_1: 24.19370  (16.15196)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.26490  (2.07598)
     | > loader_time: 0.03390  (0.05366)


[1m   --> STEP: 128/161 -- GLOBAL_STEP: 9200[0m
     | > loss_gen: 2.81744  (2.86253)
     | > loss_kl: 1.61757  (1.63750)
     | > loss_feat: 6.12739  (5.71904)
     | > loss_mel: 24.41186  (24.24908)
     | > loss_duration: 1.76920  (1.71413)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.74346  (36.18228)
     | > grad_norm_0: 53.44908  (108.85315)
     | > loss_disc: 2.09853  (2.01590)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.09853  (2.01590)
     | > grad_norm_1: 5.56060  (15.96130)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.49610  (2.12660)
     | > loader_time: 0.06090  (0.05488)


[1m   --> STEP: 153/161 -- GLOBAL_STEP: 9225[0m
     | > loss_gen: 2.96547  (2.86277)
     | > loss_kl: 1.40632  (1.62821)
     | > loss_feat: 5.69526  (5.72439)
     | > loss_mel: 22.85545  (24.24532)
     | > loss_duration: 1.77671  (1.71100)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.69922  (36.17168)
     | > grad_norm_0: 52.81737  (103.67702)
     | > loss_disc: 2.07767  (2.01331)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.07767  (2.01331)
     | > grad_norm_1: 7.40111  (15.81250)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.43200  (2.12574)
     | > loader_time: 0.03790  (0.05357)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02713 [0m(-0.00082)
     | > avg_loss_gen:[92m 2.84697 [0m(-0.07920)
     | > avg_loss_kl:[91m 1.72477 [0m(+0.01726)
     | > avg_loss_feat:[92m 5.45965 [0m(-0.76984)
     | > avg_loss_mel:[92m 24.26530 [0m(-0.37774)
     | > avg_loss_duration:[91m 1.69480 [0m(+0.00405)
     | > avg_loss_0:[92m 35.99149 [0m(-1.20547)
     | > avg_loss_disc:[91m 2.01112 [0m(+0.06606)
     | > avg_loss_1:[91m 2.01112 [0m(+0.06606)


[4m[1m > EPOCH: 57/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 22:03:58) [0m

[1m   --> STEP: 16/161 -- GLOBAL_STEP: 9250[0m
     | > loss_gen: 2.89090  (2.87439)
     | > loss_kl: 1.64669  (1.64079)
     | > loss_feat: 6.31159  (5.84667)
     | > loss_mel: 23.03484  (24.16698)
     | > loss_duration: 1.78348  (1.72532)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.66750  (36.25414)
     | > grad_norm_0: 37.32253  (104.83379)
     | > loss_disc: 1.96066  (2.02711)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.96066  (2.02711)
     | > grad_norm_1: 7.31747  (13.25076)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.05560  (2.07089)
     | > loader_time: 0.03600  (0.06253)


[1m   --> STEP: 41/161 -- GLOBAL_STEP: 9275[0m
     | > loss_gen: 2.74986  (2.86133)
     | > loss_kl: 1.65764  (1.64278)
     | > loss_feat: 6.15385  (5.72153)
     | > loss_mel: 23.40017  (24.18173)
     | > loss_duration: 1.65332  (1.71154)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.61485  (36.11891)
     | > grad_norm_0: 47.69346  (96.76217)
     | > loss_disc: 1.89726  (2.03062)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.89726  (2.03062)
     | > grad_norm_1: 10.11042  (14.74863)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.65460  (2.09784)
     | > loader_time: 0.02330  (0.04608)


[1m   --> STEP: 66/161 -- GLOBAL_STEP: 9300[0m
     | > loss_gen: 3.01019  (2.84593)
     | > loss_kl: 1.54717  (1.64375)
     | > loss_feat: 5.34259  (5.65872)
     | > loss_mel: 26.38476  (24.14687)
     | > loss_duration: 1.70679  (1.70925)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.99150  (36.00452)
     | > grad_norm_0: 93.45522  (99.10072)
     | > loss_disc: 2.02899  (2.03664)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.02899  (2.03664)
     | > grad_norm_1: 21.31014  (14.16255)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.07760  (2.07727)
     | > loader_time: 0.05980  (0.04820)


[1m   --> STEP: 91/161 -- GLOBAL_STEP: 9325[0m
     | > loss_gen: 2.60200  (2.84557)
     | > loss_kl: 1.66263  (1.63911)
     | > loss_feat: 5.25832  (5.67731)
     | > loss_mel: 23.70539  (24.11404)
     | > loss_duration: 1.70267  (1.70857)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.93100  (35.98459)
     | > grad_norm_0: 163.88315  (95.35008)
     | > loss_disc: 2.20693  (2.03214)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.20693  (2.03214)
     | > grad_norm_1: 40.41330  (15.53065)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.94500  (2.06265)
     | > loader_time: 0.13140  (0.04901)


[1m   --> STEP: 116/161 -- GLOBAL_STEP: 9350[0m
     | > loss_gen: 3.14510  (2.85369)
     | > loss_kl: 1.59841  (1.63865)
     | > loss_feat: 5.74422  (5.70705)
     | > loss_mel: 23.00116  (24.16407)
     | > loss_duration: 1.64417  (1.70775)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.13306  (36.07121)
     | > grad_norm_0: 61.96767  (92.40955)
     | > loss_disc: 2.05740  (2.02742)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.05740  (2.02742)
     | > grad_norm_1: 13.30289  (15.21956)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.37110  (2.09452)
     | > loader_time: 0.04080  (0.05077)


[1m   --> STEP: 141/161 -- GLOBAL_STEP: 9375[0m
     | > loss_gen: 3.19421  (2.85770)
     | > loss_kl: 1.40125  (1.63868)
     | > loss_feat: 5.22016  (5.70862)
     | > loss_mel: 22.48475  (24.14151)
     | > loss_duration: 1.76481  (1.70801)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.06519  (36.05453)
     | > grad_norm_0: 48.88192  (95.55611)
     | > loss_disc: 2.08340  (2.02901)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.08340  (2.02901)
     | > grad_norm_1: 16.88964  (15.28284)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.61320  (2.11525)
     | > loader_time: 0.02740  (0.04960)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02644 [0m(-0.00069)
     | > avg_loss_gen:[91m 3.01403 [0m(+0.16707)
     | > avg_loss_kl:[92m 1.67039 [0m(-0.05437)
     | > avg_loss_feat:[91m 6.17857 [0m(+0.71891)
     | > avg_loss_mel:[92m 23.82288 [0m(-0.44242)
     | > avg_loss_duration:[92m 1.67832 [0m(-0.01648)
     | > avg_loss_0:[91m 36.36419 [0m(+0.37270)
     | > avg_loss_disc:[92m 1.87408 [0m(-0.13704)
     | > avg_loss_1:[92m 1.87408 [0m(-0.13704)


[4m[1m > EPOCH: 58/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 22:10:05) [0m

[1m   --> STEP: 4/161 -- GLOBAL_STEP: 9400[0m
     | > loss_gen: 3.02175  (2.79797)
     | > loss_kl: 1.59009  (1.68839)
     | > loss_feat: 5.29484  (5.57987)
     | > loss_mel: 23.47783  (23.75097)
     | > loss_duration: 1.60446  (1.71708)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.98897  (35.53428)
     | > grad_norm_0: 124.27432  (123.67113)
     | > loss_disc: 1.99385  (2.03649)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.99385  (2.03649)
     | > grad_norm_1: 14.45951  (19.12864)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.68380  (1.97490)
     | > loader_time: 0.13620  (0.11769)


[1m   --> STEP: 29/161 -- GLOBAL_STEP: 9425[0m
     | > loss_gen: 3.02324  (2.84066)
     | > loss_kl: 1.53946  (1.66082)
     | > loss_feat: 5.78949  (5.65669)
     | > loss_mel: 24.65281  (24.26952)
     | > loss_duration: 1.66453  (1.69549)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.66952  (36.12318)
     | > grad_norm_0: 246.04396  (122.28962)
     | > loss_disc: 1.93430  (2.02387)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.93430  (2.02387)
     | > grad_norm_1: 28.40272  (14.81579)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.17880  (2.06324)
     | > loader_time: 0.03160  (0.04794)


[1m   --> STEP: 54/161 -- GLOBAL_STEP: 9450[0m
     | > loss_gen: 2.96515  (2.86529)
     | > loss_kl: 1.42207  (1.64687)
     | > loss_feat: 5.92748  (5.73252)
     | > loss_mel: 23.98080  (24.07001)
     | > loss_duration: 1.69384  (1.69892)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.98935  (36.01361)
     | > grad_norm_0: 84.90549  (118.37792)
     | > loss_disc: 2.10510  (2.01081)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.10510  (2.01081)
     | > grad_norm_1: 10.57205  (13.37793)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.58520  (2.07800)
     | > loader_time: 0.03430  (0.04762)


[1m   --> STEP: 79/161 -- GLOBAL_STEP: 9475[0m
     | > loss_gen: 3.11982  (2.87326)
     | > loss_kl: 1.70711  (1.63922)
     | > loss_feat: 5.82465  (5.79476)
     | > loss_mel: 24.90370  (24.03636)
     | > loss_duration: 1.71207  (1.70055)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.26736  (36.04415)
     | > grad_norm_0: 115.79078  (115.69802)
     | > loss_disc: 1.97315  (2.00300)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.97315  (2.00300)
     | > grad_norm_1: 15.01103  (13.00996)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.85230  (2.06464)
     | > loader_time: 0.02840  (0.05265)


[1m   --> STEP: 104/161 -- GLOBAL_STEP: 9500[0m
     | > loss_gen: 3.01728  (2.86572)
     | > loss_kl: 1.64645  (1.63921)
     | > loss_feat: 5.61734  (5.77190)
     | > loss_mel: 24.23345  (24.05407)
     | > loss_duration: 1.64380  (1.70224)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.15832  (36.03315)
     | > grad_norm_0: 250.09909  (114.61164)
     | > loss_disc: 2.01950  (2.00483)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.01950  (2.00483)
     | > grad_norm_1: 9.28151  (13.26417)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.51870  (2.07699)
     | > loader_time: 0.02730  (0.05676)


[1m   --> STEP: 129/161 -- GLOBAL_STEP: 9525[0m
     | > loss_gen: 2.94190  (2.86150)
     | > loss_kl: 1.60173  (1.63231)
     | > loss_feat: 5.26600  (5.76278)
     | > loss_mel: 24.66075  (24.10573)
     | > loss_duration: 1.70189  (1.70385)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.17227  (36.06618)
     | > grad_norm_0: 60.89113  (111.32265)
     | > loss_disc: 1.97135  (2.01583)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.97135  (2.01583)
     | > grad_norm_1: 8.97285  (13.30895)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.38050  (2.13521)
     | > loader_time: 0.04270  (0.05807)


[1m   --> STEP: 154/161 -- GLOBAL_STEP: 9550[0m
     | > loss_gen: 2.52892  (2.85726)
     | > loss_kl: 1.68221  (1.63408)
     | > loss_feat: 5.68667  (5.74471)
     | > loss_mel: 23.88835  (24.08701)
     | > loss_duration: 1.60043  (1.70202)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.38659  (36.02508)
     | > grad_norm_0: 116.54972  (111.52615)
     | > loss_disc: 2.03301  (2.02205)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.03301  (2.02205)
     | > grad_norm_1: 15.92683  (13.59085)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.60260  (2.12975)
     | > loader_time: 0.02950  (0.05595)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02729 [0m(+0.00084)
     | > avg_loss_gen:[92m 2.65199 [0m(-0.36204)
     | > avg_loss_kl:[91m 1.76427 [0m(+0.09388)
     | > avg_loss_feat:[92m 5.32690 [0m(-0.85167)
     | > avg_loss_mel:[92m 21.45868 [0m(-2.36420)
     | > avg_loss_duration:[91m 1.68920 [0m(+0.01089)
     | > avg_loss_0:[92m 32.89104 [0m(-3.47315)
     | > avg_loss_disc:[91m 1.96900 [0m(+0.09491)
     | > avg_loss_1:[91m 1.96900 [0m(+0.09491)

 > BEST MODEL : /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000/best_model_9558.pth.tar

[4m[1m > EPOCH: 59/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 22:16:14) [0m

[1m   --> STEP: 17/161 -- GLOBAL_STEP: 9575[0m
     | > loss_gen: 2.70589  (2.82632)
     | > loss_kl: 1.42302  (1.64558)
     | > loss_feat: 5.33226  (5.67630)
     | > loss_mel: 23.95732  (24.07723)
     | > loss_duration: 1.70936  (1.70495)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.12785  (35.93039)
     | > grad_norm_0: 52.91902  (72.40364)
     | > loss_disc: 2.13351  (2.01270)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.13351  (2.01270)
     | > grad_norm_1: 12.34542  (10.62862)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.03420  (2.03758)
     | > loader_time: 0.03400  (0.04382)


[1m   --> STEP: 42/161 -- GLOBAL_STEP: 9600[0m
     | > loss_gen: 3.00347  (2.85937)
     | > loss_kl: 1.71667  (1.64247)
     | > loss_feat: 5.63179  (5.73926)
     | > loss_mel: 23.10765  (24.11822)
     | > loss_duration: 1.69717  (1.69913)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.15675  (36.05844)
     | > grad_norm_0: 143.31232  (93.28888)
     | > loss_disc: 1.96642  (2.00133)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.96642  (2.00133)
     | > grad_norm_1: 11.69028  (12.17089)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.72980  (2.11525)
     | > loader_time: 0.03490  (0.04546)


[1m   --> STEP: 67/161 -- GLOBAL_STEP: 9625[0m
     | > loss_gen: 2.85397  (2.85608)
     | > loss_kl: 1.66075  (1.64803)
     | > loss_feat: 5.73969  (5.73716)
     | > loss_mel: 26.62193  (24.20412)
     | > loss_duration: 1.74343  (1.70048)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.61978  (36.14587)
     | > grad_norm_0: 200.72549  (107.86173)
     | > loss_disc: 2.08712  (2.00952)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.08712  (2.00952)
     | > grad_norm_1: 39.69737  (13.51593)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.08390  (2.09187)
     | > loader_time: 0.14420  (0.05400)


[1m   --> STEP: 92/161 -- GLOBAL_STEP: 9650[0m
     | > loss_gen: 2.67186  (2.85069)
     | > loss_kl: 1.84829  (1.63541)
     | > loss_feat: 5.58388  (5.71146)
     | > loss_mel: 22.94620  (24.12295)
     | > loss_duration: 1.64359  (1.70048)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.69382  (36.02099)
     | > grad_norm_0: 139.33163  (106.66219)
     | > loss_disc: 1.95357  (2.02654)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.95357  (2.02654)
     | > grad_norm_1: 10.21036  (13.23421)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.22940  (2.08208)
     | > loader_time: 0.17880  (0.05353)


[1m   --> STEP: 117/161 -- GLOBAL_STEP: 9675[0m
     | > loss_gen: 2.64736  (2.85753)
     | > loss_kl: 1.56774  (1.63570)
     | > loss_feat: 5.38695  (5.73393)
     | > loss_mel: 22.63673  (24.09552)
     | > loss_duration: 1.71735  (1.70025)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.95613  (36.02293)
     | > grad_norm_0: 96.02588  (108.01740)
     | > loss_disc: 2.02615  (2.02113)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.02615  (2.02113)
     | > grad_norm_1: 8.37334  (14.49099)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.47210  (2.11613)
     | > loader_time: 0.04670  (0.05318)


[1m   --> STEP: 142/161 -- GLOBAL_STEP: 9700[0m
     | > loss_gen: 3.06767  (2.85605)
     | > loss_kl: 1.82898  (1.63250)
     | > loss_feat: 5.79329  (5.71405)
     | > loss_mel: 23.71529  (24.07426)
     | > loss_duration: 1.64066  (1.69929)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.04588  (35.97616)
     | > grad_norm_0: 209.97801  (107.74751)
     | > loss_disc: 2.07448  (2.02106)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.07448  (2.02106)
     | > grad_norm_1: 16.39514  (14.66421)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.26130  (2.13440)
     | > loader_time: 0.11650  (0.05419)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02719 [0m(-0.00010)
     | > avg_loss_gen:[91m 2.97620 [0m(+0.32421)
     | > avg_loss_kl:[92m 1.73851 [0m(-0.02576)
     | > avg_loss_feat:[91m 6.29239 [0m(+0.96549)
     | > avg_loss_mel:[91m 24.40841 [0m(+2.94973)
     | > avg_loss_duration:[92m 1.68126 [0m(-0.00795)
     | > avg_loss_0:[91m 37.09676 [0m(+4.20572)
     | > avg_loss_disc:[92m 1.91406 [0m(-0.05494)
     | > avg_loss_1:[92m 1.91406 [0m(-0.05494)


[4m[1m > EPOCH: 60/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 22:22:23) [0m

[1m   --> STEP: 5/161 -- GLOBAL_STEP: 9725[0m
     | > loss_gen: 2.69174  (2.79421)
     | > loss_kl: 1.53393  (1.58287)
     | > loss_feat: 6.34589  (5.77951)
     | > loss_mel: 24.14452  (23.90247)
     | > loss_duration: 1.77955  (1.71302)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.49564  (35.77207)
     | > grad_norm_0: 126.27037  (122.88385)
     | > loss_disc: 1.95561  (1.94549)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.95561  (1.94549)
     | > grad_norm_1: 8.05703  (8.84806)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.40150  (2.06292)
     | > loader_time: 0.11960  (0.08076)


[1m   --> STEP: 30/161 -- GLOBAL_STEP: 9750[0m
     | > loss_gen: 2.68023  (2.84889)
     | > loss_kl: 1.80829  (1.61622)
     | > loss_feat: 5.60810  (5.73332)
     | > loss_mel: 23.92306  (23.86375)
     | > loss_duration: 1.68205  (1.70357)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.70172  (35.76575)
     | > grad_norm_0: 47.69418  (110.55992)
     | > loss_disc: 2.15980  (2.01784)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.15980  (2.01784)
     | > grad_norm_1: 20.98147  (13.07740)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.48720  (2.07676)
     | > loader_time: 0.04060  (0.04516)


[1m   --> STEP: 55/161 -- GLOBAL_STEP: 9775[0m
     | > loss_gen: 2.71759  (2.82358)
     | > loss_kl: 1.47624  (1.62871)
     | > loss_feat: 5.52080  (5.62202)
     | > loss_mel: 23.11008  (23.75755)
     | > loss_duration: 1.72905  (1.70078)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.55376  (35.53264)
     | > grad_norm_0: 169.32454  (105.73850)
     | > loss_disc: 1.99500  (2.03413)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.99500  (2.03413)
     | > grad_norm_1: 14.10473  (14.44488)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.18450  (2.08773)
     | > loader_time: 0.04060  (0.04226)


[1m   --> STEP: 80/161 -- GLOBAL_STEP: 9800[0m
     | > loss_gen: 2.54076  (2.84317)
     | > loss_kl: 1.67935  (1.62831)
     | > loss_feat: 5.47453  (5.68947)
     | > loss_mel: 25.02686  (23.79986)
     | > loss_duration: 1.62399  (1.70204)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.34549  (35.66285)
     | > grad_norm_0: 85.01803  (105.25275)
     | > loss_disc: 2.11435  (2.02961)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.11435  (2.02961)
     | > grad_norm_1: 15.94777  (14.39928)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.00450  (2.07047)
     | > loader_time: 0.03260  (0.04269)


[1m   --> STEP: 105/161 -- GLOBAL_STEP: 9825[0m
     | > loss_gen: 2.67723  (2.84169)
     | > loss_kl: 1.75798  (1.62292)
     | > loss_feat: 5.36120  (5.67157)
     | > loss_mel: 23.16142  (23.81681)
     | > loss_duration: 1.71306  (1.70014)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.67089  (35.65313)
     | > grad_norm_0: 175.47005  (101.97909)
     | > loss_disc: 1.99363  (2.02685)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.99363  (2.02685)
     | > grad_norm_1: 19.83161  (14.53100)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.21640  (2.08206)
     | > loader_time: 0.03620  (0.04270)


[1m   --> STEP: 130/161 -- GLOBAL_STEP: 9850[0m
     | > loss_gen: 2.92096  (2.83981)
     | > loss_kl: 1.85969  (1.62069)
     | > loss_feat: 5.76671  (5.66892)
     | > loss_mel: 23.47096  (23.81844)
     | > loss_duration: 1.66993  (1.70323)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.68824  (35.65109)
     | > grad_norm_0: 157.49852  (102.00719)
     | > loss_disc: 2.11969  (2.02881)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.11969  (2.02881)
     | > grad_norm_1: 17.64768  (14.22431)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.23300  (2.13365)
     | > loader_time: 0.03920  (0.04457)


[1m   --> STEP: 155/161 -- GLOBAL_STEP: 9875[0m
     | > loss_gen: 2.92099  (2.84167)
     | > loss_kl: 1.86943  (1.62159)
     | > loss_feat: 5.45996  (5.66689)
     | > loss_mel: 23.52858  (23.82803)
     | > loss_duration: 1.64769  (1.70362)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.42666  (35.66181)
     | > grad_norm_0: 144.72728  (102.71499)
     | > loss_disc: 1.90738  (2.02801)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.90738  (2.02801)
     | > grad_norm_1: 6.95197  (14.37249)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.86020  (2.12668)
     | > loader_time: 0.02930  (0.04505)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02683 [0m(-0.00036)
     | > avg_loss_gen:[91m 3.11801 [0m(+0.14181)
     | > avg_loss_kl:[92m 1.66161 [0m(-0.07690)
     | > avg_loss_feat:[92m 6.06553 [0m(-0.22686)
     | > avg_loss_mel:[92m 23.20128 [0m(-1.20713)
     | > avg_loss_duration:[92m 1.67845 [0m(-0.00280)
     | > avg_loss_0:[92m 35.72488 [0m(-1.37188)
     | > avg_loss_disc:[91m 2.08526 [0m(+0.17120)
     | > avg_loss_1:[91m 2.08526 [0m(+0.17120)


[4m[1m > EPOCH: 61/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 22:28:28) [0m

[1m   --> STEP: 18/161 -- GLOBAL_STEP: 9900[0m
     | > loss_gen: 2.87932  (2.79650)
     | > loss_kl: 1.62687  (1.65839)
     | > loss_feat: 5.03487  (5.52039)
     | > loss_mel: 22.06733  (23.78019)
     | > loss_duration: 1.75767  (1.69344)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.36607  (35.44892)
     | > grad_norm_0: 78.69685  (109.21826)
     | > loss_disc: 2.25446  (2.06354)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.25446  (2.06354)
     | > grad_norm_1: 38.14807  (15.00161)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.05440  (2.04172)
     | > loader_time: 0.03180  (0.04140)


[1m   --> STEP: 43/161 -- GLOBAL_STEP: 9925[0m
     | > loss_gen: 2.99450  (2.83453)
     | > loss_kl: 1.55200  (1.64339)
     | > loss_feat: 6.00216  (5.65379)
     | > loss_mel: 23.98018  (23.85842)
     | > loss_duration: 1.80164  (1.69880)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.33048  (35.68893)
     | > grad_norm_0: 246.51115  (120.84645)
     | > loss_disc: 1.95102  (2.03025)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.95102  (2.03025)
     | > grad_norm_1: 9.57765  (16.66622)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.46780  (2.12094)
     | > loader_time: 0.03770  (0.05326)


[1m   --> STEP: 68/161 -- GLOBAL_STEP: 9950[0m
     | > loss_gen: 2.63236  (2.83363)
     | > loss_kl: 1.60343  (1.62760)
     | > loss_feat: 5.35873  (5.67164)
     | > loss_mel: 23.70957  (23.84125)
     | > loss_duration: 1.74324  (1.69709)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.04734  (35.67121)
     | > grad_norm_0: 201.96938  (118.44533)
     | > loss_disc: 2.11931  (2.02873)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.11931  (2.02873)
     | > grad_norm_1: 12.26548  (15.97516)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.05460  (2.07643)
     | > loader_time: 0.03290  (0.05560)


[1m   --> STEP: 93/161 -- GLOBAL_STEP: 9975[0m
     | > loss_gen: 2.70510  (2.84973)
     | > loss_kl: 1.54974  (1.62486)
     | > loss_feat: 5.79325  (5.73202)
     | > loss_mel: 24.61392  (23.93454)
     | > loss_duration: 1.78179  (1.69700)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.44381  (35.83816)
     | > grad_norm_0: 78.26758  (114.64459)
     | > loss_disc: 1.99141  (2.01595)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.99141  (2.01595)
     | > grad_norm_1: 7.38944  (15.19422)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.80220  (2.08334)
     | > loader_time: 0.04700  (0.05817)


[1m   --> STEP: 118/161 -- GLOBAL_STEP: 10000[0m
     | > loss_gen: 3.04124  (2.85577)
     | > loss_kl: 1.75366  (1.63399)
     | > loss_feat: 6.03258  (5.73687)
     | > loss_mel: 24.56616  (23.92831)
     | > loss_duration: 1.75023  (1.70056)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.14386  (35.85550)
     | > grad_norm_0: 41.99460  (110.77538)
     | > loss_disc: 1.85119  (2.02013)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.85119  (2.02013)
     | > grad_norm_1: 18.33112  (15.21257)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.43360  (2.11350)
     | > loader_time: 0.04450  (0.05721)


 > CHECKPOINT : /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000/checkpoint_10000.pth.tar

[1m   --> STEP: 143/161 -- GLOBAL_STEP: 10025[0m
     | > loss_gen: 3.15494  (2.85909)
     | > loss_kl: 1.54212  (1.63702)
     | > loss_feat: 5.84068  (5.74308)
     | > loss_mel: 24.67182  (23.98355)
     | > loss_duration: 1.67233  (1.69823)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.88188  (35.92096)
     | > grad_norm_0: 97.55450  (112.17912)
     | > loss_disc: 2.08613  (2.01902)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.08613  (2.01902)
     | > grad_norm_1: 12.63345  (15.30829)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.32270  (2.13228)
     | > loader_time: 0.03940  (0.06101)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02688 [0m(+0.00005)
     | > avg_loss_gen:[92m 2.97485 [0m(-0.14316)
     | > avg_loss_kl:[92m 1.64595 [0m(-0.01567)
     | > avg_loss_feat:[91m 6.07184 [0m(+0.00631)
     | > avg_loss_mel:[91m 23.95348 [0m(+0.75220)
     | > avg_loss_duration:[92m 1.67375 [0m(-0.00470)
     | > avg_loss_0:[91m 36.31987 [0m(+0.59499)
     | > avg_loss_disc:[92m 1.87075 [0m(-0.21451)
     | > avg_loss_1:[92m 1.87075 [0m(-0.21451)


[4m[1m > EPOCH: 62/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 22:34:38) [0m

[1m   --> STEP: 6/161 -- GLOBAL_STEP: 10050[0m
     | > loss_gen: 3.08892  (2.78055)
     | > loss_kl: 1.72986  (1.66789)
     | > loss_feat: 4.69348  (5.39134)
     | > loss_mel: 24.85381  (23.83315)
     | > loss_duration: 1.74411  (1.69647)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 36.11017  (35.36940)
     | > grad_norm_0: 149.20775  (98.99156)
     | > loss_disc: 2.19906  (2.09949)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.19906  (2.09949)
     | > grad_norm_1: 121.74088  (48.70515)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.39110  (2.07834)
     | > loader_time: 0.04290  (0.06152)


[1m   --> STEP: 31/161 -- GLOBAL_STEP: 10075[0m
     | > loss_gen: 3.18271  (2.83922)
     | > loss_kl: 1.67782  (1.64980)
     | > loss_feat: 5.71620  (5.67738)
     | > loss_mel: 25.21790  (24.12140)
     | > loss_duration: 1.80333  (1.70288)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 37.59795  (35.99068)
     | > grad_norm_0: 197.57091  (103.77119)
     | > loss_disc: 1.98973  (2.03055)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.98973  (2.03055)
     | > grad_norm_1: 13.10343  (19.41658)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.46120  (2.07513)
     | > loader_time: 0.04560  (0.04045)


[1m   --> STEP: 56/161 -- GLOBAL_STEP: 10100[0m
     | > loss_gen: 2.87805  (2.83555)
     | > loss_kl: 1.48079  (1.64380)
     | > loss_feat: 5.21566  (5.68988)
     | > loss_mel: 24.79134  (24.12694)
     | > loss_duration: 1.72897  (1.69861)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 36.09479  (35.99478)
     | > grad_norm_0: 109.52171  (114.46554)
     | > loss_disc: 2.01573  (2.03374)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.01573  (2.03374)
     | > grad_norm_1: 7.15825  (16.31735)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.17410  (2.07690)
     | > loader_time: 0.03730  (0.04833)


[1m   --> STEP: 81/161 -- GLOBAL_STEP: 10125[0m
     | > loss_gen: 3.12091  (2.84375)
     | > loss_kl: 1.46885  (1.63017)
     | > loss_feat: 5.25665  (5.71247)
     | > loss_mel: 22.39743  (24.06988)
     | > loss_duration: 1.73529  (1.69459)
     | > amp_scaler: 256.00000  (467.75309)
     | > loss_0: 33.97913  (35.95085)
     | > grad_norm_0: 181.47270  (109.58006)
     | > loss_disc: 2.02959  (2.02714)
     | > amp_scaler-1: 256.00000  (467.75309)
     | > loss_1: 2.02959  (2.02714)
     | > grad_norm_1: 19.52006  (15.58778)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.86370  (2.06473)
     | > loader_time: 0.04540  (0.04906)


[1m   --> STEP: 106/161 -- GLOBAL_STEP: 10150[0m
     | > loss_gen: 2.80682  (2.85446)
     | > loss_kl: 1.53952  (1.62710)
     | > loss_feat: 5.39548  (5.76147)
     | > loss_mel: 23.88958  (24.10012)
     | > loss_duration: 1.72465  (1.69618)
     | > amp_scaler: 256.00000  (417.81132)
     | > loss_0: 35.35605  (36.03934)
     | > grad_norm_0: 67.65015  (112.54906)
     | > loss_disc: 2.18790  (2.01886)
     | > amp_scaler-1: 256.00000  (417.81132)
     | > loss_1: 2.18790  (2.01886)
     | > grad_norm_1: 18.89833  (14.95866)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.96500  (2.07275)
     | > loader_time: 0.10330  (0.05340)


[1m   --> STEP: 131/161 -- GLOBAL_STEP: 10175[0m
     | > loss_gen: 2.99548  (2.85189)
     | > loss_kl: 1.60218  (1.61681)
     | > loss_feat: 5.66610  (5.74050)
     | > loss_mel: 23.81255  (23.99145)
     | > loss_duration: 1.71580  (1.69894)
     | > amp_scaler: 256.00000  (386.93130)
     | > loss_0: 35.79212  (35.89960)
     | > grad_norm_0: 87.08548  (110.63443)
     | > loss_disc: 1.96110  (2.02235)
     | > amp_scaler-1: 256.00000  (386.93130)
     | > loss_1: 1.96110  (2.02235)
     | > grad_norm_1: 12.71204  (14.66321)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.07340  (2.14309)
     | > loader_time: 0.03770  (0.05571)


[1m   --> STEP: 156/161 -- GLOBAL_STEP: 10200[0m
     | > loss_gen: 2.61131  (2.85099)
     | > loss_kl: 1.73756  (1.62377)
     | > loss_feat: 5.13177  (5.73675)
     | > loss_mel: 23.00541  (23.97537)
     | > loss_duration: 1.67282  (1.70009)
     | > amp_scaler: 256.00000  (365.94872)
     | > loss_0: 34.15886  (35.88697)
     | > grad_norm_0: 73.81769  (106.00436)
     | > loss_disc: 2.09237  (2.02220)
     | > amp_scaler-1: 256.00000  (365.94872)
     | > loss_1: 2.09237  (2.02220)
     | > grad_norm_1: 22.76764  (14.65483)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.91080  (2.12947)
     | > loader_time: 0.03190  (0.05473)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02710 [0m(+0.00022)
     | > avg_loss_gen:[92m 2.64882 [0m(-0.32603)
     | > avg_loss_kl:[91m 1.81307 [0m(+0.16713)
     | > avg_loss_feat:[91m 6.21113 [0m(+0.13929)
     | > avg_loss_mel:[91m 24.80148 [0m(+0.84799)
     | > avg_loss_duration:[91m 1.69159 [0m(+0.01784)
     | > avg_loss_0:[91m 37.16609 [0m(+0.84622)
     | > avg_loss_disc:[91m 2.04138 [0m(+0.17063)
     | > avg_loss_1:[91m 2.04138 [0m(+0.17063)


[4m[1m > EPOCH: 63/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 22:40:44) [0m

[1m   --> STEP: 19/161 -- GLOBAL_STEP: 10225[0m
     | > loss_gen: 3.06471  (2.87713)
     | > loss_kl: 1.26745  (1.59876)
     | > loss_feat: 5.23462  (5.73733)
     | > loss_mel: 23.37688  (24.38638)
     | > loss_duration: 1.78547  (1.70376)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.72914  (36.30336)
     | > grad_norm_0: 38.03697  (84.83934)
     | > loss_disc: 2.11090  (2.03941)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.11090  (2.03941)
     | > grad_norm_1: 13.66929  (16.34998)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.02780  (2.07606)
     | > loader_time: 0.03410  (0.06817)


[1m   --> STEP: 44/161 -- GLOBAL_STEP: 10250[0m
     | > loss_gen: 2.72709  (2.89020)
     | > loss_kl: 1.61244  (1.58985)
     | > loss_feat: 5.46409  (5.78305)
     | > loss_mel: 23.14113  (24.01041)
     | > loss_duration: 1.69864  (1.70332)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.64338  (35.97682)
     | > grad_norm_0: 38.19254  (104.04936)
     | > loss_disc: 2.00083  (2.01487)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.00083  (2.01487)
     | > grad_norm_1: 7.52694  (15.36337)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.90100  (2.09986)
     | > loader_time: 0.03080  (0.06953)


[1m   --> STEP: 69/161 -- GLOBAL_STEP: 10275[0m
     | > loss_gen: 2.87204  (2.86524)
     | > loss_kl: 1.74622  (1.61585)
     | > loss_feat: 6.01070  (5.75184)
     | > loss_mel: 23.99949  (23.86206)
     | > loss_duration: 1.71063  (1.70214)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.33908  (35.79713)
     | > grad_norm_0: 131.53470  (110.47124)
     | > loss_disc: 1.99899  (2.02084)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.99899  (2.02084)
     | > grad_norm_1: 8.19932  (14.62816)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.91500  (2.06913)
     | > loader_time: 0.15360  (0.06689)


[1m   --> STEP: 94/161 -- GLOBAL_STEP: 10300[0m
     | > loss_gen: 2.73602  (2.88029)
     | > loss_kl: 1.62321  (1.61423)
     | > loss_feat: 6.08583  (5.81949)
     | > loss_mel: 24.76735  (23.92297)
     | > loss_duration: 1.75650  (1.69980)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.96891  (35.93678)
     | > grad_norm_0: 159.26166  (113.64650)
     | > loss_disc: 1.89914  (2.00364)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.89914  (2.00364)
     | > grad_norm_1: 9.88838  (13.72626)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.79610  (2.08431)
     | > loader_time: 0.05080  (0.05986)


[1m   --> STEP: 119/161 -- GLOBAL_STEP: 10325[0m
     | > loss_gen: 2.91196  (2.87114)
     | > loss_kl: 1.58481  (1.61715)
     | > loss_feat: 6.16356  (5.81426)
     | > loss_mel: 25.53314  (23.98455)
     | > loss_duration: 1.71871  (1.70126)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.91218  (35.98836)
     | > grad_norm_0: 173.78203  (112.61533)
     | > loss_disc: 1.89533  (2.00562)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.89533  (2.00562)
     | > grad_norm_1: 9.62334  (15.08415)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.97280  (2.10549)
     | > loader_time: 0.03640  (0.05879)


[1m   --> STEP: 144/161 -- GLOBAL_STEP: 10350[0m
     | > loss_gen: 2.75341  (2.87010)
     | > loss_kl: 1.50935  (1.61760)
     | > loss_feat: 5.67840  (5.81155)
     | > loss_mel: 24.30128  (23.93856)
     | > loss_duration: 1.70476  (1.70247)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.94719  (35.94028)
     | > grad_norm_0: 118.24446  (115.34077)
     | > loss_disc: 2.14031  (2.00738)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.14031  (2.00738)
     | > grad_norm_1: 31.60373  (14.78816)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.02620  (2.12075)
     | > loader_time: 0.03620  (0.05756)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02654 [0m(-0.00056)
     | > avg_loss_gen:[92m 2.60227 [0m(-0.04655)
     | > avg_loss_kl:[92m 1.46956 [0m(-0.34352)
     | > avg_loss_feat:[91m 6.28243 [0m(+0.07130)
     | > avg_loss_mel:[92m 24.69324 [0m(-0.10823)
     | > avg_loss_duration:[92m 1.67557 [0m(-0.01602)
     | > avg_loss_0:[92m 36.72308 [0m(-0.44301)
     | > avg_loss_disc:[92m 1.89349 [0m(-0.14789)
     | > avg_loss_1:[92m 1.89349 [0m(-0.14789)


[4m[1m > EPOCH: 64/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 22:46:52) [0m

[1m   --> STEP: 7/161 -- GLOBAL_STEP: 10375[0m
     | > loss_gen: 3.27082  (2.91093)
     | > loss_kl: 1.44516  (1.59861)
     | > loss_feat: 5.88817  (5.76454)
     | > loss_mel: 24.03433  (23.63504)
     | > loss_duration: 1.71822  (1.68223)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.35671  (35.59135)
     | > grad_norm_0: 148.54779  (91.25016)
     | > loss_disc: 1.87756  (2.06708)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.87756  (2.06708)
     | > grad_norm_1: 20.35530  (13.94951)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.03900  (2.09908)
     | > loader_time: 0.03650  (0.08207)


[1m   --> STEP: 32/161 -- GLOBAL_STEP: 10400[0m
     | > loss_gen: 2.92563  (2.97555)
     | > loss_kl: 1.77357  (1.61854)
     | > loss_feat: 6.34069  (6.14530)
     | > loss_mel: 25.10930  (24.62999)
     | > loss_duration: 1.65827  (1.69269)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.80746  (37.06208)
     | > grad_norm_0: 237.75052  (104.87947)
     | > loss_disc: 1.82275  (2.00436)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.82275  (2.00436)
     | > grad_norm_1: 19.19464  (15.29417)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.32080  (2.08525)
     | > loader_time: 0.05020  (0.05575)


[1m   --> STEP: 57/161 -- GLOBAL_STEP: 10425[0m
     | > loss_gen: 2.62546  (2.95162)
     | > loss_kl: 1.70990  (1.63850)
     | > loss_feat: 6.07878  (6.09941)
     | > loss_mel: 24.32521  (24.62293)
     | > loss_duration: 1.67180  (1.69228)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.41114  (37.00475)
     | > grad_norm_0: 99.35112  (99.01758)
     | > loss_disc: 2.07130  (2.10388)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.07130  (2.10388)
     | > grad_norm_1: 15.12186  (17.59431)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.07430  (2.09364)
     | > loader_time: 0.03480  (0.04931)


[1m   --> STEP: 82/161 -- GLOBAL_STEP: 10450[0m
     | > loss_gen: 2.74099  (2.90844)
     | > loss_kl: 1.64601  (1.64580)
     | > loss_feat: 5.58620  (5.93254)
     | > loss_mel: 23.71660  (24.26373)
     | > loss_duration: 1.68348  (1.68821)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.37329  (36.43872)
     | > grad_norm_0: 75.65195  (92.94250)
     | > loss_disc: 2.05079  (2.09860)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.05079  (2.09860)
     | > grad_norm_1: 9.04097  (15.86494)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.73090  (2.07391)
     | > loader_time: 0.02790  (0.04997)


[1m   --> STEP: 107/161 -- GLOBAL_STEP: 10475[0m
     | > loss_gen: 2.96055  (2.88256)
     | > loss_kl: 1.64815  (1.63938)
     | > loss_feat: 5.76642  (5.83978)
     | > loss_mel: 23.35811  (24.15007)
     | > loss_duration: 1.66079  (1.69287)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.39402  (36.20466)
     | > grad_norm_0: 193.15894  (88.75846)
     | > loss_disc: 1.88435  (2.08155)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.88435  (2.08155)
     | > grad_norm_1: 10.46749  (14.41284)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.30280  (2.08423)
     | > loader_time: 0.05300  (0.05313)


[1m   --> STEP: 132/161 -- GLOBAL_STEP: 10500[0m
     | > loss_gen: 2.55741  (2.87462)
     | > loss_kl: 1.89935  (1.64164)
     | > loss_feat: 5.91366  (5.83020)
     | > loss_mel: 23.10494  (24.02787)
     | > loss_duration: 1.67215  (1.69714)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.14751  (36.07146)
     | > grad_norm_0: 222.46133  (88.57175)
     | > loss_disc: 2.06021  (2.06568)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06021  (2.06568)
     | > grad_norm_1: 18.26459  (14.63588)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.76810  (2.12720)
     | > loader_time: 0.02980  (0.05557)


[1m   --> STEP: 157/161 -- GLOBAL_STEP: 10525[0m
     | > loss_gen: 2.75236  (2.86826)
     | > loss_kl: 1.71110  (1.65192)
     | > loss_feat: 6.02441  (5.80299)
     | > loss_mel: 24.28150  (23.99235)
     | > loss_duration: 1.70475  (1.69604)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.47413  (36.01155)
     | > grad_norm_0: 72.55211  (90.47218)
     | > loss_disc: 1.93575  (2.05798)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.93575  (2.05798)
     | > grad_norm_1: 9.58148  (15.45982)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.02630  (2.12408)
     | > loader_time: 0.03560  (0.05575)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02719 [0m(+0.00066)
     | > avg_loss_gen:[91m 2.76505 [0m(+0.16278)
     | > avg_loss_kl:[91m 1.83103 [0m(+0.36147)
     | > avg_loss_feat:[92m 5.88784 [0m(-0.39459)
     | > avg_loss_mel:[92m 23.18428 [0m(-1.50896)
     | > avg_loss_duration:[91m 1.67904 [0m(+0.00347)
     | > avg_loss_0:[92m 35.34724 [0m(-1.37584)
     | > avg_loss_disc:[91m 1.98988 [0m(+0.09639)
     | > avg_loss_1:[91m 1.98988 [0m(+0.09639)


[4m[1m > EPOCH: 65/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 22:52:59) [0m

[1m   --> STEP: 20/161 -- GLOBAL_STEP: 10550[0m
     | > loss_gen: 2.97963  (2.89650)
     | > loss_kl: 1.53193  (1.68448)
     | > loss_feat: 6.00195  (5.88021)
     | > loss_mel: 23.19102  (23.98185)
     | > loss_duration: 1.70012  (1.68576)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.40466  (36.12880)
     | > grad_norm_0: 50.34105  (77.38853)
     | > loss_disc: 1.87144  (1.98276)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.87144  (1.98276)
     | > grad_norm_1: 11.91519  (12.56541)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.03290  (2.09378)
     | > loader_time: 0.03540  (0.07019)


[1m   --> STEP: 45/161 -- GLOBAL_STEP: 10575[0m
     | > loss_gen: 2.83363  (2.86936)
     | > loss_kl: 1.74002  (1.66845)
     | > loss_feat: 6.07064  (5.83983)
     | > loss_mel: 23.53557  (23.99915)
     | > loss_duration: 1.64478  (1.69213)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.82464  (36.06893)
     | > grad_norm_0: 80.12794  (78.59016)
     | > loss_disc: 1.99160  (1.99361)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.99160  (1.99361)
     | > grad_norm_1: 7.52168  (15.40450)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.02710  (2.11122)
     | > loader_time: 0.02690  (0.05362)


[1m   --> STEP: 70/161 -- GLOBAL_STEP: 10600[0m
     | > loss_gen: 2.55583  (2.85666)
     | > loss_kl: 1.78337  (1.67741)
     | > loss_feat: 5.47436  (5.81546)
     | > loss_mel: 25.66532  (24.03147)
     | > loss_duration: 1.70113  (1.69253)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.18002  (36.07352)
     | > grad_norm_0: 238.52260  (82.21515)
     | > loss_disc: 2.04502  (2.00923)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.04502  (2.00923)
     | > grad_norm_1: 26.63317  (14.36647)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.70600  (2.07414)
     | > loader_time: 0.03330  (0.04861)


[1m   --> STEP: 95/161 -- GLOBAL_STEP: 10625[0m
     | > loss_gen: 2.91501  (2.85499)
     | > loss_kl: 1.43388  (1.66378)
     | > loss_feat: 6.07692  (5.78965)
     | > loss_mel: 23.62862  (23.95145)
     | > loss_duration: 1.71446  (1.69564)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.76889  (35.95552)
     | > grad_norm_0: 73.64004  (84.42060)
     | > loss_disc: 1.91350  (2.00880)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.91350  (2.00880)
     | > grad_norm_1: 6.54162  (14.63694)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.32140  (2.08877)
     | > loader_time: 0.14040  (0.04772)


[1m   --> STEP: 120/161 -- GLOBAL_STEP: 10650[0m
     | > loss_gen: 2.88761  (2.86093)
     | > loss_kl: 1.63662  (1.65451)
     | > loss_feat: 6.24128  (5.80382)
     | > loss_mel: 23.63327  (23.88502)
     | > loss_duration: 1.69832  (1.69963)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.09711  (35.90390)
     | > grad_norm_0: 135.56590  (92.36796)
     | > loss_disc: 1.95195  (2.00795)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.95195  (2.00795)
     | > grad_norm_1: 10.66422  (14.53417)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.46890  (2.11271)
     | > loader_time: 0.03900  (0.05078)


[1m   --> STEP: 145/161 -- GLOBAL_STEP: 10675[0m
     | > loss_gen: 2.87905  (2.84986)
     | > loss_kl: 1.67427  (1.65318)
     | > loss_feat: 5.42231  (5.76592)
     | > loss_mel: 23.26068  (23.86748)
     | > loss_duration: 1.68285  (1.69960)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.91917  (35.83604)
     | > grad_norm_0: 36.89635  (92.22212)
     | > loss_disc: 2.16611  (2.01526)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.16611  (2.01526)
     | > grad_norm_1: 12.13167  (15.03757)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.85970  (2.13117)
     | > loader_time: 0.11370  (0.05028)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02738 [0m(+0.00018)
     | > avg_loss_gen:[92m 2.63658 [0m(-0.12846)
     | > avg_loss_kl:[92m 1.66558 [0m(-0.16545)
     | > avg_loss_feat:[92m 4.91071 [0m(-0.97712)
     | > avg_loss_mel:[92m 22.04421 [0m(-1.14008)
     | > avg_loss_duration:[91m 1.67983 [0m(+0.00079)
     | > avg_loss_0:[92m 32.93691 [0m(-2.41032)
     | > avg_loss_disc:[91m 2.17215 [0m(+0.18227)
     | > avg_loss_1:[91m 2.17215 [0m(+0.18227)


[4m[1m > EPOCH: 66/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 22:59:06) [0m

[1m   --> STEP: 8/161 -- GLOBAL_STEP: 10700[0m
     | > loss_gen: 2.59864  (2.83886)
     | > loss_kl: 1.75779  (1.68651)
     | > loss_feat: 6.00183  (5.76123)
     | > loss_mel: 24.29326  (23.45771)
     | > loss_duration: 1.70053  (1.68455)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.35205  (35.42886)
     | > grad_norm_0: 54.76681  (79.46227)
     | > loss_disc: 2.06448  (2.02070)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06448  (2.02070)
     | > grad_norm_1: 10.42182  (14.55598)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.02110  (2.09174)
     | > loader_time: 0.03110  (0.05609)


[1m   --> STEP: 33/161 -- GLOBAL_STEP: 10725[0m
     | > loss_gen: 2.75983  (2.85045)
     | > loss_kl: 1.62266  (1.63113)
     | > loss_feat: 6.64202  (5.80126)
     | > loss_mel: 23.57046  (23.61084)
     | > loss_duration: 1.70069  (1.69187)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.29565  (35.58555)
     | > grad_norm_0: 106.08493  (102.21436)
     | > loss_disc: 1.97320  (2.01076)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.97320  (2.01076)
     | > grad_norm_1: 7.24301  (15.84085)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.00250  (2.09639)
     | > loader_time: 0.03250  (0.03976)


[1m   --> STEP: 58/161 -- GLOBAL_STEP: 10750[0m
     | > loss_gen: 2.97083  (2.84483)
     | > loss_kl: 1.37896  (1.63452)
     | > loss_feat: 5.43377  (5.79064)
     | > loss_mel: 22.25615  (23.73482)
     | > loss_duration: 1.64771  (1.68758)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.68742  (35.69239)
     | > grad_norm_0: 74.28445  (99.01752)
     | > loss_disc: 2.15542  (2.02482)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.15542  (2.02482)
     | > grad_norm_1: 14.15196  (16.14045)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.99680  (2.07962)
     | > loader_time: 0.03210  (0.04255)


[1m   --> STEP: 83/161 -- GLOBAL_STEP: 10775[0m
     | > loss_gen: 2.59669  (2.84980)
     | > loss_kl: 1.66831  (1.62947)
     | > loss_feat: 5.05669  (5.79240)
     | > loss_mel: 24.13654  (23.83275)
     | > loss_duration: 1.68138  (1.68981)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.13961  (35.79423)
     | > grad_norm_0: 53.98011  (96.92717)
     | > loss_disc: 2.17800  (2.02495)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.17800  (2.02495)
     | > grad_norm_1: 7.72309  (16.02786)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.35950  (2.06266)
     | > loader_time: 0.04030  (0.05019)


[1m   --> STEP: 108/161 -- GLOBAL_STEP: 10800[0m
     | > loss_gen: 3.15757  (2.84031)
     | > loss_kl: 1.52225  (1.63978)
     | > loss_feat: 5.52440  (5.74133)
     | > loss_mel: 23.28791  (23.81532)
     | > loss_duration: 1.67313  (1.69322)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.16526  (35.72996)
     | > grad_norm_0: 148.04073  (100.25001)
     | > loss_disc: 2.04911  (2.03567)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.04911  (2.03567)
     | > grad_norm_1: 21.93721  (16.49506)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.35550  (2.08057)
     | > loader_time: 0.03950  (0.04905)


[1m   --> STEP: 133/161 -- GLOBAL_STEP: 10825[0m
     | > loss_gen: 2.79725  (2.83665)
     | > loss_kl: 1.49572  (1.63197)
     | > loss_feat: 5.50079  (5.73651)
     | > loss_mel: 25.45756  (23.79352)
     | > loss_duration: 1.75007  (1.69801)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.00138  (35.69666)
     | > grad_norm_0: 89.52811  (100.66969)
     | > loss_disc: 2.06744  (2.03682)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06744  (2.03682)
     | > grad_norm_1: 19.06404  (16.29843)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.81870  (2.12159)
     | > loader_time: 0.03010  (0.05028)


[1m   --> STEP: 158/161 -- GLOBAL_STEP: 10850[0m
     | > loss_gen: 2.99386  (2.84356)
     | > loss_kl: 1.68689  (1.64342)
     | > loss_feat: 6.40419  (5.75983)
     | > loss_mel: 23.93405  (23.81345)
     | > loss_duration: 1.68568  (1.69716)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.70467  (35.75743)
     | > grad_norm_0: 291.80603  (100.42685)
     | > loss_disc: 1.86064  (2.02986)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.86064  (2.02986)
     | > grad_norm_1: 21.40977  (16.13468)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.17650  (2.11808)
     | > loader_time: 0.03740  (0.04995)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02825 [0m(+0.00087)
     | > avg_loss_gen:[91m 2.80186 [0m(+0.16528)
     | > avg_loss_kl:[92m 1.63282 [0m(-0.03276)
     | > avg_loss_feat:[91m 5.24146 [0m(+0.33075)
     | > avg_loss_mel:[92m 21.44685 [0m(-0.59736)
     | > avg_loss_duration:[92m 1.67676 [0m(-0.00307)
     | > avg_loss_0:[92m 32.79975 [0m(-0.13717)
     | > avg_loss_disc:[92m 2.11315 [0m(-0.05900)
     | > avg_loss_1:[92m 2.11315 [0m(-0.05900)


[4m[1m > EPOCH: 67/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 23:05:12) [0m

[1m   --> STEP: 21/161 -- GLOBAL_STEP: 10875[0m
     | > loss_gen: 2.84651  (2.82933)
     | > loss_kl: 1.55824  (1.65560)
     | > loss_feat: 6.01782  (5.64641)
     | > loss_mel: 23.93308  (23.83797)
     | > loss_duration: 1.66296  (1.69629)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.01860  (35.66560)
     | > grad_norm_0: 54.06349  (124.84332)
     | > loss_disc: 2.05605  (2.04636)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.05605  (2.04636)
     | > grad_norm_1: 7.71249  (15.07186)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.03110  (2.02122)
     | > loader_time: 0.03580  (0.04303)


[1m   --> STEP: 46/161 -- GLOBAL_STEP: 10900[0m
     | > loss_gen: 3.02663  (2.82384)
     | > loss_kl: 1.54708  (1.63977)
     | > loss_feat: 5.54430  (5.65531)
     | > loss_mel: 24.00249  (23.82689)
     | > loss_duration: 1.67345  (1.69602)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.79395  (35.64183)
     | > grad_norm_0: 102.13193  (112.98119)
     | > loss_disc: 2.10804  (2.05363)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.10804  (2.05363)
     | > grad_norm_1: 16.91605  (14.27395)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.54980  (2.07651)
     | > loader_time: 0.04400  (0.04379)


[1m   --> STEP: 71/161 -- GLOBAL_STEP: 10925[0m
     | > loss_gen: 3.08539  (2.83868)
     | > loss_kl: 1.64851  (1.63169)
     | > loss_feat: 5.72186  (5.69428)
     | > loss_mel: 23.46776  (23.76786)
     | > loss_duration: 1.62070  (1.68880)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.54422  (35.62130)
     | > grad_norm_0: 103.22417  (114.24294)
     | > loss_disc: 2.02432  (2.04297)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.02432  (2.04297)
     | > grad_norm_1: 7.07594  (15.12835)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.36340  (2.05431)
     | > loader_time: 0.03380  (0.04419)


[1m   --> STEP: 96/161 -- GLOBAL_STEP: 10950[0m
     | > loss_gen: 2.68957  (2.84474)
     | > loss_kl: 1.46832  (1.63709)
     | > loss_feat: 5.44762  (5.70728)
     | > loss_mel: 24.04026  (23.75269)
     | > loss_duration: 1.77517  (1.69210)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.42095  (35.63391)
     | > grad_norm_0: 175.09782  (115.78369)
     | > loss_disc: 2.08825  (2.03161)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.08825  (2.03161)
     | > grad_norm_1: 11.47482  (15.00955)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.39090  (2.08205)
     | > loader_time: 0.04340  (0.04698)


[1m   --> STEP: 121/161 -- GLOBAL_STEP: 10975[0m
     | > loss_gen: 3.11029  (2.85091)
     | > loss_kl: 1.67574  (1.63671)
     | > loss_feat: 5.87465  (5.76214)
     | > loss_mel: 23.60726  (23.78945)
     | > loss_duration: 1.73058  (1.69577)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.99853  (35.73498)
     | > grad_norm_0: 71.92734  (111.82814)
     | > loss_disc: 1.98374  (2.02371)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.98374  (2.02371)
     | > grad_norm_1: 13.74227  (15.49736)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.50380  (2.12435)
     | > loader_time: 0.04650  (0.05157)


[1m   --> STEP: 146/161 -- GLOBAL_STEP: 11000[0m
     | > loss_gen: 2.83152  (2.85143)
     | > loss_kl: 1.65728  (1.63543)
     | > loss_feat: 5.39489  (5.76787)
     | > loss_mel: 23.46268  (23.75751)
     | > loss_duration: 1.73887  (1.69650)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.08524  (35.70875)
     | > grad_norm_0: 154.53030  (112.62650)
     | > loss_disc: 2.24420  (2.02562)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.24420  (2.02562)
     | > grad_norm_1: 12.73539  (15.48358)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.28480  (2.13926)
     | > loader_time: 0.03920  (0.05085)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02779 [0m(-0.00045)
     | > avg_loss_gen:[91m 3.01198 [0m(+0.21012)
     | > avg_loss_kl:[91m 1.67127 [0m(+0.03845)
     | > avg_loss_feat:[91m 6.01641 [0m(+0.77494)
     | > avg_loss_mel:[91m 22.88945 [0m(+1.44260)
     | > avg_loss_duration:[91m 1.68114 [0m(+0.00438)
     | > avg_loss_0:[91m 35.27024 [0m(+2.47050)
     | > avg_loss_disc:[92m 1.93480 [0m(-0.17835)
     | > avg_loss_1:[92m 1.93480 [0m(-0.17835)


[4m[1m > EPOCH: 68/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 23:11:18) [0m

[1m   --> STEP: 9/161 -- GLOBAL_STEP: 11025[0m
     | > loss_gen: 3.09570  (2.81901)
     | > loss_kl: 1.64552  (1.66262)
     | > loss_feat: 5.58295  (5.64044)
     | > loss_mel: 23.99727  (24.01873)
     | > loss_duration: 1.70333  (1.67286)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.02477  (35.81366)
     | > grad_norm_0: 148.53798  (138.84029)
     | > loss_disc: 2.01164  (2.04534)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.01164  (2.04534)
     | > grad_norm_1: 9.31004  (14.28741)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.54420  (2.03819)
     | > loader_time: 0.09030  (0.09339)


[1m   --> STEP: 34/161 -- GLOBAL_STEP: 11050[0m
     | > loss_gen: 3.02805  (2.86397)
     | > loss_kl: 1.74327  (1.67277)
     | > loss_feat: 5.99222  (5.86373)
     | > loss_mel: 22.62972  (23.65347)
     | > loss_duration: 1.67863  (1.70078)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.07190  (35.75471)
     | > grad_norm_0: 87.62324  (106.07464)
     | > loss_disc: 1.97200  (2.00854)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.97200  (2.00854)
     | > grad_norm_1: 6.54211  (11.86498)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.99920  (2.12335)
     | > loader_time: 0.04390  (0.05897)


[1m   --> STEP: 59/161 -- GLOBAL_STEP: 11075[0m
     | > loss_gen: 2.70452  (2.87497)
     | > loss_kl: 1.77665  (1.67281)
     | > loss_feat: 6.76294  (5.92714)
     | > loss_mel: 23.59854  (23.60575)
     | > loss_duration: 1.65331  (1.69849)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.49596  (35.77917)
     | > grad_norm_0: 98.80889  (118.84465)
     | > loss_disc: 1.80948  (1.98886)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.80948  (1.98886)
     | > grad_norm_1: 11.87568  (11.87141)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.74160  (2.09747)
     | > loader_time: 0.02890  (0.05044)


[1m   --> STEP: 84/161 -- GLOBAL_STEP: 11100[0m
     | > loss_gen: 3.40679  (2.91842)
     | > loss_kl: 1.74396  (1.67689)
     | > loss_feat: 7.35194  (6.02480)
     | > loss_mel: 22.44674  (23.79857)
     | > loss_duration: 1.65876  (1.69597)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.60819  (36.11464)
     | > grad_norm_0: 132.57747  (113.63614)
     | > loss_disc: 1.53275  (2.00199)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.53275  (2.00199)
     | > grad_norm_1: 10.86450  (12.55421)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.97490  (2.08700)
     | > loader_time: 0.03600  (0.05328)


[1m   --> STEP: 109/161 -- GLOBAL_STEP: 11125[0m
     | > loss_gen: 2.73030  (2.91627)
     | > loss_kl: 1.41320  (1.66668)
     | > loss_feat: 5.56101  (5.98739)
     | > loss_mel: 22.94606  (23.85192)
     | > loss_duration: 1.73311  (1.70089)
     | > amp_scaler: 512.00000  (288.88073)
     | > loss_0: 34.38369  (36.12315)
     | > grad_norm_0: 104.97741  (111.15848)
     | > loss_disc: 1.94522  (2.03705)
     | > amp_scaler-1: 512.00000  (288.88073)
     | > loss_1: 1.94522  (2.03705)
     | > grad_norm_1: 12.72195  (13.84273)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.52380  (2.09717)
     | > loader_time: 0.04640  (0.05459)


[1m   --> STEP: 134/161 -- GLOBAL_STEP: 11150[0m
     | > loss_gen: 2.73879  (2.89220)
     | > loss_kl: 1.83119  (1.66403)
     | > loss_feat: 5.18787  (5.90684)
     | > loss_mel: 23.39443  (23.79651)
     | > loss_duration: 1.64297  (1.69914)
     | > amp_scaler: 256.00000  (296.11940)
     | > loss_0: 34.79525  (35.95872)
     | > grad_norm_0: 290.16434  (109.80108)
     | > loss_disc: 2.24175  (2.04530)
     | > amp_scaler-1: 256.00000  (296.11940)
     | > loss_1: 2.24175  (2.04530)
     | > grad_norm_1: 37.70302  (14.74976)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.62660  (2.14091)
     | > loader_time: 0.02440  (0.05445)


[1m   --> STEP: 159/161 -- GLOBAL_STEP: 11175[0m
     | > loss_gen: 2.89784  (2.88275)
     | > loss_kl: 1.82940  (1.66545)
     | > loss_feat: 6.55865  (5.87205)
     | > loss_mel: 24.60900  (23.79258)
     | > loss_duration: 1.67017  (1.70055)
     | > amp_scaler: 256.00000  (289.81132)
     | > loss_0: 37.56508  (35.91337)
     | > grad_norm_0: 100.90324  (108.95788)
     | > loss_disc: 1.94551  (2.04286)
     | > amp_scaler-1: 256.00000  (289.81132)
     | > loss_1: 1.94551  (2.04286)
     | > grad_norm_1: 7.72496  (14.70750)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.93830  (2.13027)
     | > loader_time: 0.03380  (0.05419)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02751 [0m(-0.00029)
     | > avg_loss_gen:[92m 2.91340 [0m(-0.09858)
     | > avg_loss_kl:[91m 1.78951 [0m(+0.11824)
     | > avg_loss_feat:[92m 5.35711 [0m(-0.65930)
     | > avg_loss_mel:[92m 22.60350 [0m(-0.28595)
     | > avg_loss_duration:[92m 1.67078 [0m(-0.01036)
     | > avg_loss_0:[92m 34.33430 [0m(-0.93594)
     | > avg_loss_disc:[91m 2.10386 [0m(+0.16906)
     | > avg_loss_1:[91m 2.10386 [0m(+0.16906)


[4m[1m > EPOCH: 69/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 23:17:26) [0m

[1m   --> STEP: 22/161 -- GLOBAL_STEP: 11200[0m
     | > loss_gen: 2.78142  (2.87043)
     | > loss_kl: 1.56913  (1.67637)
     | > loss_feat: 5.97954  (5.76206)
     | > loss_mel: 24.12318  (23.74402)
     | > loss_duration: 1.63997  (1.69873)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.09324  (35.75161)
     | > grad_norm_0: 117.75013  (96.74496)
     | > loss_disc: 1.96373  (2.00467)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.96373  (2.00467)
     | > grad_norm_1: 11.49390  (12.28051)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.92470  (2.02432)
     | > loader_time: 0.04170  (0.05672)


[1m   --> STEP: 47/161 -- GLOBAL_STEP: 11225[0m
     | > loss_gen: 2.51058  (2.87517)
     | > loss_kl: 1.92403  (1.65742)
     | > loss_feat: 6.29801  (5.83089)
     | > loss_mel: 23.89034  (23.98718)
     | > loss_duration: 1.65559  (1.69048)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.27856  (36.04114)
     | > grad_norm_0: 254.49698  (105.34990)
     | > loss_disc: 2.08247  (1.99334)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.08247  (1.99334)
     | > grad_norm_1: 12.53336  (12.31609)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.93160  (2.07365)
     | > loader_time: 0.03070  (0.04710)


[1m   --> STEP: 72/161 -- GLOBAL_STEP: 11250[0m
     | > loss_gen: 3.00299  (2.86506)
     | > loss_kl: 1.73470  (1.66327)
     | > loss_feat: 6.53428  (5.81477)
     | > loss_mel: 24.70767  (23.87086)
     | > loss_duration: 1.65914  (1.69415)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.63878  (35.90812)
     | > grad_norm_0: 52.29192  (106.70908)
     | > loss_disc: 1.77877  (2.00484)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.77877  (2.00484)
     | > grad_norm_1: 6.66113  (14.95270)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.08760  (2.05028)
     | > loader_time: 0.03660  (0.04893)


[1m   --> STEP: 97/161 -- GLOBAL_STEP: 11275[0m
     | > loss_gen: 2.88523  (2.85599)
     | > loss_kl: 1.55088  (1.66385)
     | > loss_feat: 6.13970  (5.80050)
     | > loss_mel: 25.85838  (23.88614)
     | > loss_duration: 1.74993  (1.69341)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.18412  (35.89989)
     | > grad_norm_0: 34.44847  (102.93022)
     | > loss_disc: 1.95878  (2.00517)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.95878  (2.00517)
     | > grad_norm_1: 10.51355  (14.17314)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.35240  (2.07506)
     | > loader_time: 0.04200  (0.04954)


[1m   --> STEP: 122/161 -- GLOBAL_STEP: 11300[0m
     | > loss_gen: 2.91631  (2.85293)
     | > loss_kl: 1.45937  (1.65527)
     | > loss_feat: 5.71449  (5.78526)
     | > loss_mel: 23.74077  (23.80284)
     | > loss_duration: 1.67553  (1.69446)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.50646  (35.79076)
     | > grad_norm_0: 37.46864  (96.81958)
     | > loss_disc: 2.06751  (2.01059)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06751  (2.01059)
     | > grad_norm_1: 23.27550  (14.26153)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.57580  (2.10813)
     | > loader_time: 0.04740  (0.04791)


[1m   --> STEP: 147/161 -- GLOBAL_STEP: 11325[0m
     | > loss_gen: 3.13269  (2.86104)
     | > loss_kl: 1.42294  (1.64988)
     | > loss_feat: 5.70308  (5.81188)
     | > loss_mel: 22.38392  (23.71399)
     | > loss_duration: 1.78004  (1.69580)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.42267  (35.73258)
     | > grad_norm_0: 73.03627  (95.13867)
     | > loss_disc: 2.02781  (2.00719)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.02781  (2.00719)
     | > grad_norm_1: 12.80818  (14.07644)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.35730  (2.12501)
     | > loader_time: 0.04150  (0.04701)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02752 [0m(+0.00001)
     | > avg_loss_gen:[92m 2.90157 [0m(-0.01183)
     | > avg_loss_kl:[92m 1.73742 [0m(-0.05209)
     | > avg_loss_feat:[91m 6.40732 [0m(+1.05021)
     | > avg_loss_mel:[91m 23.22203 [0m(+0.61853)
     | > avg_loss_duration:[92m 1.66345 [0m(-0.00733)
     | > avg_loss_0:[91m 35.93180 [0m(+1.59750)
     | > avg_loss_disc:[92m 1.88821 [0m(-0.21565)
     | > avg_loss_1:[92m 1.88821 [0m(-0.21565)


[4m[1m > EPOCH: 70/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 23:23:35) [0m

[1m   --> STEP: 10/161 -- GLOBAL_STEP: 11350[0m
     | > loss_gen: 2.48642  (2.88840)
     | > loss_kl: 1.64389  (1.66379)
     | > loss_feat: 5.62159  (5.89570)
     | > loss_mel: 22.23720  (23.62084)
     | > loss_duration: 1.73250  (1.70046)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.72160  (35.76919)
     | > grad_norm_0: 76.06710  (93.59869)
     | > loss_disc: 2.14622  (2.02463)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.14622  (2.02463)
     | > grad_norm_1: 56.59841  (22.99526)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.03280  (2.03258)
     | > loader_time: 0.02940  (0.06504)


[1m   --> STEP: 35/161 -- GLOBAL_STEP: 11375[0m
     | > loss_gen: 2.74859  (2.86365)
     | > loss_kl: 1.47854  (1.65637)
     | > loss_feat: 5.63858  (5.84237)
     | > loss_mel: 23.74899  (23.78817)
     | > loss_duration: 1.65892  (1.69111)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.27363  (35.84167)
     | > grad_norm_0: 147.01622  (117.04381)
     | > loss_disc: 2.17104  (2.04556)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.17104  (2.04556)
     | > grad_norm_1: 9.48650  (16.17605)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.64900  (2.09498)
     | > loader_time: 0.04420  (0.04583)


[1m   --> STEP: 60/161 -- GLOBAL_STEP: 11400[0m
     | > loss_gen: 2.80522  (2.82942)
     | > loss_kl: 1.65115  (1.67813)
     | > loss_feat: 5.45331  (5.71970)
     | > loss_mel: 22.17060  (23.67228)
     | > loss_duration: 1.68114  (1.68532)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.76141  (35.58485)
     | > grad_norm_0: 156.43977  (115.56319)
     | > loss_disc: 2.02191  (2.05590)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.02191  (2.05590)
     | > grad_norm_1: 15.59917  (15.70340)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.93960  (2.06220)
     | > loader_time: 0.08830  (0.04379)


[1m   --> STEP: 85/161 -- GLOBAL_STEP: 11425[0m
     | > loss_gen: 2.87687  (2.84545)
     | > loss_kl: 1.57247  (1.66812)
     | > loss_feat: 6.12936  (5.77037)
     | > loss_mel: 25.04526  (23.66604)
     | > loss_duration: 1.79751  (1.69272)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.42147  (35.64270)
     | > grad_norm_0: 54.48301  (111.93173)
     | > loss_disc: 1.92660  (2.03770)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.92660  (2.03770)
     | > grad_norm_1: 10.85520  (14.99675)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.48230  (2.06334)
     | > loader_time: 0.04040  (0.04712)


[1m   --> STEP: 110/161 -- GLOBAL_STEP: 11450[0m
     | > loss_gen: 2.98620  (2.85134)
     | > loss_kl: 1.70890  (1.65323)
     | > loss_feat: 5.65040  (5.76909)
     | > loss_mel: 23.15387  (23.67401)
     | > loss_duration: 1.67822  (1.69382)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.17759  (35.64150)
     | > grad_norm_0: 100.25101  (111.85776)
     | > loss_disc: 2.06577  (2.03450)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06577  (2.03450)
     | > grad_norm_1: 6.89536  (14.90042)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.86180  (2.07525)
     | > loader_time: 0.03470  (0.05237)


[1m   --> STEP: 135/161 -- GLOBAL_STEP: 11475[0m
     | > loss_gen: 2.76568  (2.84918)
     | > loss_kl: 1.84516  (1.65480)
     | > loss_feat: 4.99346  (5.77084)
     | > loss_mel: 22.92445  (23.69665)
     | > loss_duration: 1.63408  (1.69467)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.16283  (35.66613)
     | > grad_norm_0: 299.91913  (108.86689)
     | > loss_disc: 2.12149  (2.03454)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.12149  (2.03454)
     | > grad_norm_1: 27.18453  (15.53317)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.76510  (2.10372)
     | > loader_time: 0.02810  (0.05067)


[1m   --> STEP: 160/161 -- GLOBAL_STEP: 11500[0m
     | > loss_gen: 3.04936  (2.85125)
     | > loss_kl: 1.64489  (1.65201)
     | > loss_feat: 5.19828  (5.78350)
     | > loss_mel: 23.60517  (23.67049)
     | > loss_duration: 1.70238  (1.69475)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.20008  (35.65201)
     | > grad_norm_0: 60.28321  (111.08904)
     | > loss_disc: 2.16611  (2.03210)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.16611  (2.03210)
     | > grad_norm_1: 13.31646  (15.88865)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.20570  (2.10362)
     | > loader_time: 0.03780  (0.05132)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02663 [0m(-0.00089)
     | > avg_loss_gen:[92m 2.86369 [0m(-0.03788)
     | > avg_loss_kl:[92m 1.47576 [0m(-0.26166)
     | > avg_loss_feat:[92m 5.57168 [0m(-0.83564)
     | > avg_loss_mel:[92m 22.95785 [0m(-0.26418)
     | > avg_loss_duration:[91m 1.68091 [0m(+0.01746)
     | > avg_loss_0:[92m 34.54989 [0m(-1.38191)
     | > avg_loss_disc:[91m 2.03600 [0m(+0.14779)
     | > avg_loss_1:[91m 2.03600 [0m(+0.14779)


[4m[1m > EPOCH: 71/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 23:29:42) [0m

[1m   --> STEP: 23/161 -- GLOBAL_STEP: 11525[0m
     | > loss_gen: 2.79686  (2.83870)
     | > loss_kl: 1.70616  (1.64300)
     | > loss_feat: 5.77422  (5.70223)
     | > loss_mel: 23.62868  (23.46434)
     | > loss_duration: 1.65213  (1.69629)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.55806  (35.34456)
     | > grad_norm_0: 210.75754  (103.83124)
     | > loss_disc: 2.01345  (2.02949)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.01345  (2.02949)
     | > grad_norm_1: 18.34335  (14.54595)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.01940  (2.06915)
     | > loader_time: 0.03130  (0.05090)


[1m   --> STEP: 48/161 -- GLOBAL_STEP: 11550[0m
     | > loss_gen: 2.65812  (2.84806)
     | > loss_kl: 1.79199  (1.66869)
     | > loss_feat: 5.87911  (5.81442)
     | > loss_mel: 25.23490  (23.70298)
     | > loss_duration: 1.58751  (1.69624)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.15163  (35.73039)
     | > grad_norm_0: 55.60073  (98.93867)
     | > loss_disc: 2.00421  (2.03178)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.00421  (2.03178)
     | > grad_norm_1: 8.64645  (17.08256)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.86010  (2.08011)
     | > loader_time: 0.02730  (0.05759)


[1m   --> STEP: 73/161 -- GLOBAL_STEP: 11575[0m
     | > loss_gen: 3.11049  (2.84206)
     | > loss_kl: 1.80635  (1.67186)
     | > loss_feat: 5.46633  (5.77434)
     | > loss_mel: 22.52648  (23.57586)
     | > loss_duration: 1.82096  (1.69217)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.73063  (35.55629)
     | > grad_norm_0: 55.66158  (93.97606)
     | > loss_disc: 2.10784  (2.03580)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.10784  (2.03580)
     | > grad_norm_1: 15.49881  (15.41916)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.11560  (2.05925)
     | > loader_time: 0.03400  (0.04948)


[1m   --> STEP: 98/161 -- GLOBAL_STEP: 11600[0m
     | > loss_gen: 2.70662  (2.84304)
     | > loss_kl: 1.67305  (1.68398)
     | > loss_feat: 4.74326  (5.78878)
     | > loss_mel: 22.53260  (23.58019)
     | > loss_duration: 1.65479  (1.69689)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.31031  (35.59288)
     | > grad_norm_0: 101.65921  (89.69736)
     | > loss_disc: 2.13446  (2.03220)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.13446  (2.03220)
     | > grad_norm_1: 15.73241  (15.09889)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.95020  (2.08136)
     | > loader_time: 0.03400  (0.05231)


[1m   --> STEP: 123/161 -- GLOBAL_STEP: 11625[0m
     | > loss_gen: 2.85985  (2.84281)
     | > loss_kl: 1.38560  (1.66757)
     | > loss_feat: 6.16431  (5.78449)
     | > loss_mel: 23.65950  (23.57094)
     | > loss_duration: 1.71424  (1.69793)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.78351  (35.56373)
     | > grad_norm_0: 120.71795  (94.35237)
     | > loss_disc: 1.93939  (2.02996)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.93939  (2.02996)
     | > grad_norm_1: 9.72257  (16.12213)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 3.12780  (2.11498)
     | > loader_time: 0.07000  (0.05130)


[1m   --> STEP: 148/161 -- GLOBAL_STEP: 11650[0m
     | > loss_gen: 2.63224  (2.85041)
     | > loss_kl: 1.66070  (1.66320)
     | > loss_feat: 6.12441  (5.81716)
     | > loss_mel: 24.57186  (23.60399)
     | > loss_duration: 1.62807  (1.69899)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.61729  (35.63376)
     | > grad_norm_0: 107.22585  (97.99476)
     | > loss_disc: 2.17397  (2.02406)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.17397  (2.02406)
     | > grad_norm_1: 16.49405  (16.06306)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.23010  (2.12579)
     | > loader_time: 0.03890  (0.04988)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02897 [0m(+0.00234)
     | > avg_loss_gen:[91m 2.88999 [0m(+0.02630)
     | > avg_loss_kl:[91m 1.66204 [0m(+0.18628)
     | > avg_loss_feat:[91m 5.90050 [0m(+0.32882)
     | > avg_loss_mel:[91m 23.39731 [0m(+0.43946)
     | > avg_loss_duration:[92m 1.67939 [0m(-0.00151)
     | > avg_loss_0:[91m 35.52924 [0m(+0.97935)
     | > avg_loss_disc:[91m 2.09366 [0m(+0.05765)
     | > avg_loss_1:[91m 2.09366 [0m(+0.05765)


[4m[1m > EPOCH: 72/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 23:35:51) [0m

[1m   --> STEP: 11/161 -- GLOBAL_STEP: 11675[0m
     | > loss_gen: 2.60005  (2.79911)
     | > loss_kl: 1.81137  (1.68827)
     | > loss_feat: 5.64513  (5.74490)
     | > loss_mel: 23.16480  (23.92688)
     | > loss_duration: 1.68667  (1.68243)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.90802  (35.84159)
     | > grad_norm_0: 284.75211  (118.53622)
     | > loss_disc: 2.16212  (2.07094)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.16212  (2.07094)
     | > grad_norm_1: 24.62459  (14.66522)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.64530  (1.98707)
     | > loader_time: 0.02650  (0.06102)


[1m   --> STEP: 36/161 -- GLOBAL_STEP: 11700[0m
     | > loss_gen: 2.74142  (2.84732)
     | > loss_kl: 1.75347  (1.68749)
     | > loss_feat: 5.24544  (5.83867)
     | > loss_mel: 23.62383  (23.77377)
     | > loss_duration: 1.76626  (1.69798)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.13040  (35.84523)
     | > grad_norm_0: 48.96173  (120.27251)
     | > loss_disc: 2.14083  (2.03610)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.14083  (2.03610)
     | > grad_norm_1: 11.18769  (14.12471)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.24620  (2.10863)
     | > loader_time: 0.04110  (0.05745)


[1m   --> STEP: 61/161 -- GLOBAL_STEP: 11725[0m
     | > loss_gen: 2.67779  (2.84376)
     | > loss_kl: 1.72759  (1.68581)
     | > loss_feat: 6.35219  (5.80871)
     | > loss_mel: 24.89320  (23.73773)
     | > loss_duration: 1.85375  (1.69438)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.50452  (35.77039)
     | > grad_norm_0: 100.24992  (111.83884)
     | > loss_disc: 2.00072  (2.03463)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.00072  (2.03463)
     | > grad_norm_1: 16.44993  (14.47933)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.21180  (2.07963)
     | > loader_time: 0.03600  (0.04762)


[1m   --> STEP: 86/161 -- GLOBAL_STEP: 11750[0m
     | > loss_gen: 2.72172  (2.83702)
     | > loss_kl: 1.63483  (1.67888)
     | > loss_feat: 6.25491  (5.77781)
     | > loss_mel: 22.43493  (23.61876)
     | > loss_duration: 1.74013  (1.69315)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.78653  (35.60562)
     | > grad_norm_0: 49.60480  (108.78115)
     | > loss_disc: 2.07158  (2.04496)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.07158  (2.04496)
     | > grad_norm_1: 9.02175  (16.39895)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.61960  (2.06728)
     | > loader_time: 0.02590  (0.04720)


[1m   --> STEP: 111/161 -- GLOBAL_STEP: 11775[0m
     | > loss_gen: 2.98894  (2.84685)
     | > loss_kl: 1.70087  (1.67661)
     | > loss_feat: 6.38648  (5.80286)
     | > loss_mel: 24.58386  (23.58761)
     | > loss_duration: 1.78714  (1.69551)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.44728  (35.60945)
     | > grad_norm_0: 92.04144  (109.21634)
     | > loss_disc: 1.94607  (2.04162)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.94607  (2.04162)
     | > grad_norm_1: 12.74674  (15.53051)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.48020  (2.09672)
     | > loader_time: 0.18650  (0.04921)


[1m   --> STEP: 136/161 -- GLOBAL_STEP: 11800[0m
     | > loss_gen: 2.65248  (2.85084)
     | > loss_kl: 1.62297  (1.67135)
     | > loss_feat: 5.48129  (5.82131)
     | > loss_mel: 23.78786  (23.60500)
     | > loss_duration: 1.66651  (1.69227)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.21110  (35.64077)
     | > grad_norm_0: 111.67503  (107.58281)
     | > loss_disc: 2.08314  (2.03608)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.08314  (2.03608)
     | > grad_norm_1: 16.32873  (14.75445)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.10770  (2.12991)
     | > loader_time: 0.03220  (0.04913)


[1m   --> STEP: 161/161 -- GLOBAL_STEP: 11825[0m
     | > loss_gen: 2.35193  (2.84619)
     | > loss_kl: 1.61234  (1.66639)
     | > loss_feat: 6.28615  (5.81714)
     | > loss_mel: 23.73368  (23.57591)
     | > loss_duration: 1.66033  (1.68890)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.64442  (35.59454)
     | > grad_norm_0: 189.89670  (109.83797)
     | > loss_disc: 2.03918  (2.03597)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.03918  (2.03597)
     | > grad_norm_1: 33.34988  (14.50599)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.22510  (2.12053)
     | > loader_time: 0.02710  (0.04813)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02780 [0m(-0.00117)
     | > avg_loss_gen:[91m 3.37587 [0m(+0.48588)
     | > avg_loss_kl:[92m 1.65439 [0m(-0.00766)
     | > avg_loss_feat:[92m 5.27652 [0m(-0.62398)
     | > avg_loss_mel:[92m 23.31622 [0m(-0.08109)
     | > avg_loss_duration:[92m 1.67267 [0m(-0.00672)
     | > avg_loss_0:[92m 35.29567 [0m(-0.23357)
     | > avg_loss_disc:[91m 2.27535 [0m(+0.18169)
     | > avg_loss_1:[91m 2.27535 [0m(+0.18169)


[4m[1m > EPOCH: 73/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 23:41:59) [0m

[1m   --> STEP: 24/161 -- GLOBAL_STEP: 11850[0m
     | > loss_gen: 2.81774  (2.84392)
     | > loss_kl: 1.60039  (1.65649)
     | > loss_feat: 6.23717  (5.76242)
     | > loss_mel: 23.99759  (23.51701)
     | > loss_duration: 1.65109  (1.68791)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.30398  (35.46775)
     | > grad_norm_0: 137.10623  (106.90176)
     | > loss_disc: 1.88375  (2.00278)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.88375  (2.00278)
     | > grad_norm_1: 12.02583  (17.05029)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.87290  (2.00948)
     | > loader_time: 0.03110  (0.04138)


[1m   --> STEP: 49/161 -- GLOBAL_STEP: 11875[0m
     | > loss_gen: 2.52880  (2.84849)
     | > loss_kl: 1.81451  (1.67648)
     | > loss_feat: 5.82876  (5.78523)
     | > loss_mel: 22.31775  (23.52623)
     | > loss_duration: 1.64320  (1.69063)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.13301  (35.52706)
     | > grad_norm_0: 42.85452  (103.75029)
     | > loss_disc: 2.03496  (2.01624)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.03496  (2.01624)
     | > grad_norm_1: 9.88887  (14.99819)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.34020  (2.07924)
     | > loader_time: 0.03690  (0.04152)


[1m   --> STEP: 74/161 -- GLOBAL_STEP: 11900[0m
     | > loss_gen: 2.59646  (2.84091)
     | > loss_kl: 1.79037  (1.67933)
     | > loss_feat: 5.39465  (5.76964)
     | > loss_mel: 24.19270  (23.51560)
     | > loss_duration: 1.68507  (1.68631)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.65924  (35.49179)
     | > grad_norm_0: 85.42726  (106.69854)
     | > loss_disc: 2.16959  (2.02391)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.16959  (2.02391)
     | > grad_norm_1: 19.00910  (16.52106)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.76140  (2.04851)
     | > loader_time: 0.03130  (0.04901)


[1m   --> STEP: 99/161 -- GLOBAL_STEP: 11925[0m
     | > loss_gen: 2.65539  (2.83322)
     | > loss_kl: 1.69222  (1.69475)
     | > loss_feat: 4.90469  (5.74909)
     | > loss_mel: 22.09652  (23.46895)
     | > loss_duration: 1.75010  (1.69356)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.09893  (35.43958)
     | > grad_norm_0: 131.09671  (106.90053)
     | > loss_disc: 2.19353  (2.03361)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.19353  (2.03361)
     | > grad_norm_1: 19.88740  (16.66010)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.72850  (2.06826)
     | > loader_time: 0.02930  (0.04962)


[1m   --> STEP: 124/161 -- GLOBAL_STEP: 11950[0m
     | > loss_gen: 2.72516  (2.83445)
     | > loss_kl: 1.70919  (1.67814)
     | > loss_feat: 6.06453  (5.75795)
     | > loss_mel: 23.17208  (23.46677)
     | > loss_duration: 1.59009  (1.69599)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.26105  (35.43329)
     | > grad_norm_0: 113.90652  (113.01752)
     | > loss_disc: 2.01003  (2.03084)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.01003  (2.03084)
     | > grad_norm_1: 13.47898  (17.30057)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.06450  (2.10724)
     | > loader_time: 0.03650  (0.04897)


[1m   --> STEP: 149/161 -- GLOBAL_STEP: 11975[0m
     | > loss_gen: 2.60285  (2.83276)
     | > loss_kl: 1.48777  (1.67280)
     | > loss_feat: 5.90979  (5.76363)
     | > loss_mel: 22.88246  (23.47145)
     | > loss_duration: 1.64370  (1.69428)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.52657  (35.43492)
     | > grad_norm_0: 178.34772  (114.89200)
     | > loss_disc: 2.03966  (2.03089)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.03966  (2.03089)
     | > grad_norm_1: 13.49757  (17.38244)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.10690  (2.11697)
     | > loader_time: 0.03670  (0.04748)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02959 [0m(+0.00179)
     | > avg_loss_gen:[92m 2.75548 [0m(-0.62039)
     | > avg_loss_kl:[92m 1.62690 [0m(-0.02749)
     | > avg_loss_feat:[92m 5.24771 [0m(-0.02881)
     | > avg_loss_mel:[92m 22.17310 [0m(-1.14312)
     | > avg_loss_duration:[92m 1.66064 [0m(-0.01203)
     | > avg_loss_0:[92m 33.46383 [0m(-1.83184)
     | > avg_loss_disc:[92m 2.17173 [0m(-0.10362)
     | > avg_loss_1:[92m 2.17173 [0m(-0.10362)


[4m[1m > EPOCH: 74/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 23:48:09) [0m

[1m   --> STEP: 12/161 -- GLOBAL_STEP: 12000[0m
     | > loss_gen: 2.72001  (2.84252)
     | > loss_kl: 1.72352  (1.70929)
     | > loss_feat: 5.88091  (5.82225)
     | > loss_mel: 22.14760  (23.50481)
     | > loss_duration: 1.66400  (1.67766)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.13603  (35.55654)
     | > grad_norm_0: 66.00648  (112.47655)
     | > loss_disc: 2.07159  (2.07762)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.07159  (2.07762)
     | > grad_norm_1: 10.89315  (13.33019)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.84620  (1.97197)
     | > loader_time: 0.02600  (0.05703)


[1m   --> STEP: 37/161 -- GLOBAL_STEP: 12025[0m
     | > loss_gen: 2.55515  (2.83530)
     | > loss_kl: 1.68919  (1.67551)
     | > loss_feat: 5.07975  (5.77503)
     | > loss_mel: 24.01324  (23.73322)
     | > loss_duration: 1.66736  (1.69373)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.00469  (35.71280)
     | > grad_norm_0: 190.50182  (131.72684)
     | > loss_disc: 2.27101  (2.05718)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.27101  (2.05718)
     | > grad_norm_1: 13.12090  (14.78152)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.70210  (2.08852)
     | > loader_time: 0.02840  (0.05283)


[1m   --> STEP: 62/161 -- GLOBAL_STEP: 12050[0m
     | > loss_gen: 3.15524  (2.84632)
     | > loss_kl: 1.66952  (1.67811)
     | > loss_feat: 6.43377  (5.77569)
     | > loss_mel: 25.60980  (23.59987)
     | > loss_duration: 1.68562  (1.68817)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.55395  (35.58815)
     | > grad_norm_0: 81.92834  (124.92442)
     | > loss_disc: 1.80873  (2.04253)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.80873  (2.04253)
     | > grad_norm_1: 6.95697  (14.36044)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.70140  (2.07129)
     | > loader_time: 0.11460  (0.04764)


[1m   --> STEP: 87/161 -- GLOBAL_STEP: 12075[0m
     | > loss_gen: 3.31610  (2.87826)
     | > loss_kl: 1.53846  (1.66715)
     | > loss_feat: 6.44800  (5.89489)
     | > loss_mel: 24.61948  (23.64092)
     | > loss_duration: 1.70187  (1.68851)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.62391  (35.76973)
     | > grad_norm_0: 184.47868  (119.16380)
     | > loss_disc: 1.97550  (2.02736)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.97550  (2.02736)
     | > grad_norm_1: 18.24727  (13.54243)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.42010  (2.06069)
     | > loader_time: 0.03410  (0.05121)


[1m   --> STEP: 112/161 -- GLOBAL_STEP: 12100[0m
     | > loss_gen: 2.44110  (2.87719)
     | > loss_kl: 1.67044  (1.65957)
     | > loss_feat: 5.04272  (5.89211)
     | > loss_mel: 25.51370  (23.65127)
     | > loss_duration: 1.70895  (1.69101)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.37691  (35.77115)
     | > grad_norm_0: 42.01604  (115.76935)
     | > loss_disc: 2.67586  (2.06863)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.67586  (2.06863)
     | > grad_norm_1: 10.91813  (15.67980)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.18300  (2.08268)
     | > loader_time: 0.03860  (0.04814)


[1m   --> STEP: 137/161 -- GLOBAL_STEP: 12125[0m
     | > loss_gen: 2.80285  (2.86060)
     | > loss_kl: 1.65356  (1.65967)
     | > loss_feat: 5.48599  (5.82130)
     | > loss_mel: 23.97602  (23.61755)
     | > loss_duration: 1.65164  (1.69345)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.57005  (35.65258)
     | > grad_norm_0: 118.10022  (112.00566)
     | > loss_disc: 2.08511  (2.08528)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.08511  (2.08528)
     | > grad_norm_1: 10.31792  (15.04860)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.30720  (2.12215)
     | > loader_time: 0.03950  (0.05044)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02875 [0m(-0.00083)
     | > avg_loss_gen:[91m 2.90583 [0m(+0.15035)
     | > avg_loss_kl:[91m 1.68120 [0m(+0.05430)
     | > avg_loss_feat:[91m 5.80437 [0m(+0.55666)
     | > avg_loss_mel:[91m 22.81678 [0m(+0.64368)
     | > avg_loss_duration:[91m 1.67833 [0m(+0.01769)
     | > avg_loss_0:[91m 34.88651 [0m(+1.42268)
     | > avg_loss_disc:[92m 1.88648 [0m(-0.28526)
     | > avg_loss_1:[92m 1.88648 [0m(-0.28526)


[4m[1m > EPOCH: 75/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-07 23:54:18) [0m

[1m   --> STEP: 0/161 -- GLOBAL_STEP: 12150[0m
     | > loss_gen: 2.72211  (2.72211)
     | > loss_kl: 1.62567  (1.62567)
     | > loss_feat: 5.88278  (5.88278)
     | > loss_mel: 22.83377  (22.83377)
     | > loss_duration: 1.65539  (1.65539)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 34.71971  (34.71971)
     | > grad_norm_0: 178.56546  (178.56546)
     | > loss_disc: 2.00535  (2.00535)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.00535  (2.00535)
     | > grad_norm_1: 12.61540  (12.61540)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 3.80890  (3.80888)
     | > loader_time: 3.90610  (3.90608)


[1m   --> STEP: 25/161 -- GLOBAL_STEP: 12175[0m
     | > loss_gen: 2.85755  (2.82404)
     | > loss_kl: 1.63585  (1.63742)
     | > loss_feat: 5.63208  (5.71345)
     | > loss_mel: 24.09923  (23.56067)
     | > loss_duration: 1.65207  (1.68403)
     | > amp_scaler: 256.00000  (399.36000)
     | > loss_0: 35.87678  (35.41962)
     | > grad_norm_0: 298.39026  (118.89775)
     | > loss_disc: 2.01000  (2.03620)
     | > amp_scaler-1: 256.00000  (399.36000)
     | > loss_1: 2.01000  (2.03620)
     | > grad_norm_1: 12.41431  (20.30301)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.01360  (2.05881)
     | > loader_time: 0.03180  (0.03630)


[1m   --> STEP: 50/161 -- GLOBAL_STEP: 12200[0m
     | > loss_gen: 2.87637  (2.82875)
     | > loss_kl: 1.69172  (1.64755)
     | > loss_feat: 5.88473  (5.72427)
     | > loss_mel: 23.50093  (23.56037)
     | > loss_duration: 1.75869  (1.69076)
     | > amp_scaler: 256.00000  (327.68000)
     | > loss_0: 35.71243  (35.45169)
     | > grad_norm_0: 33.04118  (113.29987)
     | > loss_disc: 1.90037  (2.03632)
     | > amp_scaler-1: 256.00000  (327.68000)
     | > loss_1: 1.90037  (2.03632)
     | > grad_norm_1: 10.34717  (17.10153)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.94180  (2.10410)
     | > loader_time: 0.03600  (0.03840)


[1m   --> STEP: 75/161 -- GLOBAL_STEP: 12225[0m
     | > loss_gen: 2.98115  (2.82671)
     | > loss_kl: 1.54878  (1.66541)
     | > loss_feat: 5.32841  (5.72829)
     | > loss_mel: 22.83699  (23.55726)
     | > loss_duration: 1.70613  (1.68998)
     | > amp_scaler: 256.00000  (303.78667)
     | > loss_0: 34.40146  (35.46765)
     | > grad_norm_0: 155.08167  (112.09948)
     | > loss_disc: 2.16235  (2.05144)
     | > amp_scaler-1: 256.00000  (303.78667)
     | > loss_1: 2.16235  (2.05144)
     | > grad_norm_1: 15.35645  (16.86580)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.94580  (2.09227)
     | > loader_time: 0.04230  (0.03791)


[1m   --> STEP: 100/161 -- GLOBAL_STEP: 12250[0m
     | > loss_gen: 2.82312  (2.81929)
     | > loss_kl: 1.78606  (1.66777)
     | > loss_feat: 6.14241  (5.71695)
     | > loss_mel: 23.53412  (23.46659)
     | > loss_duration: 1.65132  (1.69123)
     | > amp_scaler: 256.00000  (291.84000)
     | > loss_0: 35.93703  (35.36182)
     | > grad_norm_0: 221.32666  (117.11133)
     | > loss_disc: 1.96755  (2.04799)
     | > amp_scaler-1: 256.00000  (291.84000)
     | > loss_1: 1.96755  (2.04799)
     | > grad_norm_1: 25.00216  (16.12663)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.24740  (2.10366)
     | > loader_time: 0.03790  (0.04164)


[1m   --> STEP: 125/161 -- GLOBAL_STEP: 12275[0m
     | > loss_gen: 2.79889  (2.81424)
     | > loss_kl: 1.79690  (1.66321)
     | > loss_feat: 5.58654  (5.70222)
     | > loss_mel: 23.75899  (23.43385)
     | > loss_duration: 1.75566  (1.69267)
     | > amp_scaler: 256.00000  (284.67200)
     | > loss_0: 35.69699  (35.30620)
     | > grad_norm_0: 65.43494  (119.51485)
     | > loss_disc: 2.02958  (2.05011)
     | > amp_scaler-1: 256.00000  (284.67200)
     | > loss_1: 2.02958  (2.05011)
     | > grad_norm_1: 10.05348  (16.53466)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.26890  (2.13923)
     | > loader_time: 0.03800  (0.04300)


[1m   --> STEP: 150/161 -- GLOBAL_STEP: 12300[0m
     | > loss_gen: 2.78372  (2.82307)
     | > loss_kl: 1.68251  (1.65485)
     | > loss_feat: 5.76851  (5.72795)
     | > loss_mel: 22.34690  (23.38246)
     | > loss_duration: 1.60542  (1.69346)
     | > amp_scaler: 256.00000  (279.89333)
     | > loss_0: 34.18705  (35.28179)
     | > grad_norm_0: 105.31567  (117.07162)
     | > loss_disc: 2.08346  (2.04484)
     | > amp_scaler-1: 256.00000  (279.89333)
     | > loss_1: 2.08346  (2.04484)
     | > grad_norm_1: 18.43323  (16.22551)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.47950  (2.14420)
     | > loader_time: 0.04000  (0.04366)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02872 [0m(-0.00004)
     | > avg_loss_gen:[91m 3.04131 [0m(+0.13548)
     | > avg_loss_kl:[91m 1.83215 [0m(+0.15095)
     | > avg_loss_feat:[91m 6.23299 [0m(+0.42862)
     | > avg_loss_mel:[91m 23.72826 [0m(+0.91148)
     | > avg_loss_duration:[91m 1.68293 [0m(+0.00460)
     | > avg_loss_0:[91m 36.51764 [0m(+1.63113)
     | > avg_loss_disc:[91m 1.91382 [0m(+0.02734)
     | > avg_loss_1:[91m 1.91382 [0m(+0.02734)


[4m[1m > EPOCH: 76/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 00:00:27) [0m

[1m   --> STEP: 13/161 -- GLOBAL_STEP: 12325[0m
     | > loss_gen: 2.86280  (2.94072)
     | > loss_kl: 1.76527  (1.66302)
     | > loss_feat: 6.12031  (6.14678)
     | > loss_mel: 24.32400  (23.97135)
     | > loss_duration: 1.64388  (1.68672)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.71626  (36.40858)
     | > grad_norm_0: 89.99649  (109.56967)
     | > loss_disc: 2.01489  (1.94051)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.01489  (1.94051)
     | > grad_norm_1: 41.22747  (14.46519)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.44250  (2.05715)
     | > loader_time: 0.03480  (0.08047)


[1m   --> STEP: 38/161 -- GLOBAL_STEP: 12350[0m
     | > loss_gen: 3.06396  (2.88589)
     | > loss_kl: 1.62183  (1.65966)
     | > loss_feat: 5.47103  (5.88115)
     | > loss_mel: 22.06028  (23.45371)
     | > loss_duration: 1.75864  (1.69311)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.97574  (35.57352)
     | > grad_norm_0: 129.47939  (109.90896)
     | > loss_disc: 2.10373  (2.00123)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.10373  (2.00123)
     | > grad_norm_1: 9.02667  (17.04195)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.31740  (2.10749)
     | > loader_time: 0.03980  (0.05512)


[1m   --> STEP: 63/161 -- GLOBAL_STEP: 12375[0m
     | > loss_gen: 2.42629  (2.87374)
     | > loss_kl: 1.73894  (1.65328)
     | > loss_feat: 4.74895  (5.86141)
     | > loss_mel: 21.64977  (23.51521)
     | > loss_duration: 1.71552  (1.68620)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 32.27947  (35.58984)
     | > grad_norm_0: 288.03738  (128.26224)
     | > loss_disc: 2.16661  (2.00411)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.16661  (2.00411)
     | > grad_norm_1: 25.80722  (17.22702)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.84430  (2.06123)
     | > loader_time: 0.02950  (0.04902)


[1m   --> STEP: 88/161 -- GLOBAL_STEP: 12400[0m
     | > loss_gen: 2.94622  (2.87050)
     | > loss_kl: 1.85394  (1.65545)
     | > loss_feat: 5.97474  (5.87323)
     | > loss_mel: 23.44426  (23.53301)
     | > loss_duration: 1.63424  (1.68649)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.85340  (35.61869)
     | > grad_norm_0: 75.04338  (130.47781)
     | > loss_disc: 1.93485  (2.01396)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.93485  (2.01396)
     | > grad_norm_1: 7.26198  (17.56723)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.24680  (2.06793)
     | > loader_time: 0.03940  (0.05071)


[1m   --> STEP: 113/161 -- GLOBAL_STEP: 12425[0m
     | > loss_gen: 3.26848  (2.86042)
     | > loss_kl: 1.71776  (1.65310)
     | > loss_feat: 6.08353  (5.84309)
     | > loss_mel: 23.50978  (23.56073)
     | > loss_duration: 1.64217  (1.68927)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.22173  (35.60660)
     | > grad_norm_0: 72.66770  (130.90237)
     | > loss_disc: 1.91801  (2.02259)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.91801  (2.02259)
     | > grad_norm_1: 14.31820  (18.06615)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.55100  (2.08142)
     | > loader_time: 0.14420  (0.05006)


[1m   --> STEP: 138/161 -- GLOBAL_STEP: 12450[0m
     | > loss_gen: 2.72350  (2.84115)
     | > loss_kl: 1.50091  (1.65526)
     | > loss_feat: 5.84803  (5.79586)
     | > loss_mel: 24.22935  (23.45930)
     | > loss_duration: 1.73863  (1.69130)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.04040  (35.44288)
     | > grad_norm_0: 195.43854  (126.72373)
     | > loss_disc: 1.97173  (2.03409)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.97173  (2.03409)
     | > grad_norm_1: 33.01046  (19.12924)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.31000  (2.11395)
     | > loader_time: 0.03700  (0.05089)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.03672 [0m(+0.00800)
     | > avg_loss_gen:[92m 2.96910 [0m(-0.07221)
     | > avg_loss_kl:[92m 1.76038 [0m(-0.07177)
     | > avg_loss_feat:[92m 6.05095 [0m(-0.18203)
     | > avg_loss_mel:[92m 23.42747 [0m(-0.30079)
     | > avg_loss_duration:[92m 1.68119 [0m(-0.00174)
     | > avg_loss_0:[92m 35.88910 [0m(-0.62854)
     | > avg_loss_disc:[91m 2.05715 [0m(+0.14334)
     | > avg_loss_1:[91m 2.05715 [0m(+0.14334)


[4m[1m > EPOCH: 77/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 00:06:35) [0m

[1m   --> STEP: 1/161 -- GLOBAL_STEP: 12475[0m
     | > loss_gen: 2.93333  (2.93333)
     | > loss_kl: 1.75667  (1.75667)
     | > loss_feat: 5.70460  (5.70460)
     | > loss_mel: 23.94645  (23.94645)
     | > loss_duration: 1.63774  (1.63774)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.97880  (35.97880)
     | > grad_norm_0: 230.68747  (230.68747)
     | > loss_disc: 2.10869  (2.10869)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.10869  (2.10869)
     | > grad_norm_1: 8.03677  (8.03677)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.21050  (2.21046)
     | > loader_time: 0.02480  (0.02476)


[1m   --> STEP: 26/161 -- GLOBAL_STEP: 12500[0m
     | > loss_gen: 2.67454  (2.81785)
     | > loss_kl: 1.60023  (1.70020)
     | > loss_feat: 6.02102  (5.71022)
     | > loss_mel: 23.66287  (23.43082)
     | > loss_duration: 1.74241  (1.70026)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.70107  (35.35935)
     | > grad_norm_0: 47.20284  (99.22751)
     | > loss_disc: 1.95855  (2.04534)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.95855  (2.04534)
     | > grad_norm_1: 16.98510  (12.69199)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.54710  (2.09107)
     | > loader_time: 0.04790  (0.03548)


[1m   --> STEP: 51/161 -- GLOBAL_STEP: 12525[0m
     | > loss_gen: 2.70848  (2.83288)
     | > loss_kl: 1.80221  (1.68782)
     | > loss_feat: 5.91932  (5.78590)
     | > loss_mel: 22.99408  (23.59401)
     | > loss_duration: 1.72071  (1.69618)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.14480  (35.59678)
     | > grad_norm_0: 118.16830  (104.81285)
     | > loss_disc: 2.10368  (2.04218)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.10368  (2.04218)
     | > grad_norm_1: 15.81377  (13.98704)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.05580  (2.09762)
     | > loader_time: 0.03370  (0.04148)


[1m   --> STEP: 76/161 -- GLOBAL_STEP: 12550[0m
     | > loss_gen: 3.00523  (2.82777)
     | > loss_kl: 1.51027  (1.69148)
     | > loss_feat: 5.42602  (5.73853)
     | > loss_mel: 22.53577  (23.49102)
     | > loss_duration: 1.71188  (1.69445)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.18916  (35.44324)
     | > grad_norm_0: 94.18990  (115.23210)
     | > loss_disc: 2.14899  (2.04778)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.14899  (2.04778)
     | > grad_norm_1: 9.28253  (15.69963)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.34210  (2.07988)
     | > loader_time: 0.04440  (0.04414)


[1m   --> STEP: 101/161 -- GLOBAL_STEP: 12575[0m
     | > loss_gen: 3.11878  (2.82993)
     | > loss_kl: 1.63590  (1.68302)
     | > loss_feat: 5.76318  (5.77932)
     | > loss_mel: 22.29437  (23.47726)
     | > loss_duration: 1.67431  (1.69379)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.48655  (35.46333)
     | > grad_norm_0: 58.53149  (113.68367)
     | > loss_disc: 2.03780  (2.04596)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.03780  (2.04596)
     | > grad_norm_1: 20.46995  (16.31179)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.92370  (2.08904)
     | > loader_time: 0.03320  (0.04774)


[1m   --> STEP: 126/161 -- GLOBAL_STEP: 12600[0m
     | > loss_gen: 2.76174  (2.82732)
     | > loss_kl: 1.96706  (1.68659)
     | > loss_feat: 5.95839  (5.77467)
     | > loss_mel: 22.57037  (23.45427)
     | > loss_duration: 1.56748  (1.69183)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.82503  (35.43468)
     | > grad_norm_0: 157.00400  (111.49138)
     | > loss_disc: 2.03459  (2.04340)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.03459  (2.04340)
     | > grad_norm_1: 14.11741  (16.47575)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.29440  (2.14130)
     | > loader_time: 0.03420  (0.05259)


[1m   --> STEP: 151/161 -- GLOBAL_STEP: 12625[0m
     | > loss_gen: 2.93863  (2.82709)
     | > loss_kl: 1.69375  (1.67978)
     | > loss_feat: 5.90601  (5.76530)
     | > loss_mel: 23.53860  (23.40746)
     | > loss_duration: 1.64051  (1.69125)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.71749  (35.37087)
     | > grad_norm_0: 171.52905  (109.65664)
     | > loss_disc: 1.90980  (2.04288)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.90980  (2.04288)
     | > grad_norm_1: 18.72311  (16.77518)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.02990  (2.15000)
     | > loader_time: 0.03630  (0.05100)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02784 [0m(-0.00888)
     | > avg_loss_gen:[92m 2.70184 [0m(-0.26726)
     | > avg_loss_kl:[92m 1.66654 [0m(-0.09384)
     | > avg_loss_feat:[92m 5.10995 [0m(-0.94100)
     | > avg_loss_mel:[92m 22.27944 [0m(-1.14804)
     | > avg_loss_duration:[92m 1.67714 [0m(-0.00405)
     | > avg_loss_0:[92m 33.43491 [0m(-2.45419)
     | > avg_loss_disc:[91m 2.20001 [0m(+0.14286)
     | > avg_loss_1:[91m 2.20001 [0m(+0.14286)


[4m[1m > EPOCH: 78/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 00:12:42) [0m

[1m   --> STEP: 14/161 -- GLOBAL_STEP: 12650[0m
     | > loss_gen: 2.74473  (2.81886)
     | > loss_kl: 1.91674  (1.69806)
     | > loss_feat: 6.18964  (5.82252)
     | > loss_mel: 24.56122  (22.98360)
     | > loss_duration: 1.68542  (1.68697)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.09776  (35.01002)
     | > grad_norm_0: 108.91496  (119.25773)
     | > loss_disc: 1.97709  (2.02325)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.97709  (2.02325)
     | > grad_norm_1: 7.73926  (18.96344)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.11350  (2.04749)
     | > loader_time: 0.03360  (0.06971)


[1m   --> STEP: 39/161 -- GLOBAL_STEP: 12675[0m
     | > loss_gen: 3.05651  (2.84399)
     | > loss_kl: 1.66167  (1.67529)
     | > loss_feat: 5.60849  (5.81696)
     | > loss_mel: 23.40107  (23.12614)
     | > loss_duration: 1.74173  (1.69571)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.46947  (35.15809)
     | > grad_norm_0: 209.77295  (127.52996)
     | > loss_disc: 1.93992  (2.03360)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.93992  (2.03360)
     | > grad_norm_1: 29.70137  (19.07006)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.96690  (2.11435)
     | > loader_time: 0.03600  (0.04804)


[1m   --> STEP: 64/161 -- GLOBAL_STEP: 12700[0m
     | > loss_gen: 2.96149  (2.82893)
     | > loss_kl: 1.68199  (1.67967)
     | > loss_feat: 5.35168  (5.78117)
     | > loss_mel: 22.80774  (23.16368)
     | > loss_duration: 1.68480  (1.69044)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.48770  (35.14388)
     | > grad_norm_0: 228.90302  (131.37617)
     | > loss_disc: 2.28938  (2.05108)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.28938  (2.05108)
     | > grad_norm_1: 90.56705  (21.13639)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.10460  (2.08121)
     | > loader_time: 0.03320  (0.04396)


[1m   --> STEP: 89/161 -- GLOBAL_STEP: 12725[0m
     | > loss_gen: 2.70254  (2.83371)
     | > loss_kl: 1.77338  (1.68069)
     | > loss_feat: 5.61360  (5.77944)
     | > loss_mel: 22.98090  (23.23272)
     | > loss_duration: 1.71388  (1.68992)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.78430  (35.21647)
     | > grad_norm_0: 109.10858  (130.80154)
     | > loss_disc: 2.04512  (2.04761)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.04512  (2.04761)
     | > grad_norm_1: 50.63979  (21.19424)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.46830  (2.06353)
     | > loader_time: 0.02650  (0.04871)


[1m   --> STEP: 114/161 -- GLOBAL_STEP: 12750[0m
     | > loss_gen: 3.03035  (2.82861)
     | > loss_kl: 1.76063  (1.67789)
     | > loss_feat: 5.28815  (5.75381)
     | > loss_mel: 23.21492  (23.31292)
     | > loss_duration: 1.61720  (1.69121)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.91124  (35.26445)
     | > grad_norm_0: 86.75159  (136.07376)
     | > loss_disc: 2.12379  (2.04736)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.12379  (2.04736)
     | > grad_norm_1: 15.64957  (20.19495)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.95020  (2.08448)
     | > loader_time: 0.03550  (0.04732)


[1m   --> STEP: 139/161 -- GLOBAL_STEP: 12775[0m
     | > loss_gen: 2.88579  (2.82804)
     | > loss_kl: 1.71747  (1.67761)
     | > loss_feat: 5.85827  (5.75600)
     | > loss_mel: 23.91620  (23.30186)
     | > loss_duration: 1.69599  (1.69156)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.07373  (35.25507)
     | > grad_norm_0: 50.19492  (134.93159)
     | > loss_disc: 2.12450  (2.04327)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.12450  (2.04327)
     | > grad_norm_1: 12.36821  (20.19163)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.07220  (2.11467)
     | > loader_time: 0.03040  (0.04728)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02741 [0m(-0.00043)
     | > avg_loss_gen:[91m 3.36082 [0m(+0.65898)
     | > avg_loss_kl:[92m 1.55321 [0m(-0.11333)
     | > avg_loss_feat:[91m 5.91428 [0m(+0.80433)
     | > avg_loss_mel:[91m 23.13614 [0m(+0.85670)
     | > avg_loss_duration:[91m 1.67941 [0m(+0.00227)
     | > avg_loss_0:[91m 35.64385 [0m(+2.20894)
     | > avg_loss_disc:[92m 1.99177 [0m(-0.20824)
     | > avg_loss_1:[92m 1.99177 [0m(-0.20824)


[4m[1m > EPOCH: 79/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 00:18:51) [0m

[1m   --> STEP: 2/161 -- GLOBAL_STEP: 12800[0m
     | > loss_gen: 2.50592  (2.60086)
     | > loss_kl: 1.84672  (1.79809)
     | > loss_feat: 6.42972  (5.73522)
     | > loss_mel: 23.69229  (23.23509)
     | > loss_duration: 1.70390  (1.69623)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.17855  (35.06548)
     | > grad_norm_0: 90.18909  (100.17629)
     | > loss_disc: 1.92569  (2.00483)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.92569  (2.00483)
     | > grad_norm_1: 21.98676  (23.78620)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.64020  (1.68086)
     | > loader_time: 0.08670  (0.09433)


[1m   --> STEP: 27/161 -- GLOBAL_STEP: 12825[0m
     | > loss_gen: 2.77774  (2.81152)
     | > loss_kl: 1.67191  (1.69847)
     | > loss_feat: 5.70645  (5.74130)
     | > loss_mel: 22.93724  (23.25054)
     | > loss_duration: 1.67892  (1.69093)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.77227  (35.19276)
     | > grad_norm_0: 177.90131  (136.83078)
     | > loss_disc: 2.06299  (2.03320)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06299  (2.03320)
     | > grad_norm_1: 10.22764  (18.81479)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.70140  (2.03516)
     | > loader_time: 0.03560  (0.05004)


[1m   --> STEP: 52/161 -- GLOBAL_STEP: 12850[0m
     | > loss_gen: 2.84014  (2.81562)
     | > loss_kl: 1.60816  (1.68622)
     | > loss_feat: 5.60712  (5.74083)
     | > loss_mel: 23.85176  (23.39317)
     | > loss_duration: 1.66163  (1.68996)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.56881  (35.32581)
     | > grad_norm_0: 68.28735  (117.02378)
     | > loss_disc: 2.09237  (2.03811)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.09237  (2.03811)
     | > grad_norm_1: 9.98030  (16.46275)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.80290  (2.07200)
     | > loader_time: 0.02540  (0.04646)


[1m   --> STEP: 77/161 -- GLOBAL_STEP: 12875[0m
     | > loss_gen: 2.65934  (2.83491)
     | > loss_kl: 1.71022  (1.67947)
     | > loss_feat: 5.72252  (5.81307)
     | > loss_mel: 23.96609  (23.37273)
     | > loss_duration: 1.66956  (1.69077)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.72773  (35.39095)
     | > grad_norm_0: 73.50460  (125.69654)
     | > loss_disc: 2.33138  (2.03289)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.33138  (2.03289)
     | > grad_norm_1: 13.68319  (15.82655)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.35870  (2.06773)
     | > loader_time: 0.10020  (0.05658)


[1m   --> STEP: 102/161 -- GLOBAL_STEP: 12900[0m
     | > loss_gen: 2.69031  (2.84844)
     | > loss_kl: 1.64307  (1.68002)
     | > loss_feat: 5.25075  (5.87760)
     | > loss_mel: 23.72058  (23.40775)
     | > loss_duration: 1.69229  (1.69084)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.99700  (35.50466)
     | > grad_norm_0: 61.51215  (120.91704)
     | > loss_disc: 2.32277  (2.05662)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.32277  (2.05662)
     | > grad_norm_1: 16.08995  (15.57705)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.62410  (2.06396)
     | > loader_time: 0.02610  (0.05224)


[1m   --> STEP: 127/161 -- GLOBAL_STEP: 12925[0m
     | > loss_gen: 3.28876  (2.89066)
     | > loss_kl: 1.75451  (1.68715)
     | > loss_feat: 6.74667  (5.99894)
     | > loss_mel: 23.00070  (23.53831)
     | > loss_duration: 1.74749  (1.69068)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.53813  (35.80574)
     | > grad_norm_0: 171.35590  (117.21638)
     | > loss_disc: 1.59911  (2.04400)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.59911  (2.04400)
     | > grad_norm_1: 7.78069  (15.55332)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.76970  (2.11191)
     | > loader_time: 0.20870  (0.05172)


[1m   --> STEP: 152/161 -- GLOBAL_STEP: 12950[0m
     | > loss_gen: 3.11539  (2.90569)
     | > loss_kl: 1.66441  (1.67977)
     | > loss_feat: 5.52386  (6.02244)
     | > loss_mel: 22.27368  (23.49674)
     | > loss_duration: 1.65798  (1.69151)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.23533  (35.79615)
     | > grad_norm_0: 126.16809  (116.34871)
     | > loss_disc: 2.49062  (2.05901)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.49062  (2.05901)
     | > grad_norm_1: 21.92367  (16.35080)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.45650  (2.11286)
     | > loader_time: 0.02390  (0.05004)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02788 [0m(+0.00047)
     | > avg_loss_gen:[92m 2.58201 [0m(-0.77880)
     | > avg_loss_kl:[91m 1.78943 [0m(+0.23623)
     | > avg_loss_feat:[92m 5.11155 [0m(-0.80273)
     | > avg_loss_mel:[91m 23.78314 [0m(+0.64701)
     | > avg_loss_duration:[92m 1.67049 [0m(-0.00892)
     | > avg_loss_0:[92m 34.93663 [0m(-0.70722)
     | > avg_loss_disc:[91m 2.40050 [0m(+0.40873)
     | > avg_loss_1:[91m 2.40050 [0m(+0.40873)


[4m[1m > EPOCH: 80/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 00:25:00) [0m

[1m   --> STEP: 15/161 -- GLOBAL_STEP: 12975[0m
     | > loss_gen: 2.67968  (3.05822)
     | > loss_kl: 1.66392  (1.64458)
     | > loss_feat: 5.99302  (6.08470)
     | > loss_mel: 24.16499  (23.60006)
     | > loss_duration: 1.70485  (1.68664)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.20647  (36.07420)
     | > grad_norm_0: 67.58926  (127.20135)
     | > loss_disc: 2.24042  (1.99096)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.24042  (1.99096)
     | > grad_norm_1: 15.31707  (19.44348)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.04830  (2.05811)
     | > loader_time: 0.03730  (0.04254)


[1m   --> STEP: 40/161 -- GLOBAL_STEP: 13000[0m
     | > loss_gen: 2.63004  (2.85120)
     | > loss_kl: 1.66497  (1.64620)
     | > loss_feat: 4.93184  (5.64878)
     | > loss_mel: 22.49647  (23.38946)
     | > loss_duration: 1.70157  (1.68644)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.42489  (35.22209)
     | > grad_norm_0: 181.30011  (98.81844)
     | > loss_disc: 2.18642  (2.06747)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.18642  (2.06747)
     | > grad_norm_1: 70.22902  (20.52060)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.07370  (2.09727)
     | > loader_time: 0.03580  (0.04685)


[1m   --> STEP: 65/161 -- GLOBAL_STEP: 13025[0m
     | > loss_gen: 2.66257  (2.82940)
     | > loss_kl: 1.78438  (1.65577)
     | > loss_feat: 5.25087  (5.66558)
     | > loss_mel: 24.18294  (23.25154)
     | > loss_duration: 1.64051  (1.67812)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.52127  (35.08040)
     | > grad_norm_0: 142.30495  (98.45826)
     | > loss_disc: 1.99547  (2.05446)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.99547  (2.05446)
     | > grad_norm_1: 11.65876  (17.83303)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.70830  (2.06131)
     | > loader_time: 0.03060  (0.05679)


[1m   --> STEP: 90/161 -- GLOBAL_STEP: 13050[0m
     | > loss_gen: 2.83065  (2.82351)
     | > loss_kl: 1.64106  (1.65578)
     | > loss_feat: 5.33130  (5.66404)
     | > loss_mel: 22.72099  (23.23227)
     | > loss_duration: 1.68443  (1.67840)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.20844  (35.05399)
     | > grad_norm_0: 286.19046  (107.97885)
     | > loss_disc: 2.15102  (2.05141)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.15102  (2.05141)
     | > grad_norm_1: 29.75481  (17.10621)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.45630  (2.05112)
     | > loader_time: 0.02170  (0.05577)


[1m   --> STEP: 115/161 -- GLOBAL_STEP: 13075[0m
     | > loss_gen: 2.64822  (2.82199)
     | > loss_kl: 1.49024  (1.65875)
     | > loss_feat: 5.43173  (5.67897)
     | > loss_mel: 22.93363  (23.21589)
     | > loss_duration: 1.72549  (1.68078)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.22931  (35.05638)
     | > grad_norm_0: 76.96384  (106.30022)
     | > loss_disc: 2.16613  (2.04657)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.16613  (2.04657)
     | > grad_norm_1: 14.81834  (16.31471)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.40400  (2.09074)
     | > loader_time: 0.03680  (0.05374)


[1m   --> STEP: 140/161 -- GLOBAL_STEP: 13100[0m
     | > loss_gen: 2.84422  (2.82664)
     | > loss_kl: 1.65120  (1.66627)
     | > loss_feat: 5.73152  (5.69286)
     | > loss_mel: 22.53171  (23.25474)
     | > loss_duration: 1.65269  (1.68279)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.41133  (35.12331)
     | > grad_norm_0: 208.54445  (106.77267)
     | > loss_disc: 1.91687  (2.04064)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.91687  (2.04064)
     | > grad_norm_1: 12.56309  (16.23503)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.01450  (2.12404)
     | > loader_time: 0.03620  (0.05393)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02708 [0m(-0.00080)
     | > avg_loss_gen:[91m 2.80055 [0m(+0.21854)
     | > avg_loss_kl:[92m 1.65284 [0m(-0.13660)
     | > avg_loss_feat:[91m 6.06003 [0m(+0.94848)
     | > avg_loss_mel:[91m 23.94425 [0m(+0.16111)
     | > avg_loss_duration:[92m 1.66442 [0m(-0.00607)
     | > avg_loss_0:[91m 36.12209 [0m(+1.18546)
     | > avg_loss_disc:[92m 2.05737 [0m(-0.34312)
     | > avg_loss_1:[92m 2.05737 [0m(-0.34312)


[4m[1m > EPOCH: 81/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 00:31:11) [0m

[1m   --> STEP: 3/161 -- GLOBAL_STEP: 13125[0m
     | > loss_gen: 3.10917  (2.85441)
     | > loss_kl: 1.49003  (1.67501)
     | > loss_feat: 5.45742  (5.69739)
     | > loss_mel: 23.07777  (23.50981)
     | > loss_duration: 1.66976  (1.64871)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.80415  (35.38533)
     | > grad_norm_0: 187.55649  (101.78520)
     | > loss_disc: 2.07629  (2.04995)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.07629  (2.04995)
     | > grad_norm_1: 27.97175  (17.93765)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.48390  (2.01135)
     | > loader_time: 0.03940  (0.06973)


[1m   --> STEP: 28/161 -- GLOBAL_STEP: 13150[0m
     | > loss_gen: 3.05476  (2.82774)
     | > loss_kl: 1.67438  (1.70744)
     | > loss_feat: 5.64647  (5.64999)
     | > loss_mel: 24.30818  (23.27097)
     | > loss_duration: 1.67729  (1.67967)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.36108  (35.13580)
     | > grad_norm_0: 215.10182  (139.24234)
     | > loss_disc: 1.99120  (2.05600)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.99120  (2.05600)
     | > grad_norm_1: 23.05792  (19.74899)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.77960  (2.06087)
     | > loader_time: 0.03070  (0.04903)


[1m   --> STEP: 53/161 -- GLOBAL_STEP: 13175[0m
     | > loss_gen: 3.04000  (2.81624)
     | > loss_kl: 1.79566  (1.71526)
     | > loss_feat: 5.73827  (5.66052)
     | > loss_mel: 23.24421  (23.17369)
     | > loss_duration: 1.69041  (1.68421)
     | > amp_scaler: 512.00000  (309.13208)
     | > loss_0: 35.50854  (35.04991)
     | > grad_norm_0: 203.18626  (125.77216)
     | > loss_disc: 2.09896  (2.05725)
     | > amp_scaler-1: 512.00000  (309.13208)
     | > loss_1: 2.09896  (2.05725)
     | > grad_norm_1: 10.60337  (17.49592)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.51640  (2.07557)
     | > loader_time: 0.02160  (0.04993)


[1m   --> STEP: 78/161 -- GLOBAL_STEP: 13200[0m
     | > loss_gen: 2.93129  (2.82324)
     | > loss_kl: 1.99094  (1.70721)
     | > loss_feat: 5.69936  (5.71373)
     | > loss_mel: 23.43442  (23.19461)
     | > loss_duration: 1.68548  (1.68456)
     | > amp_scaler: 512.00000  (374.15385)
     | > loss_0: 35.74149  (35.12335)
     | > grad_norm_0: 169.31242  (132.76669)
     | > loss_disc: 2.06487  (2.04572)
     | > amp_scaler-1: 512.00000  (374.15385)
     | > loss_1: 2.06487  (2.04572)
     | > grad_norm_1: 14.41001  (17.40507)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.65590  (2.06885)
     | > loader_time: 0.02680  (0.05157)


[1m   --> STEP: 103/161 -- GLOBAL_STEP: 13225[0m
     | > loss_gen: 2.87273  (2.82021)
     | > loss_kl: 1.55461  (1.70178)
     | > loss_feat: 6.06049  (5.72215)
     | > loss_mel: 22.83533  (23.20379)
     | > loss_duration: 1.72233  (1.68524)
     | > amp_scaler: 512.00000  (407.61165)
     | > loss_0: 35.04549  (35.13317)
     | > grad_norm_0: 142.12321  (128.34833)
     | > loss_disc: 2.01437  (2.04179)
     | > amp_scaler-1: 512.00000  (407.61165)
     | > loss_1: 2.01437  (2.04179)
     | > grad_norm_1: 26.00755  (17.29954)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.34190  (2.06984)
     | > loader_time: 0.03280  (0.04930)


[1m   --> STEP: 128/161 -- GLOBAL_STEP: 13250[0m
     | > loss_gen: 2.83261  (2.82853)
     | > loss_kl: 1.74539  (1.69915)
     | > loss_feat: 6.03465  (5.75046)
     | > loss_mel: 23.16079  (23.24849)
     | > loss_duration: 1.72630  (1.69100)
     | > amp_scaler: 512.00000  (428.00000)
     | > loss_0: 35.49974  (35.21763)
     | > grad_norm_0: 61.69915  (127.19946)
     | > loss_disc: 1.91698  (2.03814)
     | > amp_scaler-1: 512.00000  (428.00000)
     | > loss_1: 1.91698  (2.03814)
     | > grad_norm_1: 9.79858  (17.56023)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.53330  (2.11675)
     | > loader_time: 0.05210  (0.04964)


[1m   --> STEP: 153/161 -- GLOBAL_STEP: 13275[0m
     | > loss_gen: 2.74047  (2.82949)
     | > loss_kl: 1.61458  (1.69836)
     | > loss_feat: 5.88462  (5.76938)
     | > loss_mel: 23.22955  (23.27194)
     | > loss_duration: 1.80381  (1.69020)
     | > amp_scaler: 512.00000  (441.72549)
     | > loss_0: 35.27303  (35.25937)
     | > grad_norm_0: 41.22502  (126.23815)
     | > loss_disc: 2.00975  (2.03789)
     | > amp_scaler-1: 512.00000  (441.72549)
     | > loss_1: 2.00975  (2.03789)
     | > grad_norm_1: 11.01380  (18.01163)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.31920  (2.11710)
     | > loader_time: 0.03730  (0.04996)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02699 [0m(-0.00009)
     | > avg_loss_gen:[92m 2.68718 [0m(-0.11337)
     | > avg_loss_kl:[91m 1.70629 [0m(+0.05345)
     | > avg_loss_feat:[92m 5.81115 [0m(-0.24888)
     | > avg_loss_mel:[92m 23.94005 [0m(-0.00420)
     | > avg_loss_duration:[92m 1.65966 [0m(-0.00477)
     | > avg_loss_0:[92m 35.80432 [0m(-0.31777)
     | > avg_loss_disc:[91m 2.20757 [0m(+0.15020)
     | > avg_loss_1:[91m 2.20757 [0m(+0.15020)


[4m[1m > EPOCH: 82/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 00:37:19) [0m

[1m   --> STEP: 16/161 -- GLOBAL_STEP: 13300[0m
     | > loss_gen: 2.78111  (2.83296)
     | > loss_kl: 1.56455  (1.70811)
     | > loss_feat: 5.21289  (5.83270)
     | > loss_mel: 22.27489  (23.39153)
     | > loss_duration: 1.76176  (1.70511)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 33.59521  (35.47041)
     | > grad_norm_0: 52.52557  (97.74921)
     | > loss_disc: 2.21641  (2.04420)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.21641  (2.04420)
     | > grad_norm_1: 10.15660  (16.50032)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.14960  (2.09365)
     | > loader_time: 0.04220  (0.04273)


[1m   --> STEP: 41/161 -- GLOBAL_STEP: 13325[0m
     | > loss_gen: 2.79986  (2.83630)
     | > loss_kl: 1.69720  (1.70210)
     | > loss_feat: 6.42126  (5.84329)
     | > loss_mel: 23.25661  (23.34017)
     | > loss_duration: 1.64507  (1.69062)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 35.82000  (35.41248)
     | > grad_norm_0: 87.58492  (104.62977)
     | > loss_disc: 1.97035  (2.04349)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.97035  (2.04349)
     | > grad_norm_1: 9.18335  (15.34002)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.59270  (2.08956)
     | > loader_time: 0.02360  (0.04668)


[1m   --> STEP: 66/161 -- GLOBAL_STEP: 13350[0m
     | > loss_gen: 3.09067  (2.83475)
     | > loss_kl: 1.56666  (1.71116)
     | > loss_feat: 5.64821  (5.82753)
     | > loss_mel: 22.39784  (23.25799)
     | > loss_duration: 1.67262  (1.68517)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 34.37600  (35.31660)
     | > grad_norm_0: 163.07802  (109.21664)
     | > loss_disc: 2.08269  (2.03123)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.08269  (2.03123)
     | > grad_norm_1: 14.86152  (14.91900)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.01050  (2.06739)
     | > loader_time: 0.03060  (0.04222)


[1m   --> STEP: 91/161 -- GLOBAL_STEP: 13375[0m
     | > loss_gen: 3.08810  (2.82639)
     | > loss_kl: 1.85377  (1.71046)
     | > loss_feat: 6.11813  (5.76489)
     | > loss_mel: 23.07870  (23.15794)
     | > loss_duration: 1.64790  (1.68347)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 35.78661  (35.14315)
     | > grad_norm_0: 167.88727  (112.89861)
     | > loss_disc: 1.87810  (2.04673)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.87810  (2.04673)
     | > grad_norm_1: 8.79318  (16.73776)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.98860  (2.04882)
     | > loader_time: 0.02920  (0.04591)


[1m   --> STEP: 116/161 -- GLOBAL_STEP: 13400[0m
     | > loss_gen: 2.65913  (2.82965)
     | > loss_kl: 1.38937  (1.69814)
     | > loss_feat: 5.13603  (5.78031)
     | > loss_mel: 22.22459  (23.11236)
     | > loss_duration: 1.75128  (1.68767)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 33.16040  (35.10814)
     | > grad_norm_0: 205.23637  (119.90471)
     | > loss_disc: 2.18077  (2.04314)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.18077  (2.04314)
     | > grad_norm_1: 21.23912  (16.81902)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.17580  (2.07542)
     | > loader_time: 0.03830  (0.04530)


[1m   --> STEP: 141/161 -- GLOBAL_STEP: 13425[0m
     | > loss_gen: 3.34497  (2.85287)
     | > loss_kl: 1.67758  (1.69629)
     | > loss_feat: 6.29396  (5.86640)
     | > loss_mel: 24.58048  (23.19928)
     | > loss_duration: 1.65750  (1.68920)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 37.55450  (35.30404)
     | > grad_norm_0: 56.03414  (116.85191)
     | > loss_disc: 2.07660  (2.03962)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.07660  (2.03962)
     | > grad_norm_1: 19.59887  (16.46303)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.69110  (2.09992)
     | > loader_time: 0.02530  (0.04627)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02781 [0m(+0.00082)
     | > avg_loss_gen:[91m 3.01209 [0m(+0.32491)
     | > avg_loss_kl:[92m 1.54910 [0m(-0.15719)
     | > avg_loss_feat:[91m 6.73032 [0m(+0.91917)
     | > avg_loss_mel:[91m 24.18662 [0m(+0.24657)
     | > avg_loss_duration:[91m 1.66544 [0m(+0.00578)
     | > avg_loss_0:[91m 37.14356 [0m(+1.33924)
     | > avg_loss_disc:[92m 1.66001 [0m(-0.54756)
     | > avg_loss_1:[92m 1.66001 [0m(-0.54756)


[4m[1m > EPOCH: 83/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 00:43:27) [0m

[1m   --> STEP: 4/161 -- GLOBAL_STEP: 13450[0m
     | > loss_gen: 2.35423  (2.55038)
     | > loss_kl: 1.84060  (1.83288)
     | > loss_feat: 5.50138  (5.83496)
     | > loss_mel: 24.65808  (23.33961)
     | > loss_duration: 1.64740  (1.67089)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 36.00169  (35.22873)
     | > grad_norm_0: 54.76539  (127.98974)
     | > loss_disc: 2.47986  (2.13857)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.47986  (2.13857)
     | > grad_norm_1: 24.54392  (22.36142)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.89530  (1.93836)
     | > loader_time: 0.03820  (0.10065)


[1m   --> STEP: 29/161 -- GLOBAL_STEP: 13475[0m
     | > loss_gen: 2.61124  (2.87447)
     | > loss_kl: 1.73537  (1.73784)
     | > loss_feat: 5.96046  (6.03954)
     | > loss_mel: 24.15596  (23.47160)
     | > loss_duration: 1.70246  (1.67705)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 36.16550  (35.80051)
     | > grad_norm_0: 189.80202  (101.89936)
     | > loss_disc: 1.87521  (2.13382)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 1.87521  (2.13382)
     | > grad_norm_1: 11.67742  (18.86029)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.90130  (2.08037)
     | > loader_time: 0.03250  (0.05542)


[1m   --> STEP: 54/161 -- GLOBAL_STEP: 13500[0m
     | > loss_gen: 2.62198  (2.83554)
     | > loss_kl: 1.63452  (1.71726)
     | > loss_feat: 5.00764  (5.83536)
     | > loss_mel: 22.93320  (23.36705)
     | > loss_duration: 1.66075  (1.68007)
     | > amp_scaler: 512.00000  (512.00000)
     | > loss_0: 33.85809  (35.43528)
     | > grad_norm_0: 74.62825  (99.46841)
     | > loss_disc: 2.20859  (2.11384)
     | > amp_scaler-1: 512.00000  (512.00000)
     | > loss_1: 2.20859  (2.11384)
     | > grad_norm_1: 16.84971  (16.36449)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.37880  (2.10214)
     | > loader_time: 0.14250  (0.04866)


[1m   --> STEP: 79/161 -- GLOBAL_STEP: 13525[0m
     | > loss_gen: 2.82455  (2.83481)
     | > loss_kl: 1.69319  (1.71295)
     | > loss_feat: 5.83814  (5.83034)
     | > loss_mel: 23.78139  (23.34989)
     | > loss_duration: 1.64098  (1.68412)
     | > amp_scaler: 256.00000  (479.59494)
     | > loss_0: 35.77825  (35.41210)
     | > grad_norm_0: 147.89412  (110.71465)
     | > loss_disc: 1.99203  (2.08270)
     | > amp_scaler-1: 256.00000  (479.59494)
     | > loss_1: 1.99203  (2.08270)
     | > grad_norm_1: 14.89632  (16.96363)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.99690  (2.08705)
     | > loader_time: 0.03360  (0.04944)


[1m   --> STEP: 104/161 -- GLOBAL_STEP: 13550[0m
     | > loss_gen: 3.16747  (2.84460)
     | > loss_kl: 1.81991  (1.70683)
     | > loss_feat: 6.26609  (5.84746)
     | > loss_mel: 23.64713  (23.33397)
     | > loss_duration: 1.69228  (1.68391)
     | > amp_scaler: 256.00000  (425.84615)
     | > loss_0: 36.59288  (35.41677)
     | > grad_norm_0: 159.38890  (119.17121)
     | > loss_disc: 1.87075  (2.05778)
     | > amp_scaler-1: 256.00000  (425.84615)
     | > loss_1: 1.87075  (2.05778)
     | > grad_norm_1: 11.32450  (15.94272)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.60370  (2.08960)
     | > loader_time: 0.02730  (0.04960)


[1m   --> STEP: 129/161 -- GLOBAL_STEP: 13575[0m
     | > loss_gen: 2.96908  (2.85303)
     | > loss_kl: 1.70686  (1.70484)
     | > loss_feat: 5.98062  (5.86218)
     | > loss_mel: 22.51013  (23.30313)
     | > loss_duration: 1.72109  (1.68601)
     | > amp_scaler: 256.00000  (392.93023)
     | > loss_0: 34.88778  (35.40920)
     | > grad_norm_0: 189.56209  (116.98924)
     | > loss_disc: 2.00995  (2.05036)
     | > amp_scaler-1: 256.00000  (392.93023)
     | > loss_1: 2.00995  (2.05036)
     | > grad_norm_1: 21.52944  (15.71786)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.33470  (2.14437)
     | > loader_time: 0.04390  (0.05014)


[1m   --> STEP: 154/161 -- GLOBAL_STEP: 13600[0m
     | > loss_gen: 2.75186  (2.84998)
     | > loss_kl: 1.77231  (1.70528)
     | > loss_feat: 5.77036  (5.85128)
     | > loss_mel: 23.51884  (23.29754)
     | > loss_duration: 1.66693  (1.68629)
     | > amp_scaler: 256.00000  (370.70130)
     | > loss_0: 35.48029  (35.39036)
     | > grad_norm_0: 173.74251  (122.49753)
     | > loss_disc: 2.11960  (2.04617)
     | > amp_scaler-1: 256.00000  (370.70130)
     | > loss_1: 2.11960  (2.04617)
     | > grad_norm_1: 17.89478  (16.25427)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.61980  (2.13573)
     | > loader_time: 0.03120  (0.05054)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02878 [0m(+0.00097)
     | > avg_loss_gen:[92m 2.93201 [0m(-0.08008)
     | > avg_loss_kl:[91m 1.62030 [0m(+0.07120)
     | > avg_loss_feat:[92m 6.59020 [0m(-0.14011)
     | > avg_loss_mel:[92m 23.81541 [0m(-0.37121)
     | > avg_loss_duration:[91m 1.66990 [0m(+0.00447)
     | > avg_loss_0:[92m 36.62783 [0m(-0.51573)
     | > avg_loss_disc:[91m 1.82735 [0m(+0.16735)
     | > avg_loss_1:[91m 1.82735 [0m(+0.16735)


[4m[1m > EPOCH: 84/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 00:49:33) [0m

[1m   --> STEP: 17/161 -- GLOBAL_STEP: 13625[0m
     | > loss_gen: 3.05646  (2.85696)
     | > loss_kl: 1.70759  (1.64861)
     | > loss_feat: 5.78140  (5.74803)
     | > loss_mel: 23.08399  (23.03494)
     | > loss_duration: 1.68013  (1.68495)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.30958  (34.97348)
     | > grad_norm_0: 138.10109  (103.95222)
     | > loss_disc: 1.96552  (2.02634)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.96552  (2.02634)
     | > grad_norm_1: 12.65201  (13.08384)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.09780  (2.06237)
     | > loader_time: 0.03450  (0.04057)


[1m   --> STEP: 42/161 -- GLOBAL_STEP: 13650[0m
     | > loss_gen: 2.44809  (2.81362)
     | > loss_kl: 1.61123  (1.69774)
     | > loss_feat: 5.64104  (5.73030)
     | > loss_mel: 22.63092  (23.16267)
     | > loss_duration: 1.64249  (1.68617)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.97377  (35.09050)
     | > grad_norm_0: 88.40752  (99.61767)
     | > loss_disc: 2.01301  (2.05091)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.01301  (2.05091)
     | > grad_norm_1: 20.53237  (17.39466)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.84940  (2.09150)
     | > loader_time: 0.02800  (0.03710)


[1m   --> STEP: 67/161 -- GLOBAL_STEP: 13675[0m
     | > loss_gen: 2.80755  (2.83060)
     | > loss_kl: 1.62393  (1.71410)
     | > loss_feat: 5.51840  (5.78880)
     | > loss_mel: 22.12363  (23.14646)
     | > loss_duration: 1.72593  (1.68514)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.79945  (35.16509)
     | > grad_norm_0: 73.43484  (102.80367)
     | > loss_disc: 2.12982  (2.04415)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.12982  (2.04415)
     | > grad_norm_1: 10.56169  (16.44967)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.85090  (2.05911)
     | > loader_time: 0.03210  (0.03995)


[1m   --> STEP: 92/161 -- GLOBAL_STEP: 13700[0m
     | > loss_gen: 3.00922  (2.83224)
     | > loss_kl: 1.51049  (1.70511)
     | > loss_feat: 5.72425  (5.78147)
     | > loss_mel: 22.63645  (23.09526)
     | > loss_duration: 1.67551  (1.68688)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.55592  (35.10095)
     | > grad_norm_0: 74.91813  (115.09624)
     | > loss_disc: 2.06706  (2.04408)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06706  (2.04408)
     | > grad_norm_1: 18.56977  (17.72042)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.15110  (2.04679)
     | > loader_time: 0.03310  (0.04138)


[1m   --> STEP: 117/161 -- GLOBAL_STEP: 13725[0m
     | > loss_gen: 2.89122  (2.83282)
     | > loss_kl: 1.64539  (1.71018)
     | > loss_feat: 5.60135  (5.80122)
     | > loss_mel: 21.71481  (23.12013)
     | > loss_duration: 1.70236  (1.68952)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.55514  (35.15387)
     | > grad_norm_0: 229.17606  (109.70167)
     | > loss_disc: 2.08742  (2.04148)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.08742  (2.04148)
     | > grad_norm_1: 28.27020  (16.46865)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.44190  (2.08322)
     | > loader_time: 0.04420  (0.04179)


[1m   --> STEP: 142/161 -- GLOBAL_STEP: 13750[0m
     | > loss_gen: 2.92020  (2.83261)
     | > loss_kl: 1.71896  (1.70719)
     | > loss_feat: 5.74675  (5.79084)
     | > loss_mel: 24.24568  (23.10115)
     | > loss_duration: 1.76971  (1.68860)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.40129  (35.12039)
     | > grad_norm_0: 242.94940  (109.10928)
     | > loss_disc: 2.06137  (2.03827)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06137  (2.03827)
     | > grad_norm_1: 16.02411  (16.25221)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.99160  (2.10252)
     | > loader_time: 0.03160  (0.04392)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02781 [0m(-0.00098)
     | > avg_loss_gen:[91m 3.17144 [0m(+0.23943)
     | > avg_loss_kl:[91m 1.75912 [0m(+0.13881)
     | > avg_loss_feat:[92m 6.27056 [0m(-0.31964)
     | > avg_loss_mel:[92m 23.32203 [0m(-0.49338)
     | > avg_loss_duration:[92m 1.65429 [0m(-0.01561)
     | > avg_loss_0:[92m 36.17744 [0m(-0.45039)
     | > avg_loss_disc:[91m 1.95326 [0m(+0.12591)
     | > avg_loss_1:[91m 1.95326 [0m(+0.12591)


[4m[1m > EPOCH: 85/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 00:55:41) [0m

[1m   --> STEP: 5/161 -- GLOBAL_STEP: 13775[0m
     | > loss_gen: 2.75861  (2.82458)
     | > loss_kl: 1.56377  (1.64961)
     | > loss_feat: 5.19220  (5.91001)
     | > loss_mel: 21.24127  (23.31069)
     | > loss_duration: 1.71016  (1.67747)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 32.46601  (35.37236)
     | > grad_norm_0: 99.09279  (121.32598)
     | > loss_disc: 2.10295  (2.02419)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.10295  (2.02419)
     | > grad_norm_1: 18.00436  (19.87713)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.29100  (2.04984)
     | > loader_time: 0.16840  (0.08617)


[1m   --> STEP: 30/161 -- GLOBAL_STEP: 13800[0m
     | > loss_gen: 2.66451  (2.81190)
     | > loss_kl: 1.87835  (1.69607)
     | > loss_feat: 6.14381  (5.83327)
     | > loss_mel: 23.09845  (23.22253)
     | > loss_duration: 1.75034  (1.68602)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.53545  (35.24979)
     | > grad_norm_0: 76.81872  (107.74875)
     | > loss_disc: 2.00135  (2.05267)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.00135  (2.05267)
     | > grad_norm_1: 12.61063  (19.76616)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.44100  (2.06387)
     | > loader_time: 0.03930  (0.04527)


[1m   --> STEP: 55/161 -- GLOBAL_STEP: 13825[0m
     | > loss_gen: 2.77779  (2.83677)
     | > loss_kl: 1.72228  (1.70926)
     | > loss_feat: 5.57602  (5.82354)
     | > loss_mel: 22.47138  (23.28804)
     | > loss_duration: 1.68947  (1.67978)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.23695  (35.33739)
     | > grad_norm_0: 87.80244  (121.94823)
     | > loss_disc: 2.22124  (2.05517)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.22124  (2.05517)
     | > grad_norm_1: 16.56394  (19.78902)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.49870  (2.08433)
     | > loader_time: 0.12930  (0.04596)


[1m   --> STEP: 80/161 -- GLOBAL_STEP: 13850[0m
     | > loss_gen: 2.62229  (2.82398)
     | > loss_kl: 1.84715  (1.70439)
     | > loss_feat: 5.76477  (5.78682)
     | > loss_mel: 23.43470  (23.21726)
     | > loss_duration: 1.63787  (1.67818)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.30678  (35.21064)
     | > grad_norm_0: 36.67040  (125.65395)
     | > loss_disc: 2.04707  (2.06358)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.04707  (2.06358)
     | > grad_norm_1: 7.16730  (19.42254)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.31710  (2.07634)
     | > loader_time: 0.03460  (0.04785)


[1m   --> STEP: 105/161 -- GLOBAL_STEP: 13875[0m
     | > loss_gen: 2.56942  (2.83044)
     | > loss_kl: 1.62703  (1.70623)
     | > loss_feat: 5.44926  (5.80133)
     | > loss_mel: 23.71182  (23.17925)
     | > loss_duration: 1.67342  (1.68207)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.03095  (35.19931)
     | > grad_norm_0: 39.41597  (123.39960)
     | > loss_disc: 2.10342  (2.05004)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.10342  (2.05004)
     | > grad_norm_1: 9.26249  (18.17489)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.38130  (2.07552)
     | > loader_time: 0.19640  (0.05068)


[1m   --> STEP: 130/161 -- GLOBAL_STEP: 13900[0m
     | > loss_gen: 2.88280  (2.83226)
     | > loss_kl: 1.76752  (1.69373)
     | > loss_feat: 5.78728  (5.79897)
     | > loss_mel: 24.03522  (23.16059)
     | > loss_duration: 1.69669  (1.68377)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.16952  (35.16933)
     | > grad_norm_0: 383.86697  (126.55371)
     | > loss_disc: 1.91764  (2.04604)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.91764  (2.04604)
     | > grad_norm_1: 16.32076  (17.56325)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.41400  (2.13069)
     | > loader_time: 0.04170  (0.05784)


[1m   --> STEP: 155/161 -- GLOBAL_STEP: 13925[0m
     | > loss_gen: 2.99810  (2.83456)
     | > loss_kl: 1.68547  (1.69955)
     | > loss_feat: 5.42905  (5.80858)
     | > loss_mel: 23.30496  (23.13672)
     | > loss_duration: 1.64709  (1.68460)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.06467  (35.16400)
     | > grad_norm_0: 65.55444  (128.20264)
     | > loss_disc: 2.09686  (2.04356)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.09686  (2.04356)
     | > grad_norm_1: 15.46007  (17.95378)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.74190  (2.11980)
     | > loader_time: 0.02700  (0.05490)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02758 [0m(-0.00023)
     | > avg_loss_gen:[92m 2.98264 [0m(-0.18880)
     | > avg_loss_kl:[92m 1.73374 [0m(-0.02537)
     | > avg_loss_feat:[92m 5.89995 [0m(-0.37062)
     | > avg_loss_mel:[92m 22.63904 [0m(-0.68299)
     | > avg_loss_duration:[91m 1.65885 [0m(+0.00456)
     | > avg_loss_0:[92m 34.91423 [0m(-1.26322)
     | > avg_loss_disc:[92m 1.92103 [0m(-0.03224)
     | > avg_loss_1:[92m 1.92103 [0m(-0.03224)


[4m[1m > EPOCH: 86/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 01:01:45) [0m

[1m   --> STEP: 18/161 -- GLOBAL_STEP: 13950[0m
     | > loss_gen: 2.94569  (2.85476)
     | > loss_kl: 1.77778  (1.75653)
     | > loss_feat: 5.57342  (5.84315)
     | > loss_mel: 22.79524  (23.20440)
     | > loss_duration: 1.65919  (1.69461)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.75133  (35.35346)
     | > grad_norm_0: 119.82935  (145.80418)
     | > loss_disc: 2.06467  (2.02432)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06467  (2.02432)
     | > grad_norm_1: 71.55783  (23.43236)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.17080  (2.03669)
     | > loader_time: 0.03480  (0.05441)


[1m   --> STEP: 43/161 -- GLOBAL_STEP: 13975[0m
     | > loss_gen: 2.79057  (2.82404)
     | > loss_kl: 1.59956  (1.71974)
     | > loss_feat: 5.93469  (5.83253)
     | > loss_mel: 23.34291  (23.28162)
     | > loss_duration: 1.78027  (1.68873)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.44800  (35.34666)
     | > grad_norm_0: 153.34738  (139.69563)
     | > loss_disc: 2.02707  (2.03784)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.02707  (2.03784)
     | > grad_norm_1: 26.70569  (21.83881)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.41510  (2.07750)
     | > loader_time: 0.03880  (0.04920)


[1m   --> STEP: 68/161 -- GLOBAL_STEP: 14000[0m
     | > loss_gen: 2.92909  (2.82655)
     | > loss_kl: 1.90619  (1.70786)
     | > loss_feat: 6.40955  (5.80683)
     | > loss_mel: 22.86475  (23.16219)
     | > loss_duration: 1.70748  (1.68923)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.81707  (35.19266)
     | > grad_norm_0: 118.05217  (125.02801)
     | > loss_disc: 1.96368  (2.03909)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.96368  (2.03909)
     | > grad_norm_1: 9.37433  (20.65920)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.09110  (2.04819)
     | > loader_time: 0.03210  (0.04440)


[1m   --> STEP: 93/161 -- GLOBAL_STEP: 14025[0m
     | > loss_gen: 2.88791  (2.82401)
     | > loss_kl: 1.68444  (1.70817)
     | > loss_feat: 5.24420  (5.77609)
     | > loss_mel: 22.52000  (23.15731)
     | > loss_duration: 1.69234  (1.68752)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.02888  (35.15310)
     | > grad_norm_0: 306.91830  (135.81039)
     | > loss_disc: 2.01664  (2.04753)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.01664  (2.04753)
     | > grad_norm_1: 87.65669  (21.65430)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.97080  (2.05327)
     | > loader_time: 0.05400  (0.04438)


[1m   --> STEP: 118/161 -- GLOBAL_STEP: 14050[0m
     | > loss_gen: 2.65759  (2.81623)
     | > loss_kl: 1.67026  (1.71355)
     | > loss_feat: 6.24351  (5.76081)
     | > loss_mel: 23.21788  (23.13638)
     | > loss_duration: 1.72569  (1.68629)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.51493  (35.11327)
     | > grad_norm_0: 43.43202  (131.68248)
     | > loss_disc: 2.11174  (2.05424)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.11174  (2.05424)
     | > grad_norm_1: 28.38711  (20.84437)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.43320  (2.10094)
     | > loader_time: 0.04560  (0.04491)


[1m   --> STEP: 143/161 -- GLOBAL_STEP: 14075[0m
     | > loss_gen: 2.62308  (2.81764)
     | > loss_kl: 1.83083  (1.70496)
     | > loss_feat: 6.26771  (5.76125)
     | > loss_mel: 23.35642  (23.10396)
     | > loss_duration: 1.69371  (1.68648)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.77176  (35.07429)
     | > grad_norm_0: 43.59876  (129.11086)
     | > loss_disc: 1.95196  (2.05417)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.95196  (2.05417)
     | > grad_norm_1: 8.99029  (19.79095)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.14260  (2.11392)
     | > loader_time: 0.03990  (0.04845)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02727 [0m(-0.00030)
     | > avg_loss_gen:[91m 3.26288 [0m(+0.28024)
     | > avg_loss_kl:[92m 1.69519 [0m(-0.03855)
     | > avg_loss_feat:[91m 6.68912 [0m(+0.78917)
     | > avg_loss_mel:[91m 23.72984 [0m(+1.09080)
     | > avg_loss_duration:[91m 1.68452 [0m(+0.02567)
     | > avg_loss_0:[91m 37.06155 [0m(+2.14733)
     | > avg_loss_disc:[92m 1.84222 [0m(-0.07881)
     | > avg_loss_1:[92m 1.84222 [0m(-0.07881)


[4m[1m > EPOCH: 87/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 01:07:52) [0m

[1m   --> STEP: 6/161 -- GLOBAL_STEP: 14100[0m
     | > loss_gen: 2.71962  (2.75854)
     | > loss_kl: 1.53203  (1.67972)
     | > loss_feat: 5.93266  (5.70926)
     | > loss_mel: 22.78813  (23.58883)
     | > loss_duration: 1.68454  (1.68155)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.65697  (35.41789)
     | > grad_norm_0: 57.48106  (120.21804)
     | > loss_disc: 2.03618  (2.06053)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.03618  (2.06053)
     | > grad_norm_1: 7.49485  (17.19109)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.36480  (2.13594)
     | > loader_time: 0.03840  (0.05007)


[1m   --> STEP: 31/161 -- GLOBAL_STEP: 14125[0m
     | > loss_gen: 2.53035  (2.84562)
     | > loss_kl: 1.66319  (1.70090)
     | > loss_feat: 5.62707  (5.93728)
     | > loss_mel: 22.45060  (23.42301)
     | > loss_duration: 1.70874  (1.68281)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.97994  (35.58962)
     | > grad_norm_0: 334.92661  (152.76932)
     | > loss_disc: 2.30028  (2.03904)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.30028  (2.03904)
     | > grad_norm_1: 30.64839  (14.92258)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.44350  (2.09393)
     | > loader_time: 0.05970  (0.05297)


[1m   --> STEP: 56/161 -- GLOBAL_STEP: 14150[0m
     | > loss_gen: 3.31806  (2.83722)
     | > loss_kl: 1.60390  (1.68809)
     | > loss_feat: 6.12846  (5.82764)
     | > loss_mel: 24.09903  (23.23682)
     | > loss_duration: 1.66480  (1.67929)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.81425  (35.26906)
     | > grad_norm_0: 173.61206  (142.42223)
     | > loss_disc: 1.89048  (2.03948)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.89048  (2.03948)
     | > grad_norm_1: 37.64378  (15.50868)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.12350  (2.09320)
     | > loader_time: 0.03690  (0.05042)


[1m   --> STEP: 81/161 -- GLOBAL_STEP: 14175[0m
     | > loss_gen: 2.56872  (2.82129)
     | > loss_kl: 1.69212  (1.69082)
     | > loss_feat: 5.21905  (5.79460)
     | > loss_mel: 23.85699  (23.18063)
     | > loss_duration: 1.65822  (1.68174)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.99510  (35.16908)
     | > grad_norm_0: 61.14763  (139.43706)
     | > loss_disc: 2.21365  (2.05524)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.21365  (2.05524)
     | > grad_norm_1: 10.79064  (18.56674)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.87430  (2.05479)
     | > loader_time: 0.03060  (0.04946)


[1m   --> STEP: 106/161 -- GLOBAL_STEP: 14200[0m
     | > loss_gen: 2.74652  (2.81675)
     | > loss_kl: 1.60892  (1.67887)
     | > loss_feat: 5.36156  (5.76969)
     | > loss_mel: 21.97908  (23.08913)
     | > loss_duration: 1.68813  (1.68285)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.38421  (35.03728)
     | > grad_norm_0: 165.69719  (126.42722)
     | > loss_disc: 2.22277  (2.05915)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.22277  (2.05915)
     | > grad_norm_1: 23.82107  (17.65302)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.05830  (2.06049)
     | > loader_time: 0.04540  (0.04614)


[1m   --> STEP: 131/161 -- GLOBAL_STEP: 14225[0m
     | > loss_gen: 2.50700  (2.80555)
     | > loss_kl: 1.80565  (1.68303)
     | > loss_feat: 5.77767  (5.74608)
     | > loss_mel: 24.23957  (23.07476)
     | > loss_duration: 1.64748  (1.68699)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.97738  (34.99640)
     | > grad_norm_0: 70.61017  (121.10259)
     | > loss_disc: 2.15264  (2.06330)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.15264  (2.06330)
     | > grad_norm_1: 15.23461  (17.75644)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.03170  (2.11369)
     | > loader_time: 0.03320  (0.04811)


[1m   --> STEP: 156/161 -- GLOBAL_STEP: 14250[0m
     | > loss_gen: 2.67085  (2.81210)
     | > loss_kl: 1.82898  (1.68608)
     | > loss_feat: 5.98751  (5.76020)
     | > loss_mel: 23.27951  (23.08672)
     | > loss_duration: 1.71272  (1.68448)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.47956  (35.02957)
     | > grad_norm_0: 179.67242  (123.46773)
     | > loss_disc: 1.92411  (2.05968)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.92411  (2.05968)
     | > grad_norm_1: 14.42819  (17.74726)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.05200  (2.10166)
     | > loader_time: 0.03520  (0.04753)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02767 [0m(+0.00040)
     | > avg_loss_gen:[92m 2.29193 [0m(-0.97095)
     | > avg_loss_kl:[92m 1.68005 [0m(-0.01514)
     | > avg_loss_feat:[92m 5.79654 [0m(-0.89258)
     | > avg_loss_mel:[92m 22.65049 [0m(-1.07935)
     | > avg_loss_duration:[91m 1.68490 [0m(+0.00038)
     | > avg_loss_0:[92m 34.10390 [0m(-2.95765)
     | > avg_loss_disc:[91m 2.19047 [0m(+0.34825)
     | > avg_loss_1:[91m 2.19047 [0m(+0.34825)


[4m[1m > EPOCH: 88/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 01:13:59) [0m

[1m   --> STEP: 19/161 -- GLOBAL_STEP: 14275[0m
     | > loss_gen: 2.81658  (2.82437)
     | > loss_kl: 1.74246  (1.75486)
     | > loss_feat: 6.14337  (5.71949)
     | > loss_mel: 23.84007  (23.09367)
     | > loss_duration: 1.70340  (1.69109)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.24588  (35.08347)
     | > grad_norm_0: 128.88174  (155.09329)
     | > loss_disc: 2.03205  (2.05137)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.03205  (2.05137)
     | > grad_norm_1: 11.60455  (17.53020)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.11870  (2.06865)
     | > loader_time: 0.03450  (0.04461)


[1m   --> STEP: 44/161 -- GLOBAL_STEP: 14300[0m
     | > loss_gen: 2.74965  (2.80758)
     | > loss_kl: 1.76523  (1.74603)
     | > loss_feat: 5.54767  (5.71106)
     | > loss_mel: 22.46985  (23.01000)
     | > loss_duration: 1.62210  (1.68028)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.15450  (34.95493)
     | > grad_norm_0: 32.40272  (121.26020)
     | > loss_disc: 2.15437  (2.06626)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.15437  (2.06626)
     | > grad_norm_1: 16.31433  (15.60653)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.95340  (2.09565)
     | > loader_time: 0.03350  (0.04337)


[1m   --> STEP: 69/161 -- GLOBAL_STEP: 14325[0m
     | > loss_gen: 2.85388  (2.79974)
     | > loss_kl: 1.52366  (1.74568)
     | > loss_feat: 5.33122  (5.69820)
     | > loss_mel: 22.69369  (23.07852)
     | > loss_duration: 1.67071  (1.68157)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.07317  (35.00372)
     | > grad_norm_0: 134.76985  (126.63856)
     | > loss_disc: 2.05307  (2.06287)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.05307  (2.06287)
     | > grad_norm_1: 18.85239  (17.58054)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.78140  (2.05540)
     | > loader_time: 0.02740  (0.04225)


[1m   --> STEP: 94/161 -- GLOBAL_STEP: 14350[0m
     | > loss_gen: 2.47625  (2.79157)
     | > loss_kl: 1.86810  (1.73697)
     | > loss_feat: 5.25763  (5.70463)
     | > loss_mel: 23.66837  (23.02402)
     | > loss_duration: 1.70375  (1.68356)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.97411  (34.94075)
     | > grad_norm_0: 145.23608  (126.76046)
     | > loss_disc: 2.19673  (2.06899)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.19673  (2.06899)
     | > grad_norm_1: 22.66692  (19.14109)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.26560  (2.07291)
     | > loader_time: 0.04090  (0.04338)


[1m   --> STEP: 119/161 -- GLOBAL_STEP: 14375[0m
     | > loss_gen: 2.93718  (2.79317)
     | > loss_kl: 1.62566  (1.72441)
     | > loss_feat: 5.67489  (5.69334)
     | > loss_mel: 24.13208  (23.05092)
     | > loss_duration: 1.69240  (1.68461)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.06221  (34.94645)
     | > grad_norm_0: 204.35872  (123.39888)
     | > loss_disc: 2.11100  (2.06872)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.11100  (2.06872)
     | > grad_norm_1: 19.26331  (18.85789)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.08070  (2.10678)
     | > loader_time: 0.03610  (0.04561)


[1m   --> STEP: 144/161 -- GLOBAL_STEP: 14400[0m
     | > loss_gen: 2.75453  (2.80239)
     | > loss_kl: 1.74729  (1.71177)
     | > loss_feat: 5.72737  (5.72931)
     | > loss_mel: 21.54352  (23.04971)
     | > loss_duration: 1.71851  (1.68400)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.49122  (34.97718)
     | > grad_norm_0: 205.75394  (123.74879)
     | > loss_disc: 2.04410  (2.06200)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.04410  (2.06200)
     | > grad_norm_1: 21.26949  (18.14335)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.00000  (2.12802)
     | > loader_time: 0.03660  (0.04738)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02813 [0m(+0.00047)
     | > avg_loss_gen:[91m 2.69184 [0m(+0.39990)
     | > avg_loss_kl:[91m 1.93935 [0m(+0.25930)
     | > avg_loss_feat:[91m 6.71367 [0m(+0.91713)
     | > avg_loss_mel:[91m 24.26677 [0m(+1.61628)
     | > avg_loss_duration:[92m 1.66868 [0m(-0.01622)
     | > avg_loss_0:[91m 37.28030 [0m(+3.17640)
     | > avg_loss_disc:[92m 1.93622 [0m(-0.25425)
     | > avg_loss_1:[92m 1.93622 [0m(-0.25425)


[4m[1m > EPOCH: 89/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 01:20:05) [0m

[1m   --> STEP: 7/161 -- GLOBAL_STEP: 14425[0m
     | > loss_gen: 2.82809  (2.76740)
     | > loss_kl: 1.62861  (1.78221)
     | > loss_feat: 6.03567  (5.70568)
     | > loss_mel: 21.94278  (23.04329)
     | > loss_duration: 1.75953  (1.70097)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.19468  (34.99954)
     | > grad_norm_0: 36.38679  (114.02992)
     | > loss_disc: 2.01579  (2.04270)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.01579  (2.04270)
     | > grad_norm_1: 7.88901  (26.02453)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.86170  (2.03658)
     | > loader_time: 0.03420  (0.05920)


[1m   --> STEP: 32/161 -- GLOBAL_STEP: 14450[0m
     | > loss_gen: 3.24730  (2.81795)
     | > loss_kl: 1.58504  (1.71542)
     | > loss_feat: 6.09969  (5.74786)
     | > loss_mel: 23.61876  (23.13552)
     | > loss_duration: 1.67457  (1.67838)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.22536  (35.09513)
     | > grad_norm_0: 74.05942  (97.16691)
     | > loss_disc: 2.00229  (2.03996)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.00229  (2.03996)
     | > grad_norm_1: 11.06782  (19.37924)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.23830  (2.07521)
     | > loader_time: 0.04210  (0.04024)


[1m   --> STEP: 57/161 -- GLOBAL_STEP: 14475[0m
     | > loss_gen: 2.66379  (2.80984)
     | > loss_kl: 1.80106  (1.72796)
     | > loss_feat: 6.34390  (5.75277)
     | > loss_mel: 23.43837  (22.95095)
     | > loss_duration: 1.67826  (1.67686)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.92539  (34.91837)
     | > grad_norm_0: 71.54398  (115.92089)
     | > loss_disc: 1.96189  (2.04650)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.96189  (2.04650)
     | > grad_norm_1: 15.25518  (19.79567)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.02640  (2.07198)
     | > loader_time: 0.03570  (0.04391)


[1m   --> STEP: 82/161 -- GLOBAL_STEP: 14500[0m
     | > loss_gen: 2.81188  (2.82269)
     | > loss_kl: 1.84563  (1.72505)
     | > loss_feat: 5.49542  (5.79085)
     | > loss_mel: 22.89385  (23.01547)
     | > loss_duration: 1.59758  (1.67663)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.64436  (35.03070)
     | > grad_norm_0: 204.31856  (125.11280)
     | > loss_disc: 2.07280  (2.05008)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.07280  (2.05008)
     | > grad_norm_1: 13.98345  (20.79728)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.78570  (2.04367)
     | > loader_time: 0.02640  (0.04578)


[1m   --> STEP: 107/161 -- GLOBAL_STEP: 14525[0m
     | > loss_gen: 2.77906  (2.81790)
     | > loss_kl: 1.54096  (1.71800)
     | > loss_feat: 5.96733  (5.80300)
     | > loss_mel: 21.64449  (22.98048)
     | > loss_duration: 1.75527  (1.68021)
     | > amp_scaler: 512.00000  (279.92523)
     | > loss_0: 33.68711  (34.99959)
     | > grad_norm_0: 58.43365  (128.05315)
     | > loss_disc: 1.98263  (2.05457)
     | > amp_scaler-1: 512.00000  (279.92523)
     | > loss_1: 1.98263  (2.05457)
     | > grad_norm_1: 16.87577  (20.86236)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.15690  (2.04988)
     | > loader_time: 0.03590  (0.04439)


[1m   --> STEP: 132/161 -- GLOBAL_STEP: 14550[0m
     | > loss_gen: 2.75136  (2.82388)
     | > loss_kl: 1.71758  (1.71472)
     | > loss_feat: 5.76735  (5.82261)
     | > loss_mel: 22.98330  (23.00678)
     | > loss_duration: 1.71216  (1.68222)
     | > amp_scaler: 512.00000  (323.87879)
     | > loss_0: 34.93176  (35.05022)
     | > grad_norm_0: 156.66505  (128.19977)
     | > loss_disc: 2.12667  (2.05109)
     | > amp_scaler-1: 512.00000  (323.87879)
     | > loss_1: 2.12667  (2.05109)
     | > grad_norm_1: 13.85681  (20.62471)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.75930  (2.09679)
     | > loader_time: 0.02980  (0.04414)


[1m   --> STEP: 157/161 -- GLOBAL_STEP: 14575[0m
     | > loss_gen: 2.71584  (2.82207)
     | > loss_kl: 1.80788  (1.71136)
     | > loss_feat: 5.89363  (5.82323)
     | > loss_mel: 23.36698  (23.02876)
     | > loss_duration: 1.64431  (1.67983)
     | > amp_scaler: 256.00000  (337.52866)
     | > loss_0: 35.42863  (35.06525)
     | > grad_norm_0: 99.21392  (125.38184)
     | > loss_disc: 2.05896  (2.05282)
     | > amp_scaler-1: 256.00000  (337.52866)
     | > loss_1: 2.05896  (2.05282)
     | > grad_norm_1: 38.41733  (21.19632)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.20100  (2.09870)
     | > loader_time: 0.03840  (0.04420)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02797 [0m(-0.00016)
     | > avg_loss_gen:[91m 2.84850 [0m(+0.15667)
     | > avg_loss_kl:[92m 1.67477 [0m(-0.26457)
     | > avg_loss_feat:[92m 5.74600 [0m(-0.96767)
     | > avg_loss_mel:[91m 24.34465 [0m(+0.07789)
     | > avg_loss_duration:[91m 1.66893 [0m(+0.00025)
     | > avg_loss_0:[92m 36.28287 [0m(-0.99744)
     | > avg_loss_disc:[91m 2.10014 [0m(+0.16392)
     | > avg_loss_1:[91m 2.10014 [0m(+0.16392)


[4m[1m > EPOCH: 90/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 01:26:13) [0m

[1m   --> STEP: 20/161 -- GLOBAL_STEP: 14600[0m
     | > loss_gen: 2.92229  (2.82644)
     | > loss_kl: 1.71897  (1.72956)
     | > loss_feat: 5.86482  (5.83286)
     | > loss_mel: 22.50927  (23.05388)
     | > loss_duration: 1.72775  (1.69044)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.74309  (35.13317)
     | > grad_norm_0: 34.47849  (80.10499)
     | > loss_disc: 2.02344  (2.05718)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.02344  (2.05718)
     | > grad_norm_1: 10.55073  (17.47023)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.93900  (2.00291)
     | > loader_time: 0.02790  (0.05041)


[1m   --> STEP: 45/161 -- GLOBAL_STEP: 14625[0m
     | > loss_gen: 2.94761  (2.85888)
     | > loss_kl: 1.66097  (1.72171)
     | > loss_feat: 5.36412  (5.87879)
     | > loss_mel: 22.61346  (23.11573)
     | > loss_duration: 1.61888  (1.69166)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.20504  (35.26677)
     | > grad_norm_0: 98.24014  (98.94849)
     | > loss_disc: 2.16484  (2.02807)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.16484  (2.02807)
     | > grad_norm_1: 11.68454  (16.50772)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.85380  (2.04625)
     | > loader_time: 0.02680  (0.05741)


[1m   --> STEP: 70/161 -- GLOBAL_STEP: 14650[0m
     | > loss_gen: 2.98731  (2.84492)
     | > loss_kl: 1.42430  (1.71478)
     | > loss_feat: 5.55265  (5.84631)
     | > loss_mel: 22.15562  (22.96787)
     | > loss_duration: 1.66092  (1.68739)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.78080  (35.06127)
     | > grad_norm_0: 129.43030  (107.07336)
     | > loss_disc: 1.99657  (2.03551)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.99657  (2.03551)
     | > grad_norm_1: 51.37118  (17.11112)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.65780  (2.03550)
     | > loader_time: 0.02570  (0.05375)


[1m   --> STEP: 95/161 -- GLOBAL_STEP: 14675[0m
     | > loss_gen: 2.70944  (2.85156)
     | > loss_kl: 1.76784  (1.72078)
     | > loss_feat: 6.45121  (5.88280)
     | > loss_mel: 23.08853  (22.97648)
     | > loss_duration: 1.69929  (1.68517)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.71632  (35.11680)
     | > grad_norm_0: 86.60959  (107.12741)
     | > loss_disc: 2.08916  (2.03056)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.08916  (2.03056)
     | > grad_norm_1: 7.39329  (16.91557)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.43700  (2.06834)
     | > loader_time: 0.03950  (0.05410)


[1m   --> STEP: 120/161 -- GLOBAL_STEP: 14700[0m
     | > loss_gen: 2.65526  (2.84317)
     | > loss_kl: 1.76874  (1.71941)
     | > loss_feat: 5.41218  (5.85680)
     | > loss_mel: 22.84316  (22.98649)
     | > loss_duration: 1.72240  (1.68809)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.40174  (35.09396)
     | > grad_norm_0: 202.96211  (114.24623)
     | > loss_disc: 2.14575  (2.03448)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.14575  (2.03448)
     | > grad_norm_1: 44.68786  (17.66682)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.30050  (2.09333)
     | > loader_time: 0.03930  (0.05282)


[1m   --> STEP: 145/161 -- GLOBAL_STEP: 14725[0m
     | > loss_gen: 2.75105  (2.85295)
     | > loss_kl: 1.48781  (1.71635)
     | > loss_feat: 6.36594  (5.88852)
     | > loss_mel: 24.19104  (23.02454)
     | > loss_duration: 1.66707  (1.68577)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.46291  (35.16814)
     | > grad_norm_0: 112.60690  (115.80279)
     | > loss_disc: 2.21534  (2.03654)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.21534  (2.03654)
     | > grad_norm_1: 25.49899  (18.86361)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.57560  (2.11436)
     | > loader_time: 0.04120  (0.05524)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02690 [0m(-0.00108)
     | > avg_loss_gen:[92m 2.71230 [0m(-0.13620)
     | > avg_loss_kl:[91m 1.88498 [0m(+0.21021)
     | > avg_loss_feat:[92m 4.94025 [0m(-0.80575)
     | > avg_loss_mel:[92m 23.17367 [0m(-1.17098)
     | > avg_loss_duration:[91m 1.67061 [0m(+0.00168)
     | > avg_loss_0:[92m 34.38181 [0m(-1.90105)
     | > avg_loss_disc:[91m 2.41829 [0m(+0.31815)
     | > avg_loss_1:[91m 2.41829 [0m(+0.31815)


[4m[1m > EPOCH: 91/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 01:32:20) [0m

[1m   --> STEP: 8/161 -- GLOBAL_STEP: 14750[0m
     | > loss_gen: 2.71836  (2.57353)
     | > loss_kl: 1.63631  (1.77174)
     | > loss_feat: 5.39818  (5.00229)
     | > loss_mel: 22.74877  (22.60149)
     | > loss_duration: 1.70592  (1.68513)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.20753  (33.63418)
     | > grad_norm_0: 60.02431  (96.25892)
     | > loss_disc: 2.10323  (2.26431)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.10323  (2.26431)
     | > grad_norm_1: 11.52365  (19.08258)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.07990  (2.12785)
     | > loader_time: 0.03160  (0.06919)


[1m   --> STEP: 33/161 -- GLOBAL_STEP: 14775[0m
     | > loss_gen: 2.65897  (2.71172)
     | > loss_kl: 1.64373  (1.73685)
     | > loss_feat: 5.84084  (5.42120)
     | > loss_mel: 22.76052  (22.86692)
     | > loss_duration: 1.64221  (1.68497)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.54626  (34.42165)
     | > grad_norm_0: 220.60065  (97.05900)
     | > loss_disc: 2.21492  (2.14691)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.21492  (2.14691)
     | > grad_norm_1: 46.23695  (15.45784)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.95250  (2.11966)
     | > loader_time: 0.03470  (0.05507)


[1m   --> STEP: 58/161 -- GLOBAL_STEP: 14800[0m
     | > loss_gen: 2.58171  (2.76538)
     | > loss_kl: 1.85479  (1.72177)
     | > loss_feat: 5.44805  (5.62382)
     | > loss_mel: 21.45168  (22.80780)
     | > loss_duration: 1.71767  (1.67702)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.05389  (34.59580)
     | > grad_norm_0: 58.01452  (118.97589)
     | > loss_disc: 2.16284  (2.09680)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.16284  (2.09680)
     | > grad_norm_1: 17.20934  (15.78382)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.88860  (2.08676)
     | > loader_time: 0.03370  (0.05207)


[1m   --> STEP: 83/161 -- GLOBAL_STEP: 14825[0m
     | > loss_gen: 2.39358  (2.77187)
     | > loss_kl: 1.95535  (1.72273)
     | > loss_feat: 5.65459  (5.62601)
     | > loss_mel: 24.69205  (22.80000)
     | > loss_duration: 1.77402  (1.67744)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.46959  (34.59805)
     | > grad_norm_0: 182.17505  (117.34332)
     | > loss_disc: 2.06637  (2.08652)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06637  (2.08652)
     | > grad_norm_1: 69.38929  (15.80181)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.40230  (2.06681)
     | > loader_time: 0.03920  (0.05467)


[1m   --> STEP: 108/161 -- GLOBAL_STEP: 14850[0m
     | > loss_gen: 2.57974  (2.77800)
     | > loss_kl: 1.65972  (1.72309)
     | > loss_feat: 5.56707  (5.65862)
     | > loss_mel: 22.12125  (22.82778)
     | > loss_duration: 1.63240  (1.67979)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.56018  (34.66727)
     | > grad_norm_0: 489.44308  (124.80361)
     | > loss_disc: 2.16356  (2.07976)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.16356  (2.07976)
     | > grad_norm_1: 32.97255  (16.65524)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.38800  (2.08140)
     | > loader_time: 0.04030  (0.05481)


[1m   --> STEP: 133/161 -- GLOBAL_STEP: 14875[0m
     | > loss_gen: 2.28714  (2.78625)
     | > loss_kl: 1.56750  (1.71685)
     | > loss_feat: 4.84874  (5.68710)
     | > loss_mel: 21.81448  (22.87811)
     | > loss_duration: 1.76314  (1.67977)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 32.28101  (34.74808)
     | > grad_norm_0: 234.41969  (127.81661)
     | > loss_disc: 2.25109  (2.07520)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.25109  (2.07520)
     | > grad_norm_1: 13.62567  (16.48683)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.77590  (2.12440)
     | > loader_time: 0.03050  (0.05467)


[1m   --> STEP: 158/161 -- GLOBAL_STEP: 14900[0m
     | > loss_gen: 2.70744  (2.79088)
     | > loss_kl: 1.83634  (1.72156)
     | > loss_feat: 6.13693  (5.71090)
     | > loss_mel: 22.62735  (22.91535)
     | > loss_duration: 1.65223  (1.67965)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.96029  (34.81834)
     | > grad_norm_0: 114.54035  (122.33844)
     | > loss_disc: 2.11415  (2.07233)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.11415  (2.07233)
     | > grad_norm_1: 15.30508  (17.34293)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.25500  (2.12442)
     | > loader_time: 0.03890  (0.05269)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02741 [0m(+0.00051)
     | > avg_loss_gen:[91m 2.88582 [0m(+0.17352)
     | > avg_loss_kl:[92m 1.80935 [0m(-0.07563)
     | > avg_loss_feat:[91m 6.16648 [0m(+1.22623)
     | > avg_loss_mel:[91m 24.08998 [0m(+0.91631)
     | > avg_loss_duration:[91m 1.67742 [0m(+0.00680)
     | > avg_loss_0:[91m 36.62906 [0m(+2.24725)
     | > avg_loss_disc:[92m 1.94591 [0m(-0.47238)
     | > avg_loss_1:[92m 1.94591 [0m(-0.47238)


[4m[1m > EPOCH: 92/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 01:38:28) [0m

[1m   --> STEP: 21/161 -- GLOBAL_STEP: 14925[0m
     | > loss_gen: 2.69546  (2.76709)
     | > loss_kl: 1.73300  (1.71261)
     | > loss_feat: 5.53350  (5.72524)
     | > loss_mel: 24.19240  (23.31434)
     | > loss_duration: 1.66780  (1.67545)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.82215  (35.19472)
     | > grad_norm_0: 134.86554  (144.34389)
     | > loss_disc: 2.03731  (2.06440)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.03731  (2.06440)
     | > grad_norm_1: 19.81949  (19.26695)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.25810  (2.00820)
     | > loader_time: 0.09490  (0.05081)


[1m   --> STEP: 46/161 -- GLOBAL_STEP: 14950[0m
     | > loss_gen: 3.16323  (2.78419)
     | > loss_kl: 1.85787  (1.74251)
     | > loss_feat: 5.55267  (5.72791)
     | > loss_mel: 22.66664  (23.20388)
     | > loss_duration: 1.64732  (1.67712)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.88774  (35.13561)
     | > grad_norm_0: 55.12406  (136.64264)
     | > loss_disc: 2.06189  (2.07188)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06189  (2.07188)
     | > grad_norm_1: 35.92136  (18.68794)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.34200  (2.06682)
     | > loader_time: 0.03610  (0.05234)


[1m   --> STEP: 71/161 -- GLOBAL_STEP: 14975[0m
     | > loss_gen: 2.71419  (2.78320)
     | > loss_kl: 1.86336  (1.74060)
     | > loss_feat: 6.00297  (5.73370)
     | > loss_mel: 23.35859  (23.06395)
     | > loss_duration: 1.70663  (1.67483)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.64574  (34.99628)
     | > grad_norm_0: 74.67200  (140.53468)
     | > loss_disc: 2.03484  (2.07266)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.03484  (2.07266)
     | > grad_norm_1: 16.64074  (20.81610)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.35870  (2.05360)
     | > loader_time: 0.03330  (0.04652)


[1m   --> STEP: 96/161 -- GLOBAL_STEP: 15000[0m
     | > loss_gen: 2.76035  (2.79531)
     | > loss_kl: 1.85035  (1.74244)
     | > loss_feat: 5.29405  (5.77039)
     | > loss_mel: 22.25404  (22.94857)
     | > loss_duration: 1.68213  (1.67633)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.84091  (34.93304)
     | > grad_norm_0: 333.91125  (145.07549)
     | > loss_disc: 2.02979  (2.07129)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.02979  (2.07129)
     | > grad_norm_1: 15.64203  (20.27313)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.51940  (2.07096)
     | > loader_time: 0.04350  (0.04954)


[1m   --> STEP: 121/161 -- GLOBAL_STEP: 15025[0m
     | > loss_gen: 3.12319  (2.80325)
     | > loss_kl: 1.80490  (1.74000)
     | > loss_feat: 5.35579  (5.77298)
     | > loss_mel: 23.27924  (22.96845)
     | > loss_duration: 1.68305  (1.67814)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.24617  (34.96283)
     | > grad_norm_0: 211.01366  (144.26677)
     | > loss_disc: 2.12269  (2.06962)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.12269  (2.06962)
     | > grad_norm_1: 23.37187  (20.97797)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.87890  (2.09823)
     | > loader_time: 0.04430  (0.04865)


[1m   --> STEP: 146/161 -- GLOBAL_STEP: 15050[0m
     | > loss_gen: 2.76080  (2.79431)
     | > loss_kl: 1.65884  (1.73392)
     | > loss_feat: 5.12876  (5.74486)
     | > loss_mel: 22.64736  (22.96969)
     | > loss_duration: 1.65193  (1.67825)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.84769  (34.92103)
     | > grad_norm_0: 121.08774  (142.75294)
     | > loss_disc: 2.06103  (2.07206)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06103  (2.07206)
     | > grad_norm_1: 70.10191  (20.33980)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.58330  (2.11413)
     | > loader_time: 0.25140  (0.05107)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02764 [0m(+0.00023)
     | > avg_loss_gen:[91m 3.22755 [0m(+0.34173)
     | > avg_loss_kl:[92m 1.68157 [0m(-0.12778)
     | > avg_loss_feat:[91m 6.27924 [0m(+0.11275)
     | > avg_loss_mel:[92m 22.15296 [0m(-1.93703)
     | > avg_loss_duration:[92m 1.66832 [0m(-0.00910)
     | > avg_loss_0:[92m 35.00962 [0m(-1.61943)
     | > avg_loss_disc:[92m 1.91507 [0m(-0.03084)
     | > avg_loss_1:[92m 1.91507 [0m(-0.03084)


[4m[1m > EPOCH: 93/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 01:44:36) [0m

[1m   --> STEP: 9/161 -- GLOBAL_STEP: 15075[0m
     | > loss_gen: 2.78326  (2.86727)
     | > loss_kl: 1.78197  (1.74736)
     | > loss_feat: 5.92079  (5.93731)
     | > loss_mel: 24.02476  (22.82575)
     | > loss_duration: 1.65415  (1.66891)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.16492  (35.04660)
     | > grad_norm_0: 195.03499  (106.12243)
     | > loss_disc: 1.98194  (1.99138)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.98194  (1.99138)
     | > grad_norm_1: 13.84175  (13.12031)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.65890  (2.01994)
     | > loader_time: 0.02730  (0.03303)


[1m   --> STEP: 34/161 -- GLOBAL_STEP: 15100[0m
     | > loss_gen: 2.92201  (2.81872)
     | > loss_kl: 1.72323  (1.74695)
     | > loss_feat: 5.36337  (5.77942)
     | > loss_mel: 23.15649  (22.92058)
     | > loss_duration: 1.63641  (1.67461)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.80151  (34.94028)
     | > grad_norm_0: 267.57565  (156.12059)
     | > loss_disc: 1.98283  (2.04859)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.98283  (2.04859)
     | > grad_norm_1: 19.69063  (19.79309)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.00960  (2.09254)
     | > loader_time: 0.03280  (0.03475)


[1m   --> STEP: 59/161 -- GLOBAL_STEP: 15125[0m
     | > loss_gen: 2.97949  (2.82939)
     | > loss_kl: 1.76618  (1.74059)
     | > loss_feat: 6.33981  (5.83239)
     | > loss_mel: 22.90122  (22.78891)
     | > loss_duration: 1.69662  (1.67172)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.68333  (34.86300)
     | > grad_norm_0: 153.64955  (151.69264)
     | > loss_disc: 1.92557  (2.04044)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.92557  (2.04044)
     | > grad_norm_1: 8.73759  (18.11286)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.81240  (2.07364)
     | > loader_time: 0.02910  (0.03987)


[1m   --> STEP: 84/161 -- GLOBAL_STEP: 15150[0m
     | > loss_gen: 2.97067  (2.83158)
     | > loss_kl: 1.72898  (1.73985)
     | > loss_feat: 6.03831  (5.84385)
     | > loss_mel: 23.32143  (22.86693)
     | > loss_duration: 1.72178  (1.67057)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.78118  (34.95278)
     | > grad_norm_0: 43.86143  (159.72675)
     | > loss_disc: 1.91744  (2.03763)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.91744  (2.03763)
     | > grad_norm_1: 5.42489  (19.77774)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.96620  (2.06249)
     | > loader_time: 0.03550  (0.04542)


[1m   --> STEP: 109/161 -- GLOBAL_STEP: 15175[0m
     | > loss_gen: 2.89225  (2.82403)
     | > loss_kl: 1.66151  (1.74447)
     | > loss_feat: 6.11275  (5.81697)
     | > loss_mel: 24.32419  (22.92002)
     | > loss_duration: 1.73997  (1.67500)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.73068  (34.98049)
     | > grad_norm_0: 96.36831  (155.15564)
     | > loss_disc: 2.13364  (2.04896)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.13364  (2.04896)
     | > grad_norm_1: 19.81422  (19.97099)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.52750  (2.07438)
     | > loader_time: 0.04500  (0.04735)


[1m   --> STEP: 134/161 -- GLOBAL_STEP: 15200[0m
     | > loss_gen: 2.88209  (2.82175)
     | > loss_kl: 1.85970  (1.75042)
     | > loss_feat: 6.26208  (5.80723)
     | > loss_mel: 24.31680  (22.89825)
     | > loss_duration: 1.70918  (1.67757)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 37.02985  (34.95522)
     | > grad_norm_0: 200.41629  (149.26361)
     | > loss_disc: 2.02378  (2.04705)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.02378  (2.04705)
     | > grad_norm_1: 25.04858  (19.58931)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.66810  (2.12052)
     | > loader_time: 0.02580  (0.04752)


[1m   --> STEP: 159/161 -- GLOBAL_STEP: 15225[0m
     | > loss_gen: 2.83144  (2.81467)
     | > loss_kl: 1.77184  (1.74582)
     | > loss_feat: 5.26891  (5.79513)
     | > loss_mel: 22.51704  (22.91613)
     | > loss_duration: 1.68289  (1.67898)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.07211  (34.95074)
     | > grad_norm_0: 124.07709  (145.85963)
     | > loss_disc: 2.15092  (2.05675)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.15092  (2.05675)
     | > grad_norm_1: 17.05620  (21.49434)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.90510  (2.11461)
     | > loader_time: 0.03520  (0.04629)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02734 [0m(-0.00030)
     | > avg_loss_gen:[92m 2.97281 [0m(-0.25474)
     | > avg_loss_kl:[91m 1.77342 [0m(+0.09185)
     | > avg_loss_feat:[92m 5.37660 [0m(-0.90264)
     | > avg_loss_mel:[91m 22.26938 [0m(+0.11643)
     | > avg_loss_duration:[92m 1.66159 [0m(-0.00673)
     | > avg_loss_0:[92m 34.05380 [0m(-0.95583)
     | > avg_loss_disc:[91m 2.08317 [0m(+0.16810)
     | > avg_loss_1:[91m 2.08317 [0m(+0.16810)


[4m[1m > EPOCH: 94/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 01:50:40) [0m

[1m   --> STEP: 22/161 -- GLOBAL_STEP: 15250[0m
     | > loss_gen: 2.87791  (2.79208)
     | > loss_kl: 1.77686  (1.70928)
     | > loss_feat: 6.07016  (5.81852)
     | > loss_mel: 24.15155  (23.37845)
     | > loss_duration: 1.64566  (1.67943)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.52215  (35.37776)
     | > grad_norm_0: 117.55799  (124.01881)
     | > loss_disc: 1.98467  (2.06210)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.98467  (2.06210)
     | > grad_norm_1: 17.78225  (17.21871)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.91850  (2.05258)
     | > loader_time: 0.03130  (0.04563)


[1m   --> STEP: 47/161 -- GLOBAL_STEP: 15275[0m
     | > loss_gen: 2.79073  (2.79452)
     | > loss_kl: 1.75186  (1.73231)
     | > loss_feat: 5.72163  (5.76341)
     | > loss_mel: 22.97178  (23.00134)
     | > loss_duration: 1.65288  (1.68472)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.88889  (34.97630)
     | > grad_norm_0: 122.77836  (125.67751)
     | > loss_disc: 1.97251  (2.06394)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.97251  (2.06394)
     | > grad_norm_1: 13.27519  (19.96341)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.98220  (2.08634)
     | > loader_time: 0.03090  (0.04784)


[1m   --> STEP: 72/161 -- GLOBAL_STEP: 15300[0m
     | > loss_gen: 2.83162  (2.79871)
     | > loss_kl: 1.65778  (1.74710)
     | > loss_feat: 5.21389  (5.77014)
     | > loss_mel: 20.94003  (22.87999)
     | > loss_duration: 1.64029  (1.68239)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 32.28362  (34.87834)
     | > grad_norm_0: 208.59282  (142.07893)
     | > loss_disc: 2.11562  (2.05700)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.11562  (2.05700)
     | > grad_norm_1: 9.96935  (19.97658)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.04430  (2.05111)
     | > loader_time: 0.03350  (0.05172)


[1m   --> STEP: 97/161 -- GLOBAL_STEP: 15325[0m
     | > loss_gen: 2.65676  (2.80529)
     | > loss_kl: 1.71290  (1.74896)
     | > loss_feat: 5.45596  (5.77611)
     | > loss_mel: 22.57074  (22.87142)
     | > loss_duration: 1.66297  (1.68585)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.05934  (34.88763)
     | > grad_norm_0: 117.45750  (139.40610)
     | > loss_disc: 2.18519  (2.06184)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.18519  (2.06184)
     | > grad_norm_1: 10.20343  (22.39701)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.41080  (2.08166)
     | > loader_time: 0.15700  (0.05352)


[1m   --> STEP: 122/161 -- GLOBAL_STEP: 15350[0m
     | > loss_gen: 2.77973  (2.80936)
     | > loss_kl: 1.67040  (1.74017)
     | > loss_feat: 6.40174  (5.79185)
     | > loss_mel: 22.88424  (22.85548)
     | > loss_duration: 1.57465  (1.68124)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.31076  (34.87810)
     | > grad_norm_0: 81.71681  (138.33377)
     | > loss_disc: 1.94364  (2.05900)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.94364  (2.05900)
     | > grad_norm_1: 10.50475  (21.74522)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 3.06960  (2.12077)
     | > loader_time: 0.06770  (0.05184)


[1m   --> STEP: 147/161 -- GLOBAL_STEP: 15375[0m
     | > loss_gen: 2.94617  (2.81501)
     | > loss_kl: 1.59340  (1.73737)
     | > loss_feat: 5.79595  (5.81563)
     | > loss_mel: 22.35605  (22.87845)
     | > loss_duration: 1.68786  (1.68110)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.37943  (34.92755)
     | > grad_norm_0: 175.79822  (133.14856)
     | > loss_disc: 2.14661  (2.05783)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.14661  (2.05783)
     | > grad_norm_1: 44.84060  (21.14785)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.38600  (2.13546)
     | > loader_time: 0.03970  (0.05125)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02799 [0m(+0.00065)
     | > avg_loss_gen:[92m 2.83396 [0m(-0.13884)
     | > avg_loss_kl:[92m 1.67665 [0m(-0.09677)
     | > avg_loss_feat:[91m 5.40043 [0m(+0.02383)
     | > avg_loss_mel:[92m 21.72157 [0m(-0.54781)
     | > avg_loss_duration:[91m 1.66468 [0m(+0.00309)
     | > avg_loss_0:[92m 33.29729 [0m(-0.75650)
     | > avg_loss_disc:[91m 2.11871 [0m(+0.03554)
     | > avg_loss_1:[91m 2.11871 [0m(+0.03554)


[4m[1m > EPOCH: 95/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 01:56:48) [0m

[1m   --> STEP: 10/161 -- GLOBAL_STEP: 15400[0m
     | > loss_gen: 2.60176  (2.84717)
     | > loss_kl: 1.56963  (1.68472)
     | > loss_feat: 5.74866  (5.89667)
     | > loss_mel: 22.48892  (22.73442)
     | > loss_duration: 1.63859  (1.70002)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.04756  (34.86301)
     | > grad_norm_0: 43.40412  (137.19054)
     | > loss_disc: 2.16220  (2.06548)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.16220  (2.06548)
     | > grad_norm_1: 10.88903  (26.11349)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.15980  (1.97694)
     | > loader_time: 0.11820  (0.05480)


[1m   --> STEP: 35/161 -- GLOBAL_STEP: 15425[0m
     | > loss_gen: 3.03211  (2.81706)
     | > loss_kl: 1.74015  (1.70091)
     | > loss_feat: 5.54387  (5.80908)
     | > loss_mel: 22.90727  (23.19352)
     | > loss_duration: 1.69494  (1.68862)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.91835  (35.20919)
     | > grad_norm_0: 159.25323  (127.52909)
     | > loss_disc: 2.14369  (2.07186)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.14369  (2.07186)
     | > grad_norm_1: 11.85826  (27.78291)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.62640  (2.06083)
     | > loader_time: 0.04410  (0.06229)


[1m   --> STEP: 60/161 -- GLOBAL_STEP: 15450[0m
     | > loss_gen: 2.90247  (2.79396)
     | > loss_kl: 1.69604  (1.71692)
     | > loss_feat: 5.55640  (5.75023)
     | > loss_mel: 21.77416  (22.97309)
     | > loss_duration: 1.71595  (1.68285)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.64500  (34.91705)
     | > grad_norm_0: 53.06157  (115.44275)
     | > loss_disc: 2.03487  (2.08015)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.03487  (2.08015)
     | > grad_norm_1: 12.53490  (22.65244)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.99520  (2.05446)
     | > loader_time: 0.03090  (0.05234)


[1m   --> STEP: 85/161 -- GLOBAL_STEP: 15475[0m
     | > loss_gen: 3.27747  (2.80870)
     | > loss_kl: 1.66181  (1.71711)
     | > loss_feat: 7.16441  (5.82076)
     | > loss_mel: 24.48524  (23.00833)
     | > loss_duration: 1.70535  (1.68115)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 38.29428  (35.03605)
     | > grad_norm_0: 116.19541  (123.64534)
     | > loss_disc: 1.79681  (2.06828)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.79681  (2.06828)
     | > grad_norm_1: 9.89726  (21.27638)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.25680  (2.04013)
     | > loader_time: 0.03890  (0.05192)


[1m   --> STEP: 110/161 -- GLOBAL_STEP: 15500[0m
     | > loss_gen: 2.79082  (2.83822)
     | > loss_kl: 1.82156  (1.71259)
     | > loss_feat: 5.48576  (5.90293)
     | > loss_mel: 22.35573  (23.00584)
     | > loss_duration: 1.62553  (1.68256)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.07940  (35.14214)
     | > grad_norm_0: 48.66003  (128.00259)
     | > loss_disc: 2.48120  (2.07037)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.48120  (2.07037)
     | > grad_norm_1: 30.64001  (20.06021)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.85010  (2.05505)
     | > loader_time: 0.03690  (0.05322)


[1m   --> STEP: 135/161 -- GLOBAL_STEP: 15525[0m
     | > loss_gen: 2.93639  (2.82664)
     | > loss_kl: 1.68185  (1.71549)
     | > loss_feat: 5.25349  (5.84602)
     | > loss_mel: 22.87319  (22.98224)
     | > loss_duration: 1.66590  (1.68354)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.41082  (35.05393)
     | > grad_norm_0: 182.61522  (123.13313)
     | > loss_disc: 2.27656  (2.10528)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.27656  (2.10528)
     | > grad_norm_1: 37.86788  (20.01924)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.93700  (2.09442)
     | > loader_time: 0.03260  (0.05465)


[1m   --> STEP: 160/161 -- GLOBAL_STEP: 15550[0m
     | > loss_gen: 2.71943  (2.81310)
     | > loss_kl: 1.73586  (1.71782)
     | > loss_feat: 6.13112  (5.79170)
     | > loss_mel: 22.44750  (22.95044)
     | > loss_duration: 1.72991  (1.68263)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.76381  (34.95569)
     | > grad_norm_0: 232.86217  (126.70809)
     | > loss_disc: 2.02891  (2.10306)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.02891  (2.10306)
     | > grad_norm_1: 11.32453  (19.51790)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.22220  (2.09536)
     | > loader_time: 0.04000  (0.05548)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02834 [0m(+0.00035)
     | > avg_loss_gen:[92m 2.79126 [0m(-0.04270)
     | > avg_loss_kl:[91m 1.82834 [0m(+0.15169)
     | > avg_loss_feat:[91m 5.92012 [0m(+0.51969)
     | > avg_loss_mel:[91m 23.53574 [0m(+1.81417)
     | > avg_loss_duration:[92m 1.66207 [0m(-0.00261)
     | > avg_loss_0:[91m 35.73753 [0m(+2.44023)
     | > avg_loss_disc:[92m 2.08941 [0m(-0.02931)
     | > avg_loss_1:[92m 2.08941 [0m(-0.02931)


[4m[1m > EPOCH: 96/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 02:02:54) [0m

[1m   --> STEP: 23/161 -- GLOBAL_STEP: 15575[0m
     | > loss_gen: 3.21058  (2.82367)
     | > loss_kl: 1.88608  (1.72561)
     | > loss_feat: 5.74483  (5.78206)
     | > loss_mel: 22.49046  (22.75964)
     | > loss_duration: 1.57639  (1.67096)
     | > amp_scaler: 512.00000  (367.30435)
     | > loss_0: 34.90835  (34.76194)
     | > grad_norm_0: 182.93391  (135.63120)
     | > loss_disc: 2.06664  (2.04228)
     | > amp_scaler-1: 512.00000  (367.30435)
     | > loss_1: 2.06664  (2.04228)
     | > grad_norm_1: 19.10190  (18.20095)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.12850  (2.07975)
     | > loader_time: 0.03320  (0.04265)


[1m   --> STEP: 48/161 -- GLOBAL_STEP: 15600[0m
     | > loss_gen: 2.63097  (2.80268)
     | > loss_kl: 1.93732  (1.73471)
     | > loss_feat: 5.91952  (5.74120)
     | > loss_mel: 23.71003  (22.78158)
     | > loss_duration: 1.68279  (1.67126)
     | > amp_scaler: 512.00000  (442.66667)
     | > loss_0: 35.88063  (34.73144)
     | > grad_norm_0: 191.77551  (150.26315)
     | > loss_disc: 2.03746  (2.04144)
     | > amp_scaler-1: 512.00000  (442.66667)
     | > loss_1: 2.03746  (2.04144)
     | > grad_norm_1: 11.36758  (19.53140)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.73130  (2.09349)
     | > loader_time: 0.02920  (0.04099)


[1m   --> STEP: 73/161 -- GLOBAL_STEP: 15625[0m
     | > loss_gen: 2.81953  (2.79001)
     | > loss_kl: 1.72785  (1.72638)
     | > loss_feat: 6.50579  (5.71492)
     | > loss_mel: 22.51075  (22.72425)
     | > loss_duration: 1.62541  (1.67377)
     | > amp_scaler: 256.00000  (427.83562)
     | > loss_0: 35.18933  (34.62933)
     | > grad_norm_0: 46.26944  (150.01645)
     | > loss_disc: 2.01607  (2.05693)
     | > amp_scaler-1: 256.00000  (427.83562)
     | > loss_1: 2.01607  (2.05693)
     | > grad_norm_1: 11.01567  (23.73240)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.23120  (2.06831)
     | > loader_time: 0.04490  (0.04324)


[1m   --> STEP: 98/161 -- GLOBAL_STEP: 15650[0m
     | > loss_gen: 2.92766  (2.79004)
     | > loss_kl: 1.85355  (1.72288)
     | > loss_feat: 5.13532  (5.71675)
     | > loss_mel: 23.51037  (22.71407)
     | > loss_duration: 1.62233  (1.67569)
     | > amp_scaler: 256.00000  (384.00000)
     | > loss_0: 35.04923  (34.61943)
     | > grad_norm_0: 52.82421  (143.33698)
     | > loss_disc: 2.03864  (2.06057)
     | > amp_scaler-1: 256.00000  (384.00000)
     | > loss_1: 2.03864  (2.06057)
     | > grad_norm_1: 19.28245  (23.85515)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.91460  (2.07621)
     | > loader_time: 0.03440  (0.04302)


[1m   --> STEP: 123/161 -- GLOBAL_STEP: 15675[0m
     | > loss_gen: 2.81231  (2.78601)
     | > loss_kl: 1.83709  (1.72138)
     | > loss_feat: 5.94790  (5.70955)
     | > loss_mel: 21.89501  (22.72853)
     | > loss_duration: 1.67505  (1.67773)
     | > amp_scaler: 256.00000  (357.98374)
     | > loss_0: 34.16736  (34.62320)
     | > grad_norm_0: 34.57310  (142.14566)
     | > loss_disc: 1.97074  (2.06201)
     | > amp_scaler-1: 256.00000  (357.98374)
     | > loss_1: 1.97074  (2.06201)
     | > grad_norm_1: 10.91909  (23.14484)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.92460  (2.11331)
     | > loader_time: 0.06930  (0.04612)


[1m   --> STEP: 148/161 -- GLOBAL_STEP: 15700[0m
     | > loss_gen: 2.76906  (2.79382)
     | > loss_kl: 1.71708  (1.71385)
     | > loss_feat: 5.67505  (5.73433)
     | > loss_mel: 22.05054  (22.73345)
     | > loss_duration: 1.63090  (1.67766)
     | > amp_scaler: 256.00000  (340.75676)
     | > loss_0: 33.84264  (34.65310)
     | > grad_norm_0: 53.37872  (141.80721)
     | > loss_disc: 2.14915  (2.05901)
     | > amp_scaler-1: 256.00000  (340.75676)
     | > loss_1: 2.14915  (2.05901)
     | > grad_norm_1: 11.02468  (22.46498)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.33050  (2.11919)
     | > loader_time: 0.04030  (0.04692)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02713 [0m(-0.00121)
     | > avg_loss_gen:[91m 2.91371 [0m(+0.12245)
     | > avg_loss_kl:[91m 1.90155 [0m(+0.07321)
     | > avg_loss_feat:[91m 6.19358 [0m(+0.27346)
     | > avg_loss_mel:[92m 22.91010 [0m(-0.62563)
     | > avg_loss_duration:[92m 1.66081 [0m(-0.00126)
     | > avg_loss_0:[92m 35.57976 [0m(-0.15777)
     | > avg_loss_disc:[92m 1.84954 [0m(-0.23986)
     | > avg_loss_1:[92m 1.84954 [0m(-0.23986)


[4m[1m > EPOCH: 97/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 02:09:02) [0m

[1m   --> STEP: 11/161 -- GLOBAL_STEP: 15725[0m
     | > loss_gen: 2.71144  (2.82320)
     | > loss_kl: 1.84287  (1.76890)
     | > loss_feat: 5.08234  (5.67231)
     | > loss_mel: 20.95493  (22.45253)
     | > loss_duration: 1.71073  (1.67744)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 32.30231  (34.39437)
     | > grad_norm_0: 197.27164  (191.97937)
     | > loss_disc: 2.05300  (2.02422)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.05300  (2.02422)
     | > grad_norm_1: 13.44482  (21.38512)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.77480  (1.95073)
     | > loader_time: 0.02460  (0.05540)


[1m   --> STEP: 36/161 -- GLOBAL_STEP: 15750[0m
     | > loss_gen: 2.65464  (2.84340)
     | > loss_kl: 1.76469  (1.72078)
     | > loss_feat: 5.97244  (5.88584)
     | > loss_mel: 22.22929  (22.96369)
     | > loss_duration: 1.66716  (1.67673)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.28822  (35.09043)
     | > grad_norm_0: 63.90695  (164.78020)
     | > loss_disc: 2.04471  (2.03292)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.04471  (2.03292)
     | > grad_norm_1: 17.35372  (16.82881)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.41280  (2.10407)
     | > loader_time: 0.04990  (0.04655)


[1m   --> STEP: 61/161 -- GLOBAL_STEP: 15775[0m
     | > loss_gen: 3.12441  (2.81570)
     | > loss_kl: 1.94666  (1.72833)
     | > loss_feat: 6.06881  (5.78302)
     | > loss_mel: 22.15586  (22.80338)
     | > loss_duration: 1.65823  (1.67617)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.95398  (34.80661)
     | > grad_norm_0: 135.32137  (157.00780)
     | > loss_disc: 1.89991  (2.05606)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.89991  (2.05606)
     | > grad_norm_1: 10.22018  (19.41413)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.61550  (2.07743)
     | > loader_time: 0.17330  (0.04391)


[1m   --> STEP: 86/161 -- GLOBAL_STEP: 15800[0m
     | > loss_gen: 2.95431  (2.82312)
     | > loss_kl: 1.69682  (1.74447)
     | > loss_feat: 5.76225  (5.83443)
     | > loss_mel: 23.07617  (22.81611)
     | > loss_duration: 1.71691  (1.67888)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.20645  (34.89700)
     | > grad_norm_0: 112.77026  (155.79465)
     | > loss_disc: 1.90423  (2.04127)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.90423  (2.04127)
     | > grad_norm_1: 22.12319  (18.32445)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.55430  (2.06447)
     | > loader_time: 0.02730  (0.04905)


[1m   --> STEP: 111/161 -- GLOBAL_STEP: 15825[0m
     | > loss_gen: 3.01205  (2.82388)
     | > loss_kl: 1.66428  (1.74183)
     | > loss_feat: 6.29063  (5.85704)
     | > loss_mel: 24.24219  (22.90849)
     | > loss_duration: 1.70944  (1.67792)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.91859  (35.00916)
     | > grad_norm_0: 97.87819  (154.39963)
     | > loss_disc: 2.14022  (2.04259)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.14022  (2.04259)
     | > grad_norm_1: 54.82950  (20.74731)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.42380  (2.08580)
     | > loader_time: 0.04110  (0.05140)


[1m   --> STEP: 136/161 -- GLOBAL_STEP: 15850[0m
     | > loss_gen: 2.63447  (2.81332)
     | > loss_kl: 1.74303  (1.73952)
     | > loss_feat: 5.60525  (5.80400)
     | > loss_mel: 22.77520  (22.86719)
     | > loss_duration: 1.68833  (1.67863)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.44627  (34.90266)
     | > grad_norm_0: 146.56075  (150.55856)
     | > loss_disc: 2.04210  (2.05237)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.04210  (2.05237)
     | > grad_norm_1: 28.77004  (22.19177)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.01620  (2.11891)
     | > loader_time: 0.03300  (0.05207)


[1m   --> STEP: 161/161 -- GLOBAL_STEP: 15875[0m
     | > loss_gen: 2.92195  (2.81135)
     | > loss_kl: 1.84322  (1.73504)
     | > loss_feat: 5.05839  (5.79604)
     | > loss_mel: 23.36509  (22.83749)
     | > loss_duration: 1.62638  (1.67936)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.81503  (34.85928)
     | > grad_norm_0: 335.60007  (149.59181)
     | > loss_disc: 2.12795  (2.05560)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.12795  (2.05560)
     | > grad_norm_1: 113.43626  (22.48430)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.22740  (2.12001)
     | > loader_time: 0.02640  (0.05433)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02785 [0m(+0.00072)
     | > avg_loss_gen:[91m 2.99579 [0m(+0.08207)
     | > avg_loss_kl:[92m 1.85756 [0m(-0.04399)
     | > avg_loss_feat:[91m 6.25156 [0m(+0.05798)
     | > avg_loss_mel:[92m 22.89396 [0m(-0.01614)
     | > avg_loss_duration:[91m 1.67532 [0m(+0.01451)
     | > avg_loss_0:[91m 35.67418 [0m(+0.09443)
     | > avg_loss_disc:[91m 1.97627 [0m(+0.12672)
     | > avg_loss_1:[91m 1.97627 [0m(+0.12672)


[4m[1m > EPOCH: 98/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 02:15:11) [0m

[1m   --> STEP: 24/161 -- GLOBAL_STEP: 15900[0m
     | > loss_gen: 2.60090  (2.73680)
     | > loss_kl: 1.86754  (1.76355)
     | > loss_feat: 4.93277  (5.45876)
     | > loss_mel: 23.05497  (22.55016)
     | > loss_duration: 1.73166  (1.66987)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.18784  (34.17914)
     | > grad_norm_0: 251.71681  (100.44824)
     | > loss_disc: 2.24102  (2.12799)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.24102  (2.12799)
     | > grad_norm_1: 26.64174  (28.06329)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.73910  (2.01852)
     | > loader_time: 0.02850  (0.04330)


[1m   --> STEP: 49/161 -- GLOBAL_STEP: 15925[0m
     | > loss_gen: 2.85590  (2.75372)
     | > loss_kl: 1.93991  (1.74509)
     | > loss_feat: 5.95834  (5.63235)
     | > loss_mel: 23.20093  (22.73915)
     | > loss_duration: 1.58011  (1.66979)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.53519  (34.54009)
     | > grad_norm_0: 179.45518  (99.32926)
     | > loss_disc: 1.99283  (2.09967)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.99283  (2.09967)
     | > grad_norm_1: 24.85641  (22.68325)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.26710  (2.07650)
     | > loader_time: 0.03560  (0.04316)


[1m   --> STEP: 74/161 -- GLOBAL_STEP: 15950[0m
     | > loss_gen: 2.58083  (2.75871)
     | > loss_kl: 1.65525  (1.74039)
     | > loss_feat: 5.66530  (5.62529)
     | > loss_mel: 23.38519  (22.76554)
     | > loss_duration: 1.68226  (1.67397)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.96882  (34.56391)
     | > grad_norm_0: 289.57974  (108.68762)
     | > loss_disc: 2.11216  (2.09275)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.11216  (2.09275)
     | > grad_norm_1: 27.48506  (22.45192)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.03460  (2.04108)
     | > loader_time: 0.14450  (0.04642)


[1m   --> STEP: 99/161 -- GLOBAL_STEP: 15975[0m
     | > loss_gen: 2.55374  (2.75408)
     | > loss_kl: 1.73947  (1.73806)
     | > loss_feat: 5.04593  (5.62338)
     | > loss_mel: 22.28455  (22.74075)
     | > loss_duration: 1.71572  (1.67961)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.33941  (34.53589)
     | > grad_norm_0: 79.95611  (108.43743)
     | > loss_disc: 2.16030  (2.09908)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.16030  (2.09908)
     | > grad_norm_1: 36.53317  (21.21940)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.69230  (2.05521)
     | > loader_time: 0.02830  (0.04685)


[1m   --> STEP: 124/161 -- GLOBAL_STEP: 16000[0m
     | > loss_gen: 2.55701  (2.75581)
     | > loss_kl: 1.83219  (1.72861)
     | > loss_feat: 5.53544  (5.63086)
     | > loss_mel: 23.81675  (22.69406)
     | > loss_duration: 1.58446  (1.67758)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.32585  (34.48692)
     | > grad_norm_0: 214.92604  (109.65466)
     | > loss_disc: 2.11640  (2.09574)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.11640  (2.09574)
     | > grad_norm_1: 75.16104  (21.48665)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.80320  (2.09503)
     | > loader_time: 0.03630  (0.04786)


[1m   --> STEP: 149/161 -- GLOBAL_STEP: 16025[0m
     | > loss_gen: 2.54323  (2.76280)
     | > loss_kl: 1.76646  (1.73292)
     | > loss_feat: 5.47472  (5.64775)
     | > loss_mel: 22.64410  (22.70995)
     | > loss_duration: 1.67611  (1.67792)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 34.10461  (34.53133)
     | > grad_norm_0: 100.07232  (115.24950)
     | > loss_disc: 2.01578  (2.08856)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.01578  (2.08856)
     | > grad_norm_1: 11.87796  (21.57705)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.02990  (2.10781)
     | > loader_time: 0.03360  (0.04828)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 0.02684 [0m(-0.00101)
     | > avg_loss_gen:[92m 2.79567 [0m(-0.20012)
     | > avg_loss_kl:[91m 1.86331 [0m(+0.00575)
     | > avg_loss_feat:[92m 4.73823 [0m(-1.51333)
     | > avg_loss_mel:[92m 21.26533 [0m(-1.62864)
     | > avg_loss_duration:[92m 1.66906 [0m(-0.00625)
     | > avg_loss_0:[92m 32.33159 [0m(-3.34259)
     | > avg_loss_disc:[91m 2.20798 [0m(+0.23172)
     | > avg_loss_1:[91m 2.20798 [0m(+0.23172)

 > BEST MODEL : /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000/best_model_16038.pth.tar

[4m[1m > EPOCH: 99/100[0m
 --> /home/jupyter/dev/vits_libriTTS-December-07-2021_04+12PM-0000000

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135091.08557844692
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 511
 | > Batch group size: 80.

[1m > TRAINING (2021-12-08 02:21:20) [0m

[1m   --> STEP: 12/161 -- GLOBAL_STEP: 16050[0m
     | > loss_gen: 2.70180  (2.78908)
     | > loss_kl: 1.65978  (1.70894)
     | > loss_feat: 5.50225  (5.80125)
     | > loss_mel: 21.85757  (22.49488)
     | > loss_duration: 1.69440  (1.68159)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.41581  (34.47574)
     | > grad_norm_0: 156.81013  (133.10605)
     | > loss_disc: 2.07591  (2.07821)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.07591  (2.07821)
     | > grad_norm_1: 11.61346  (20.97955)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.84250  (1.96649)
     | > loader_time: 0.02620  (0.03318)


[1m   --> STEP: 37/161 -- GLOBAL_STEP: 16075[0m
     | > loss_gen: 2.62946  (2.78228)
     | > loss_kl: 1.57623  (1.72229)
     | > loss_feat: 6.03187  (5.75993)
     | > loss_mel: 23.22415  (22.60248)
     | > loss_duration: 1.72457  (1.68133)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.18629  (34.54831)
     | > grad_norm_0: 65.04132  (126.28889)
     | > loss_disc: 2.09313  (2.07585)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.09313  (2.07585)
     | > grad_norm_1: 13.90651  (19.42727)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.70800  (2.09733)
     | > loader_time: 0.02920  (0.04389)


[1m   --> STEP: 62/161 -- GLOBAL_STEP: 16100[0m
     | > loss_gen: 3.20963  (2.80921)
     | > loss_kl: 1.81988  (1.74520)
     | > loss_feat: 5.47982  (5.78201)
     | > loss_mel: 23.89659  (22.58751)
     | > loss_duration: 1.67368  (1.67657)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 36.07959  (34.60050)
     | > grad_norm_0: 103.20790  (148.42287)
     | > loss_disc: 2.06442  (2.06002)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.06442  (2.06002)
     | > grad_norm_1: 18.14639  (22.06506)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 1.75970  (2.07572)
     | > loader_time: 0.02730  (0.04544)


[1m   --> STEP: 87/161 -- GLOBAL_STEP: 16125[0m
     | > loss_gen: 2.55818  (2.80084)
     | > loss_kl: 1.78071  (1.74844)
     | > loss_feat: 5.28648  (5.75043)
     | > loss_mel: 21.99693  (22.59319)
     | > loss_duration: 1.72130  (1.68145)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 33.34361  (34.57436)
     | > grad_norm_0: 38.50955  (142.12158)
     | > loss_disc: 2.13225  (2.07238)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.13225  (2.07238)
     | > grad_norm_1: 11.53295  (21.74339)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.44460  (2.07003)
     | > loader_time: 0.03470  (0.04590)


[1m   --> STEP: 112/161 -- GLOBAL_STEP: 16150[0m
     | > loss_gen: 2.90968  (2.80306)
     | > loss_kl: 1.94541  (1.74631)
     | > loss_feat: 5.79522  (5.76837)
     | > loss_mel: 23.43626  (22.63588)
     | > loss_duration: 1.72474  (1.68113)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.81132  (34.63475)
     | > grad_norm_0: 257.14297  (135.76639)
     | > loss_disc: 2.07375  (2.06665)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 2.07375  (2.06665)
     | > grad_norm_1: 31.98532  (20.51246)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.08610  (2.09258)
     | > loader_time: 0.04000  (0.04798)


[1m   --> STEP: 137/161 -- GLOBAL_STEP: 16175[0m
     | > loss_gen: 3.04168  (2.81426)
     | > loss_kl: 1.69560  (1.74690)
     | > loss_feat: 6.14390  (5.82131)
     | > loss_mel: 22.89572  (22.66065)
     | > loss_duration: 1.74271  (1.68111)
     | > amp_scaler: 256.00000  (256.00000)
     | > loss_0: 35.51961  (34.72423)
     | > grad_norm_0: 479.56488  (144.64772)
     | > loss_disc: 1.96111  (2.05956)
     | > amp_scaler-1: 256.00000  (256.00000)
     | > loss_1: 1.96111  (2.05956)
     | > grad_norm_1: 33.26061  (20.08353)
     | > current_lr_0: 0.00020 
     | > current_lr_1: 0.00020 
     | > step_time: 2.50400  (2.12854)
     | > loader_time: 0.03820  (0.04836)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 481704.0
 | > Min length sequence: 19222.0
 | > Avg length sequence: 139824.15789473685
 | > Num. instances discarded by max-min (max=1500000, min=32768) seq limits: 5
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 0.02817 [0m(+0.00133)
     | > avg_loss_gen:[91m 2.94134 [0m(+0.14567)
     | > avg_loss_kl:[91m 1.90395 [0m(+0.04064)
     | > avg_loss_feat:[91m 6.00990 [0m(+1.27168)
     | > avg_loss_mel:[91m 22.24092 [0m(+0.97559)
     | > avg_loss_duration:[92m 1.65417 [0m(-0.01490)
     | > avg_loss_0:[91m 34.75027 [0m(+2.41868)
     | > avg_loss_disc:[92m 2.16441 [0m(-0.04357)
     | > avg_loss_1:[92m 2.16441 [0m(-0.04357)


 > Using CUDA:  True
 > Number of GPUs:  4
 > `speakers.json` is saved to /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf/speakers.json.
 > `speakers_file` is updated in the config.json.
 > Restoring from model_file.pth.tar ...
 > Restoring Model...
 > Partial model initialization...
 | > Layer missing in the model definition: encoder.convolutions.0.convolution1d.weight
 | > Layer missing in the model definition: encoder.convolutions.0.convolution1d.bias
 | > Layer missing in the model definition: encoder.convolutions.0.batch_normalization.weight
 | > Layer missing in the model definition: encoder.convolutions.0.batch_normalization.bias
 | > Layer missing in the model definition: encoder.convolutions.0.batch_normalization.running_mean
 | > Layer missing in the model definition: encoder.convolutions.0.batch_normalization.running_var
 | > Layer missing in the model definition: encoder.convolutions.0.batch_normalization.num_batches_tracked
 | > Layer missing in the model definition: encoder.convolutions.1.convolution1d.weight
 | > Layer missing in the model definition: encoder.convolutions.1.convolution1d.bias
 | > Layer missing in the model definition: encoder.convolutions.1.batch_normalization.weight
 | > Layer missing in the model definition: encoder.convolutions.1.batch_normalization.bias
 | > Layer missing in the model definition: encoder.convolutions.1.batch_normalization.running_mean
 | > Layer missing in the model definition: encoder.convolutions.1.batch_normalization.running_var
 | > Layer missing in the model definition: encoder.convolutions.1.batch_normalization.num_batches_tracked
 | > Layer missing in the model definition: encoder.convolutions.2.convolution1d.weight
 | > Layer missing in the model definition: encoder.convolutions.2.convolution1d.bias
 | > Layer missing in the model definition: encoder.convolutions.2.batch_normalization.weight
 | > Layer missing in the model definition: encoder.convolutions.2.batch_normalization.bias
 | > Layer missing in the model definition: encoder.convolutions.2.batch_normalization.running_mean
 | > Layer missing in the model definition: encoder.convolutions.2.batch_normalization.running_var
 | > Layer missing in the model definition: encoder.convolutions.2.batch_normalization.num_batches_tracked
 | > Layer missing in the model definition: encoder.lstm.weight_ih_l0
 | > Layer missing in the model definition: encoder.lstm.weight_hh_l0
 | > Layer missing in the model definition: encoder.lstm.bias_ih_l0
 | > Layer missing in the model definition: encoder.lstm.bias_hh_l0
 | > Layer missing in the model definition: encoder.lstm.weight_ih_l0_reverse
 | > Layer missing in the model definition: encoder.lstm.weight_hh_l0_reverse
 | > Layer missing in the model definition: encoder.lstm.bias_ih_l0_reverse
 | > Layer missing in the model definition: encoder.lstm.bias_hh_l0_reverse
 | > Layer missing in the model definition: decoder.attention.prior
 | > Layer missing in the model definition: decoder.attention.query_layer.weight
 | > Layer missing in the model definition: decoder.attention.query_layer.bias
 | > Layer missing in the model definition: decoder.attention.key_layer.weight
 | > Layer missing in the model definition: decoder.attention.static_filter_conv.weight
 | > Layer missing in the model definition: decoder.attention.static_filter_layer.weight
 | > Layer missing in the model definition: decoder.attention.dynamic_filter_layer.weight
 | > Layer missing in the model definition: decoder.attention.dynamic_filter_layer.bias
 | > Layer missing in the model definition: decoder.attention.v.weight
 | > Layer missing in the model definition: decoder.decoder_rnn.weight_ih
 | > Layer missing in the model definition: decoder.decoder_rnn.weight_hh
 | > Layer missing in the model definition: decoder.decoder_rnn.bias_ih
 | > Layer missing in the model definition: decoder.decoder_rnn.bias_hh
 | > Layer missing in the model definition: decoder.linear_projection.linear_layer.weight
 | > Layer missing in the model definition: decoder.linear_projection.linear_layer.bias
 | > Layer missing in the model definition: decoder.stopnet.1.linear_layer.weight
 | > Layer missing in the model definition: decoder.stopnet.1.linear_layer.bias
 | > Layer missing in the model definition: postnet.convolutions.0.convolution1d.weight
 | > Layer missing in the model definition: postnet.convolutions.0.convolution1d.bias
 | > Layer missing in the model definition: postnet.convolutions.0.batch_normalization.weight
 | > Layer missing in the model definition: postnet.convolutions.0.batch_normalization.bias
 | > Layer missing in the model definition: postnet.convolutions.0.batch_normalization.running_mean
 | > Layer missing in the model definition: postnet.convolutions.0.batch_normalization.running_var
 | > Layer missing in the model definition: postnet.convolutions.0.batch_normalization.num_batches_tracked
 | > Layer missing in the model definition: postnet.convolutions.1.convolution1d.weight
 | > Layer missing in the model definition: postnet.convolutions.1.convolution1d.bias
 | > Layer missing in the model definition: postnet.convolutions.1.batch_normalization.weight
 | > Layer missing in the model definition: postnet.convolutions.1.batch_normalization.bias
 | > Layer missing in the model definition: postnet.convolutions.1.batch_normalization.running_mean
 | > Layer missing in the model definition: postnet.convolutions.1.batch_normalization.running_var
 | > Layer missing in the model definition: postnet.convolutions.1.batch_normalization.num_batches_tracked
 | > Layer missing in the model definition: postnet.convolutions.2.convolution1d.weight
 | > Layer missing in the model definition: postnet.convolutions.2.convolution1d.bias
 | > Layer missing in the model definition: postnet.convolutions.2.batch_normalization.weight
 | > Layer missing in the model definition: postnet.convolutions.2.batch_normalization.bias
 | > Layer missing in the model definition: postnet.convolutions.2.batch_normalization.running_mean
 | > Layer missing in the model definition: postnet.convolutions.2.batch_normalization.running_var
 | > Layer missing in the model definition: postnet.convolutions.2.batch_normalization.num_batches_tracked
 | > Layer missing in the model definition: postnet.convolutions.3.convolution1d.weight
 | > Layer missing in the model definition: postnet.convolutions.3.convolution1d.bias
 | > Layer missing in the model definition: postnet.convolutions.3.batch_normalization.weight
 | > Layer missing in the model definition: postnet.convolutions.3.batch_normalization.bias
 | > Layer missing in the model definition: postnet.convolutions.3.batch_normalization.running_mean
 | > Layer missing in the model definition: postnet.convolutions.3.batch_normalization.running_var
 | > Layer missing in the model definition: postnet.convolutions.3.batch_normalization.num_batches_tracked
 | > Layer missing in the model definition: postnet.convolutions.4.convolution1d.weight
 | > Layer missing in the model definition: postnet.convolutions.4.convolution1d.bias
 | > Layer missing in the model definition: postnet.convolutions.4.batch_normalization.weight
 | > Layer missing in the model definition: postnet.convolutions.4.batch_normalization.bias
 | > Layer missing in the model definition: postnet.convolutions.4.batch_normalization.running_mean
 | > Layer missing in the model definition: postnet.convolutions.4.batch_normalization.running_var
 | > Layer missing in the model definition: postnet.convolutions.4.batch_normalization.num_batches_tracked
 | > 1 / 281 layers are restored.
 > Model restored from step 270000

 > Model has 9749029 parameters
 > Restoring best loss from  ...
 > Starting with loaded last best loss 0.09947767853736877.

[4m[1m > EPOCH: 0/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 09:38:16) [0m

[1m   --> STEP: 0/87 -- GLOBAL_STEP: 270000[0m
     | > decoder_loss: 33.73831  (33.73831)
     | > postnet_loss: 20.43425  (20.43425)
     | > stopnet_loss: 1.34109  (1.34109)
     | > decoder_coarse_loss: 33.84319  (33.84319)
     | > decoder_ddc_loss: 0.00050  (0.00050)
     | > ga_loss: 0.00278  (0.00278)
     | > decoder_diff_spec_loss: 0.42180  (0.42180)
     | > postnet_diff_spec_loss: 0.86314  (0.86314)
     | > decoder_ssim_loss: 0.54881  (0.54881)
     | > postnet_ssim_loss: 0.54878  (0.54878)
     | > loss: 23.95470  (23.95470)
     | > align_error: 0.99388  (0.99388)
     | > amp_scaler: 65536.00000  (65536.00000)
     | > grad_norm: 1.74323  (1.74323)
     | > current_lr: 2.5000000000000002e-08 
     | > step_time: 3.36840  (3.36842)
     | > loader_time: 10.92240  (10.92243)


[1m   --> STEP: 25/87 -- GLOBAL_STEP: 270025[0m
     | > decoder_loss: 35.64340  (34.07855)
     | > postnet_loss: 21.12361  (20.17520)
     | > stopnet_loss: 2.18696  (1.99958)
     | > decoder_coarse_loss: 35.77520  (34.19957)
     | > decoder_ddc_loss: 0.00050  (0.00055)
     | > ga_loss: 0.00247  (0.00268)
     | > decoder_diff_spec_loss: 0.46912  (0.43467)
     | > postnet_diff_spec_loss: 0.90188  (0.86783)
     | > decoder_ssim_loss: 0.32943  (0.38206)
     | > postnet_ssim_loss: 0.32941  (0.38203)
     | > loss: 25.84243  (24.64309)
     | > align_error: 0.99517  (0.99455)
     | > amp_scaler: 65536.00000  (65536.00000)
     | > grad_norm: 1.80264  (1.80942)
     | > current_lr: 6.5e-07 
     | > step_time: 5.76800  (4.32579)
     | > loader_time: 0.02210  (0.03844)


[1m   --> STEP: 50/87 -- GLOBAL_STEP: 270050[0m
     | > decoder_loss: 33.73965  (33.85475)
     | > postnet_loss: 19.93812  (20.16585)
     | > stopnet_loss: 1.88124  (1.96598)
     | > decoder_coarse_loss: 33.88492  (33.97415)
     | > decoder_ddc_loss: 0.00051  (0.00056)
     | > ga_loss: 0.00286  (0.00277)
     | > decoder_diff_spec_loss: 0.41294  (0.43674)
     | > postnet_diff_spec_loss: 0.85783  (0.87009)
     | > decoder_ssim_loss: 0.38930  (0.38841)
     | > postnet_ssim_loss: 0.38926  (0.38837)
     | > loss: 24.29869  (24.49959)
     | > align_error: 0.99405  (0.99437)
     | > amp_scaler: 65536.00000  (65536.00000)
     | > grad_norm: 1.76148  (1.79161)
     | > current_lr: 1.275e-06 
     | > step_time: 3.70060  (3.98645)
     | > loader_time: 0.01860  (0.03436)


[1m   --> STEP: 75/87 -- GLOBAL_STEP: 270075[0m
     | > decoder_loss: 34.61371  (33.83393)
     | > postnet_loss: 20.79226  (20.22866)
     | > stopnet_loss: 2.36783  (1.97580)
     | > decoder_coarse_loss: 34.70843  (33.95152)
     | > decoder_ddc_loss: 0.00049  (0.00054)
     | > ga_loss: 0.00248  (0.00268)
     | > decoder_diff_spec_loss: 0.45425  (0.43724)
     | > postnet_diff_spec_loss: 0.87945  (0.86894)
     | > decoder_ssim_loss: 0.30814  (0.38339)
     | > postnet_ssim_loss: 0.30811  (0.38335)
     | > loss: 25.39644  (24.51111)
     | > align_error: 0.99516  (0.99454)
     | > amp_scaler: 65536.00000  (65536.00000)
     | > grad_norm: 1.83625  (1.80011)
     | > current_lr: 1.9e-06 
     | > step_time: 4.16630  (3.99718)
     | > loader_time: 0.02140  (0.03458)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time: 4.42717 [0m(+0.00000)
     | > avg_decoder_loss: 29.67479 [0m(+0.00000)
     | > avg_postnet_loss: 16.50849 [0m(+0.00000)
     | > avg_stopnet_loss: 1.90838 [0m(+0.00000)
     | > avg_decoder_coarse_loss: 29.78556 [0m(+0.00000)
     | > avg_decoder_ddc_loss: 0.00034 [0m(+0.00000)
     | > avg_ga_loss: 0.00245 [0m(+0.00000)
     | > avg_decoder_diff_spec_loss: 0.39943 [0m(+0.00000)
     | > avg_postnet_diff_spec_loss: 0.85025 [0m(+0.00000)
     | > avg_decoder_ssim_loss: 0.39785 [0m(+0.00000)
     | > avg_postnet_ssim_loss: 0.39781 [0m(+0.00000)
     | > avg_loss: 21.42429 [0m(+0.00000)
     | > avg_align_error: 0.99505 [0m(+0.00000)


[4m[1m > EPOCH: 1/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 09:46:10) [0m

[1m   --> STEP: 12/87 -- GLOBAL_STEP: 270100[0m
     | > decoder_loss: 36.25113  (33.97071)
     | > postnet_loss: 22.22860  (19.91235)
     | > stopnet_loss: 2.52929  (1.94268)
     | > decoder_coarse_loss: 36.34045  (34.08232)
     | > decoder_ddc_loss: 0.00039  (0.00053)
     | > ga_loss: 0.00265  (0.00263)
     | > decoder_diff_spec_loss: 0.47810  (0.43284)
     | > postnet_diff_spec_loss: 0.88928  (0.86441)
     | > decoder_ssim_loss: 0.28705  (0.39335)
     | > postnet_ssim_loss: 0.28704  (0.39331)
     | > loss: 26.73304  (24.46826)
     | > align_error: 0.99573  (0.99464)
     | > amp_scaler: 65536.00000  (65536.00000)
     | > grad_norm: 1.91282  (1.80201)
     | > current_lr: 2.525e-06 
     | > step_time: 4.32210  (4.43554)
     | > loader_time: 0.02220  (0.04249)


[1m   --> STEP: 37/87 -- GLOBAL_STEP: 270125[0m
     | > decoder_loss: 33.28197  (33.91253)
     | > postnet_loss: 18.75049  (20.14780)
     | > stopnet_loss: 1.76210  (1.96714)
     | > decoder_coarse_loss: 33.39095  (34.03008)
     | > decoder_ddc_loss: 0.00042  (0.00055)
     | > ga_loss: 0.00274  (0.00278)
     | > decoder_diff_spec_loss: 0.44384  (0.43765)
     | > postnet_diff_spec_loss: 0.87106  (0.87062)
     | > decoder_ssim_loss: 0.40778  (0.38787)
     | > postnet_ssim_loss: 0.40775  (0.38783)
     | > loss: 23.66435  (24.52478)
     | > align_error: 0.99498  (0.99441)
     | > amp_scaler: 65536.00000  (65536.00000)
     | > grad_norm: 1.72293  (1.79786)
     | > current_lr: 3.1500000000000003e-06 
     | > step_time: 3.16570  (4.00465)
     | > loader_time: 0.01490  (0.03864)


[1m   --> STEP: 62/87 -- GLOBAL_STEP: 270150[0m
     | > decoder_loss: 31.69228  (33.71854)
     | > postnet_loss: 17.63675  (20.10871)
     | > stopnet_loss: 2.26199  (1.97115)
     | > decoder_coarse_loss: 31.82585  (33.83223)
     | > decoder_ddc_loss: 0.00050  (0.00054)
     | > ga_loss: 0.00239  (0.00273)
     | > decoder_diff_spec_loss: 0.40455  (0.43675)
     | > postnet_diff_spec_loss: 0.83084  (0.86923)
     | > decoder_ssim_loss: 0.32418  (0.38582)
     | > postnet_ssim_loss: 0.32416  (0.38578)
     | > loss: 23.03373  (24.41922)
     | > align_error: 0.99552  (0.99445)
     | > amp_scaler: 65536.00000  (65536.00000)
     | > grad_norm: 1.81881  (1.80658)
     | > current_lr: 3.7750000000000003e-06 
     | > step_time: 5.22810  (4.03564)
     | > loader_time: 0.05020  (0.03923)


[1m   --> STEP: 87/87 -- GLOBAL_STEP: 270175[0m
     | > decoder_loss: 32.39130  (33.73423)
     | > postnet_loss: 16.96647  (20.17276)
     | > stopnet_loss: 1.41477  (1.98281)
     | > decoder_coarse_loss: 32.49330  (33.84661)
     | > decoder_ddc_loss: 0.00037  (0.00051)
     | > ga_loss: 0.00192  (0.00264)
     | > decoder_diff_spec_loss: 0.43464  (0.43860)
     | > postnet_diff_spec_loss: 0.88321  (0.87020)
     | > decoder_ssim_loss: 0.50024  (0.38450)
     | > postnet_ssim_loss: 0.50021  (0.38447)
     | > loss: 22.46682  (24.45396)
     | > align_error: 0.99480  (0.99464)
     | > amp_scaler: 65536.00000  (65536.00000)
     | > grad_norm: 2.28002  (1.84044)
     | > current_lr: 4.4e-06 
     | > step_time: 2.41990  (3.91424)
     | > loader_time: 0.00900  (0.03531)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 4.38051 [0m(-0.04667)
     | > avg_decoder_loss:[92m 29.58357 [0m(-0.09122)
     | > avg_postnet_loss:[92m 16.45866 [0m(-0.04983)
     | > avg_stopnet_loss:[91m 1.90920 [0m(+0.00081)
     | > avg_decoder_coarse_loss:[92m 29.59303 [0m(-0.19253)
     | > avg_decoder_ddc_loss:[92m 0.00033 [0m(-0.00001)
     | > avg_ga_loss:[92m 0.00245 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.39987 [0m(+0.00045)
     | > avg_postnet_diff_spec_loss:[92m 0.85012 [0m(-0.00013)
     | > avg_decoder_ssim_loss:[92m 0.39784 [0m(-0.00001)
     | > avg_postnet_ssim_loss:[92m 0.39781 [0m(-0.00000)
     | > avg_loss:[92m 21.34177 [0m(-0.08252)
     | > avg_align_error:[91m 0.99505 [0m(+0.00000)


[4m[1m > EPOCH: 2/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 09:53:52) [0m

[1m   --> STEP: 24/87 -- GLOBAL_STEP: 270200[0m
     | > decoder_loss: 33.65675  (33.77657)
     | > postnet_loss: 19.31754  (20.02908)
     | > stopnet_loss: 2.21158  (2.00135)
     | > decoder_coarse_loss: 33.80928  (33.89118)
     | > decoder_ddc_loss: 0.00049  (0.00050)
     | > ga_loss: 0.00283  (0.00268)
     | > decoder_diff_spec_loss: 0.48932  (0.43548)
     | > postnet_diff_spec_loss: 0.92257  (0.86620)
     | > decoder_ssim_loss: 0.33258  (0.38424)
     | > postnet_ssim_loss: 0.33257  (0.38421)
     | > loss: 24.44102  (24.45662)
     | > align_error: 0.99485  (0.99457)
     | > amp_scaler: 65536.00000  (65536.00000)
     | > grad_norm: 2.12394  (1.97542)
     | > current_lr: 0.00001 
     | > step_time: 4.29150  (4.17582)
     | > loader_time: 0.01790  (0.03722)


[1m   --> STEP: 49/87 -- GLOBAL_STEP: 270225[0m
     | > decoder_loss: 33.73487  (33.56885)
     | > postnet_loss: 20.68546  (20.04580)
     | > stopnet_loss: 1.99675  (1.97711)
     | > decoder_coarse_loss: 33.85052  (33.68385)
     | > decoder_ddc_loss: 0.00041  (0.00050)
     | > ga_loss: 0.00261  (0.00277)
     | > decoder_diff_spec_loss: 0.42891  (0.44106)
     | > postnet_diff_spec_loss: 0.84671  (0.87013)
     | > decoder_ssim_loss: 0.37099  (0.38837)
     | > postnet_ssim_loss: 0.37098  (0.38835)
     | > loss: 24.58204  (24.33767)
     | > align_error: 0.99493  (0.99442)
     | > amp_scaler: 65536.00000  (65536.00000)
     | > grad_norm: 2.26923  (2.04194)
     | > current_lr: 0.00001 
     | > step_time: 4.02970  (3.91044)
     | > loader_time: 0.02430  (0.03339)


[1m   --> STEP: 74/87 -- GLOBAL_STEP: 270250[0m
     | > decoder_loss: 35.64378  (33.46353)
     | > postnet_loss: 22.47408  (20.07127)
     | > stopnet_loss: 1.88531  (1.98070)
     | > decoder_coarse_loss: 35.79073  (33.58459)
     | > decoder_ddc_loss: 0.00031  (0.00047)
     | > ga_loss: 0.00212  (0.00268)
     | > decoder_diff_spec_loss: 0.45460  (0.44381)
     | > postnet_diff_spec_loss: 0.84676  (0.86847)
     | > decoder_ssim_loss: 0.38869  (0.38437)
     | > postnet_ssim_loss: 0.38871  (0.38435)
     | > loss: 25.89281  (24.29432)
     | > align_error: 0.99609  (0.99459)
     | > amp_scaler: 65536.00000  (65536.00000)
     | > grad_norm: 2.75787  (2.17709)
     | > current_lr: 0.00001 
     | > step_time: 4.92600  (3.90082)
     | > loader_time: 0.03460  (0.03135)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 4.47164 [0m(+0.09114)
     | > avg_decoder_loss:[92m 29.29177 [0m(-0.29180)
     | > avg_postnet_loss:[92m 16.38081 [0m(-0.07785)
     | > avg_stopnet_loss:[91m 1.91971 [0m(+0.01051)
     | > avg_decoder_coarse_loss:[92m 29.08514 [0m(-0.50790)
     | > avg_decoder_ddc_loss:[92m 0.00030 [0m(-0.00003)
     | > avg_ga_loss:[92m 0.00245 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.40744 [0m(+0.00757)
     | > avg_postnet_diff_spec_loss:[92m 0.84936 [0m(-0.00076)
     | > avg_decoder_ssim_loss:[92m 0.39780 [0m(-0.00004)
     | > avg_postnet_ssim_loss:[92m 0.39780 [0m(-0.00001)
     | > avg_loss:[92m 21.13456 [0m(-0.20721)
     | > avg_align_error:[91m 0.99507 [0m(+0.00002)


[4m[1m > EPOCH: 3/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 10:01:25) [0m

[1m   --> STEP: 11/87 -- GLOBAL_STEP: 270275[0m
     | > decoder_loss: 33.91050  (32.96873)
     | > postnet_loss: 20.90972  (19.43213)
     | > stopnet_loss: 1.53266  (1.90100)
     | > decoder_coarse_loss: 34.13155  (33.12856)
     | > decoder_ddc_loss: 0.00033  (0.00042)
     | > ga_loss: 0.00247  (0.00261)
     | > decoder_diff_spec_loss: 0.44176  (0.45661)
     | > postnet_diff_spec_loss: 0.83361  (0.86152)
     | > decoder_ssim_loss: 0.47719  (0.40293)
     | > postnet_ssim_loss: 0.47721  (0.40295)
     | > loss: 24.34049  (23.82752)
     | > align_error: 0.99501  (0.99463)
     | > amp_scaler: 65536.00000  (65536.00000)
     | > grad_norm: 3.24110  (2.98596)
     | > current_lr: 0.00001 
     | > step_time: 4.55740  (4.29247)
     | > loader_time: 0.02770  (0.05177)


[1m   --> STEP: 36/87 -- GLOBAL_STEP: 270300[0m
     | > decoder_loss: 29.88634  (32.93357)
     | > postnet_loss: 18.32221  (19.86366)
     | > stopnet_loss: 1.47328  (1.98623)
     | > decoder_coarse_loss: 30.06074  (33.13350)
     | > decoder_ddc_loss: 0.00071  (0.00041)
     | > ga_loss: 0.00542  (0.00277)
     | > decoder_diff_spec_loss: 0.49260  (0.47857)
     | > postnet_diff_spec_loss: 0.87393  (0.86991)
     | > decoder_ssim_loss: 0.54084  (0.38720)
     | > postnet_ssim_loss: 0.54090  (0.38725)
     | > loss: 21.67994  (24.01357)
     | > align_error: 0.98861  (0.99449)
     | > amp_scaler: 65536.00000  (65536.00000)
     | > grad_norm: 3.12241  (3.39558)
     | > current_lr: 0.00001 
     | > step_time: 1.33060  (3.93492)
     | > loader_time: 0.01500  (0.04326)


[1m   --> STEP: 61/87 -- GLOBAL_STEP: 270325[0m
     | > decoder_loss: 32.46384  (32.49517)
     | > postnet_loss: 18.52076  (19.77866)
     | > stopnet_loss: 1.98454  (1.98040)
     | > decoder_coarse_loss: 32.83321  (32.73124)
     | > decoder_ddc_loss: 0.00043  (0.00040)
     | > ga_loss: 0.00309  (0.00272)
     | > decoder_diff_spec_loss: 0.51426  (0.49813)
     | > postnet_diff_spec_loss: 0.82924  (0.86910)
     | > decoder_ssim_loss: 0.37618  (0.38665)
     | > postnet_ssim_loss: 0.37644  (0.38677)
     | > loss: 23.47859  (23.78054)
     | > align_error: 0.99382  (0.99453)
     | > amp_scaler: 65536.00000  (65536.00000)
     | > grad_norm: 5.39073  (3.90676)
     | > current_lr: 0.00001 
     | > step_time: 3.62060  (3.85772)
     | > loader_time: 0.02080  (0.03753)


[1m   --> STEP: 86/87 -- GLOBAL_STEP: 270350[0m
     | > decoder_loss: 28.91236  (32.11394)
     | > postnet_loss: 17.99180  (19.77838)
     | > stopnet_loss: 1.15939  (2.00245)
     | > decoder_coarse_loss: 29.22879  (32.41631)
     | > decoder_ddc_loss: 0.00036  (0.00038)
     | > ga_loss: 0.00284  (0.00262)
     | > decoder_diff_spec_loss: 0.58769  (0.53186)
     | > postnet_diff_spec_loss: 0.85977  (0.86919)
     | > decoder_ssim_loss: 0.66928  (0.38288)
     | > postnet_ssim_loss: 0.67045  (0.38309)
     | > loss: 20.90372  (23.63458)
     | > align_error: 0.99351  (0.99474)
     | > amp_scaler: 65536.00000  (65536.00000)
     | > grad_norm: 6.41425  (4.63898)
     | > current_lr: 0.00001 
     | > step_time: 2.13850  (3.83976)
     | > loader_time: 0.01620  (0.03331)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.92131 [0m(-0.55034)
     | > avg_decoder_loss:[92m 27.84784 [0m(-1.44393)
     | > avg_postnet_loss:[92m 16.35747 [0m(-0.02334)
     | > avg_stopnet_loss:[91m 1.92093 [0m(+0.00121)
     | > avg_decoder_coarse_loss:[92m 27.32378 [0m(-1.76136)
     | > avg_decoder_ddc_loss:[91m 0.00031 [0m(+0.00000)
     | > avg_ga_loss:[92m 0.00244 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[91m 0.47714 [0m(+0.06970)
     | > avg_postnet_diff_spec_loss:[92m 0.84867 [0m(-0.00069)
     | > avg_decoder_ssim_loss:[92m 0.39746 [0m(-0.00035)
     | > avg_postnet_ssim_loss:[91m 0.39780 [0m(+0.00000)
     | > avg_loss:[92m 20.34572 [0m(-0.78884)
     | > avg_align_error:[92m 0.99506 [0m(-0.00001)


[4m[1m > EPOCH: 4/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 10:09:00) [0m

[1m   --> STEP: 23/87 -- GLOBAL_STEP: 270375[0m
     | > decoder_loss: 29.11839  (30.06714)
     | > postnet_loss: 19.45592  (19.35797)
     | > stopnet_loss: 1.38450  (1.98913)
     | > decoder_coarse_loss: 29.74896  (30.71048)
     | > decoder_ddc_loss: 0.00044  (0.00038)
     | > ga_loss: 0.00262  (0.00263)
     | > decoder_diff_spec_loss: 0.73772  (0.71978)
     | > postnet_diff_spec_loss: 0.85766  (0.86269)
     | > decoder_ssim_loss: 0.54674  (0.38553)
     | > postnet_ssim_loss: 0.54833  (0.38639)
     | > loss: 21.65116  (22.62490)
     | > align_error: 0.99442  (0.99463)
     | > amp_scaler: 32768.00000  (62686.60870)
     | > grad_norm: 8.91579  (7.59277)
     | > current_lr: 0.00001 
     | > step_time: 3.36800  (4.14451)
     | > loader_time: 0.01930  (0.03645)


[1m   --> STEP: 48/87 -- GLOBAL_STEP: 270400[0m
     | > decoder_loss: 27.45179  (29.21876)
     | > postnet_loss: 18.76363  (19.26833)
     | > stopnet_loss: 2.53083  (1.95701)
     | > decoder_coarse_loss: 28.55881  (29.95943)
     | > decoder_ddc_loss: 0.00047  (0.00042)
     | > ga_loss: 0.00294  (0.00272)
     | > decoder_diff_spec_loss: 1.05390  (0.79708)
     | > postnet_diff_spec_loss: 0.91593  (0.86955)
     | > decoder_ssim_loss: 0.28525  (0.38735)
     | > postnet_ssim_loss: 0.28696  (0.38863)
     | > loss: 21.87473  (22.19299)
     | > align_error: 0.99447  (0.99445)
     | > amp_scaler: 32768.00000  (47104.00000)
     | > grad_norm: 10.15878  (8.56461)
     | > current_lr: 0.00001 
     | > step_time: 3.51340  (3.86949)
     | > loader_time: 0.02350  (0.03438)


[1m   --> STEP: 73/87 -- GLOBAL_STEP: 270425[0m
     | > decoder_loss: 26.23877  (28.31607)
     | > postnet_loss: 18.51279  (19.18787)
     | > stopnet_loss: 1.44395  (1.94125)
     | > decoder_coarse_loss: 27.25938  (29.15943)
     | > decoder_ddc_loss: 0.00063  (0.00042)
     | > ga_loss: 0.00289  (0.00263)
     | > decoder_diff_spec_loss: 1.08248  (0.88553)
     | > postnet_diff_spec_loss: 0.86650  (0.86777)
     | > decoder_ssim_loss: 0.47101  (0.38244)
     | > postnet_ssim_loss: 0.47519  (0.38421)
     | > loss: 20.18509  (21.75036)
     | > align_error: 0.99329  (0.99458)
     | > amp_scaler: 32768.00000  (42194.41096)
     | > grad_norm: 10.65501  (9.36705)
     | > current_lr: 0.00001 
     | > step_time: 2.95280  (3.87673)
     | > loader_time: 0.01680  (0.03263)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.81436 [0m(-0.10694)
     | > avg_decoder_loss:[92m 23.66083 [0m(-4.18701)
     | > avg_postnet_loss:[92m 16.02499 [0m(-0.33248)
     | > avg_stopnet_loss:[92m 1.75648 [0m(-0.16445)
     | > avg_decoder_coarse_loss:[92m 23.16756 [0m(-4.15623)
     | > avg_decoder_ddc_loss:[91m 0.00038 [0m(+0.00007)
     | > avg_ga_loss:[92m 0.00240 [0m(-0.00003)
     | > avg_decoder_diff_spec_loss:[91m 0.75856 [0m(+0.28142)
     | > avg_postnet_diff_spec_loss:[92m 0.84859 [0m(-0.00009)
     | > avg_decoder_ssim_loss:[92m 0.39497 [0m(-0.00248)
     | > avg_postnet_ssim_loss:[92m 0.39777 [0m(-0.00003)
     | > avg_loss:[92m 18.08190 [0m(-2.26382)
     | > avg_align_error:[92m 0.99497 [0m(-0.00009)


[4m[1m > EPOCH: 5/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 10:16:27) [0m

[1m   --> STEP: 10/87 -- GLOBAL_STEP: 270450[0m
     | > decoder_loss: 22.69661  (24.06207)
     | > postnet_loss: 17.76546  (18.02948)
     | > stopnet_loss: 2.14139  (1.76646)
     | > decoder_coarse_loss: 23.83087  (25.21202)
     | > decoder_ddc_loss: 0.00069  (0.00058)
     | > ga_loss: 0.00248  (0.00253)
     | > decoder_diff_spec_loss: 1.29340  (1.26245)
     | > postnet_diff_spec_loss: 0.84453  (0.86370)
     | > decoder_ssim_loss: 0.30954  (0.39087)
     | > postnet_ssim_loss: 0.31292  (0.39537)
     | > loss: 18.91728  (19.33327)
     | > align_error: 0.99426  (0.99443)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 11.06048  (11.00990)
     | > current_lr: 0.00001 
     | > step_time: 4.43650  (4.44287)
     | > loader_time: 0.02130  (0.04474)


[1m   --> STEP: 35/87 -- GLOBAL_STEP: 270475[0m
     | > decoder_loss: 22.03531  (23.03062)
     | > postnet_loss: 18.48150  (18.42949)
     | > stopnet_loss: 1.57175  (1.78411)
     | > decoder_coarse_loss: 23.36873  (24.26573)
     | > decoder_ddc_loss: 0.00071  (0.00060)
     | > ga_loss: 0.00255  (0.00259)
     | > decoder_diff_spec_loss: 1.49859  (1.37585)
     | > postnet_diff_spec_loss: 0.93733  (0.86914)
     | > decoder_ssim_loss: 0.39493  (0.37688)
     | > postnet_ssim_loss: 0.40439  (0.38267)
     | > loss: 18.36487  (18.97981)
     | > align_error: 0.99375  (0.99441)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 9.17610  (10.32904)
     | > current_lr: 0.00001 
     | > step_time: 3.89140  (4.05096)
     | > loader_time: 0.01820  (0.03421)


[1m   --> STEP: 60/87 -- GLOBAL_STEP: 270500[0m
     | > decoder_loss: 18.31285  (21.39840)
     | > postnet_loss: 19.07887  (18.08137)
     | > stopnet_loss: 1.60638  (1.72494)
     | > decoder_coarse_loss: 19.59113  (22.63453)
     | > decoder_ddc_loss: 0.00062  (0.00067)
     | > ga_loss: 0.00202  (0.00260)
     | > decoder_diff_spec_loss: 1.65340  (1.44886)
     | > postnet_diff_spec_loss: 0.88099  (0.86912)
     | > decoder_ssim_loss: 0.36157  (0.37923)
     | > postnet_ssim_loss: 0.37245  (0.38669)
     | > loss: 16.67942  (18.03767)
     | > align_error: 0.99507  (0.99416)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 6.71543  (9.06939)
     | > current_lr: 0.00001 
     | > step_time: 4.39530  (3.92861)
     | > loader_time: 0.01700  (0.03270)


[1m   --> STEP: 85/87 -- GLOBAL_STEP: 270525[0m
     | > decoder_loss: 15.24884  (19.72752)
     | > postnet_loss: 18.95861  (17.71304)
     | > stopnet_loss: 1.92089  (1.73698)
     | > decoder_coarse_loss: 16.44322  (20.95506)
     | > decoder_ddc_loss: 0.00071  (0.00067)
     | > ga_loss: 0.00205  (0.00251)
     | > decoder_diff_spec_loss: 1.61361  (1.50441)
     | > postnet_diff_spec_loss: 0.86419  (0.86875)
     | > decoder_ssim_loss: 0.31902  (0.37021)
     | > postnet_ssim_loss: 0.33164  (0.37938)
     | > loss: 15.37611  (17.12928)
     | > align_error: 0.99467  (0.99430)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 6.67962  (8.23966)
     | > current_lr: 0.00001 
     | > step_time: 3.02580  (3.86840)
     | > loader_time: 0.01680  (0.03259)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.90808 [0m(+0.09372)
     | > avg_decoder_loss:[92m 14.83924 [0m(-8.82158)
     | > avg_postnet_loss:[92m 13.73226 [0m(-2.29273)
     | > avg_stopnet_loss:[92m 1.48977 [0m(-0.26671)
     | > avg_decoder_coarse_loss:[92m 14.43195 [0m(-8.73561)
     | > avg_decoder_ddc_loss:[91m 0.00060 [0m(+0.00022)
     | > avg_ga_loss:[92m 0.00237 [0m(-0.00004)
     | > avg_decoder_diff_spec_loss:[91m 0.99427 [0m(+0.23571)
     | > avg_postnet_diff_spec_loss:[91m 0.84860 [0m(+0.00002)
     | > avg_decoder_ssim_loss:[92m 0.38348 [0m(-0.01149)
     | > avg_postnet_ssim_loss:[92m 0.39731 [0m(-0.00047)
     | > avg_loss:[92m 12.90853 [0m(-5.17337)
     | > avg_align_error:[92m 0.99471 [0m(-0.00026)


[4m[1m > EPOCH: 6/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 10:23:56) [0m

[1m   --> STEP: 22/87 -- GLOBAL_STEP: 270550[0m
     | > decoder_loss: 9.49542  (12.29992)
     | > postnet_loss: 14.82748  (15.44790)
     | > stopnet_loss: 1.82406  (1.79368)
     | > decoder_coarse_loss: 10.55983  (13.43074)
     | > decoder_ddc_loss: 0.00054  (0.00081)
     | > ga_loss: 0.00207  (0.00249)
     | > decoder_diff_spec_loss: 1.45269  (1.52668)
     | > postnet_diff_spec_loss: 0.79610  (0.86264)
     | > decoder_ssim_loss: 0.32349  (0.36024)
     | > postnet_ssim_loss: 0.33883  (0.37802)
     | > loss: 11.28300  (12.88286)
     | > align_error: 0.99525  (0.99365)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.99593  (7.84181)
     | > current_lr: 0.00001 
     | > step_time: 4.58050  (4.06877)
     | > loader_time: 0.02240  (0.03235)


[1m   --> STEP: 47/87 -- GLOBAL_STEP: 270575[0m
     | > decoder_loss: 9.89781  (11.03232)
     | > postnet_loss: 15.03985  (15.00272)
     | > stopnet_loss: 1.69880  (1.74119)
     | > decoder_coarse_loss: 11.00889  (12.13425)
     | > decoder_ddc_loss: 0.00130  (0.00084)
     | > ga_loss: 0.00265  (0.00255)
     | > decoder_diff_spec_loss: 1.23263  (1.44957)
     | > postnet_diff_spec_loss: 0.83734  (0.86843)
     | > decoder_ssim_loss: 0.35410  (0.36848)
     | > postnet_ssim_loss: 0.37772  (0.38944)
     | > loss: 11.39947  (12.06546)
     | > align_error: 0.99159  (0.99331)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 6.98492  (7.72906)
     | > current_lr: 0.00001 
     | > step_time: 3.11440  (3.85206)
     | > loader_time: 0.05140  (0.03207)


[1m   --> STEP: 72/87 -- GLOBAL_STEP: 270600[0m
     | > decoder_loss: 6.64104  (9.88033)
     | > postnet_loss: 15.19248  (14.56390)
     | > stopnet_loss: 1.81331  (1.75956)
     | > decoder_coarse_loss: 7.56394  (10.95148)
     | > decoder_ddc_loss: 0.00061  (0.00083)
     | > ga_loss: 0.00194  (0.00248)
     | > decoder_diff_spec_loss: 1.05653  (1.35563)
     | > postnet_diff_spec_loss: 0.82596  (0.86762)
     | > decoder_ssim_loss: 0.32339  (0.35936)
     | > postnet_ssim_loss: 0.34763  (0.38141)
     | > loss: 9.81093  (11.36210)
     | > align_error: 0.99479  (0.99342)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.66884  (7.08763)
     | > current_lr: 0.00002 
     | > step_time: 4.30480  (3.88522)
     | > loader_time: 0.02170  (0.03368)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 4.21346 [0m(+0.30538)
     | > avg_decoder_loss:[92m 8.91240 [0m(-5.92685)
     | > avg_postnet_loss:[92m 10.56892 [0m(-3.16333)
     | > avg_stopnet_loss:[92m 1.48960 [0m(-0.00016)
     | > avg_decoder_coarse_loss:[92m 8.28552 [0m(-6.14643)
     | > avg_decoder_ddc_loss:[91m 0.00077 [0m(+0.00017)
     | > avg_ga_loss:[91m 0.00238 [0m(+0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.63444 [0m(-0.35983)
     | > avg_postnet_diff_spec_loss:[91m 0.84862 [0m(+0.00002)
     | > avg_decoder_ssim_loss:[92m 0.37109 [0m(-0.01239)
     | > avg_postnet_ssim_loss:[92m 0.39577 [0m(-0.00154)
     | > avg_loss:[92m 9.00589 [0m(-3.90264)
     | > avg_align_error:[92m 0.99426 [0m(-0.00045)


[4m[1m > EPOCH: 7/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 10:31:22) [0m

[1m   --> STEP: 9/87 -- GLOBAL_STEP: 270625[0m
     | > decoder_loss: 5.28523  (5.50556)
     | > postnet_loss: 12.67657  (12.03181)
     | > stopnet_loss: 2.23536  (1.72554)
     | > decoder_coarse_loss: 6.09750  (6.35821)
     | > decoder_ddc_loss: 0.00068  (0.00100)
     | > ga_loss: 0.00216  (0.00243)
     | > decoder_diff_spec_loss: 0.85466  (0.86675)
     | > postnet_diff_spec_loss: 0.85298  (0.86569)
     | > decoder_ssim_loss: 0.28415  (0.37088)
     | > postnet_ssim_loss: 0.30625  (0.40163)
     | > loss: 8.83564  (8.33806)
     | > align_error: 0.99433  (0.99274)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.44817  (5.44418)
     | > current_lr: 0.00002 
     | > step_time: 4.59520  (4.37813)
     | > loader_time: 0.01910  (0.04257)


[1m   --> STEP: 34/87 -- GLOBAL_STEP: 270650[0m
     | > decoder_loss: 3.89295  (5.01051)
     | > postnet_loss: 10.07841  (12.12565)
     | > stopnet_loss: 2.11366  (1.83227)
     | > decoder_coarse_loss: 4.57642  (5.81554)
     | > decoder_ddc_loss: 0.00133  (0.00097)
     | > ga_loss: 0.00364  (0.00246)
     | > decoder_diff_spec_loss: 0.66608  (0.78188)
     | > postnet_diff_spec_loss: 0.85537  (0.86693)
     | > decoder_ssim_loss: 0.30837  (0.34797)
     | > postnet_ssim_loss: 0.33901  (0.37898)
     | > loss: 7.31137  (8.17669)
     | > align_error: 0.99022  (0.99276)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 6.53385  (4.68639)
     | > current_lr: 0.00002 
     | > step_time: 3.00580  (4.13619)
     | > loader_time: 0.07490  (0.03721)


[1m   --> STEP: 59/87 -- GLOBAL_STEP: 270675[0m
     | > decoder_loss: 3.59047  (4.60770)
     | > postnet_loss: 10.00524  (11.72132)
     | > stopnet_loss: 2.37504  (1.81117)
     | > decoder_coarse_loss: 4.12896  (5.33509)
     | > decoder_ddc_loss: 0.00109  (0.00101)
     | > ga_loss: 0.00303  (0.00249)
     | > decoder_diff_spec_loss: 0.56485  (0.70666)
     | > postnet_diff_spec_loss: 0.88390  (0.86878)
     | > decoder_ssim_loss: 0.27653  (0.35015)
     | > postnet_ssim_loss: 0.30761  (0.38353)
     | > loss: 7.32984  (7.81719)
     | > align_error: 0.99134  (0.99241)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.97399  (4.67742)
     | > current_lr: 0.00002 
     | > step_time: 3.65090  (3.92817)
     | > loader_time: 0.01860  (0.03207)


[1m   --> STEP: 84/87 -- GLOBAL_STEP: 270700[0m
     | > decoder_loss: 3.66603  (4.35090)
     | > postnet_loss: 11.04200  (11.44320)
     | > stopnet_loss: 1.40996  (1.84235)
     | > decoder_coarse_loss: 4.04647  (4.99672)
     | > decoder_ddc_loss: 0.00106  (0.00098)
     | > ga_loss: 0.00237  (0.00241)
     | > decoder_diff_spec_loss: 0.47636  (0.64900)
     | > postnet_diff_spec_loss: 0.84686  (0.86859)
     | > decoder_ssim_loss: 0.41154  (0.34259)
     | > postnet_ssim_loss: 0.45527  (0.37652)
     | > loss: 6.65822  (7.61154)
     | > align_error: 0.99193  (0.99268)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.50366  (4.32106)
     | > current_lr: 0.00002 
     | > step_time: 2.34940  (3.86759)
     | > loader_time: 0.01790  (0.03106)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 4.26341 [0m(+0.04995)
     | > avg_decoder_loss:[92m 5.92881 [0m(-2.98359)
     | > avg_postnet_loss:[92m 8.24160 [0m(-2.32732)
     | > avg_stopnet_loss:[92m 1.48372 [0m(-0.00588)
     | > avg_decoder_coarse_loss:[92m 5.41608 [0m(-2.86944)
     | > avg_decoder_ddc_loss:[91m 0.00116 [0m(+0.00039)
     | > avg_ga_loss:[92m 0.00228 [0m(-0.00010)
     | > avg_decoder_diff_spec_loss:[92m 0.44337 [0m(-0.19107)
     | > avg_postnet_diff_spec_loss:[91m 0.84922 [0m(+0.00060)
     | > avg_decoder_ssim_loss:[92m 0.36052 [0m(-0.01057)
     | > avg_postnet_ssim_loss:[92m 0.39400 [0m(-0.00177)
     | > avg_loss:[92m 6.90382 [0m(-2.10208)
     | > avg_align_error:[92m 0.99351 [0m(-0.00075)


[4m[1m > EPOCH: 8/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 10:38:55) [0m

[1m   --> STEP: 21/87 -- GLOBAL_STEP: 270725[0m
     | > decoder_loss: 3.66059  (3.51794)
     | > postnet_loss: 9.76087  (9.99715)
     | > stopnet_loss: 1.30994  (1.88188)
     | > decoder_coarse_loss: 3.93009  (3.81826)
     | > decoder_ddc_loss: 0.00193  (0.00110)
     | > ga_loss: 0.00286  (0.00247)
     | > decoder_diff_spec_loss: 0.44771  (0.45384)
     | > postnet_diff_spec_loss: 0.87834  (0.86546)
     | > decoder_ssim_loss: 0.43929  (0.33744)
     | > postnet_ssim_loss: 0.49313  (0.37601)
     | > loss: 6.22723  (6.73601)
     | > align_error: 0.98842  (0.99214)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.28116  (3.76667)
     | > current_lr: 0.00002 
     | > step_time: 2.25170  (4.01210)
     | > loader_time: 0.01610  (0.03130)


[1m   --> STEP: 46/87 -- GLOBAL_STEP: 270750[0m
     | > decoder_loss: 3.19803  (3.49038)
     | > postnet_loss: 10.58974  (9.75217)
     | > stopnet_loss: 1.75206  (1.83943)
     | > decoder_coarse_loss: 3.35844  (3.73379)
     | > decoder_ddc_loss: 0.00088  (0.00109)
     | > ga_loss: 0.00184  (0.00253)
     | > decoder_diff_spec_loss: 0.42413  (0.44561)
     | > postnet_diff_spec_loss: 0.84963  (0.86874)
     | > decoder_ssim_loss: 0.33829  (0.34482)
     | > postnet_ssim_loss: 0.37916  (0.38564)
     | > loss: 6.54583  (6.60761)
     | > align_error: 0.99412  (0.99201)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.93504  (3.83136)
     | > current_lr: 0.00002 
     | > step_time: 5.09150  (3.87286)
     | > loader_time: 0.01930  (0.02982)


[1m   --> STEP: 71/87 -- GLOBAL_STEP: 270775[0m
     | > decoder_loss: 3.04366  (3.42984)
     | > postnet_loss: 7.93804  (9.51712)
     | > stopnet_loss: 2.19231  (1.87098)
     | > decoder_coarse_loss: 3.12579  (3.63115)
     | > decoder_ddc_loss: 0.00081  (0.00107)
     | > ga_loss: 0.00238  (0.00247)
     | > decoder_diff_spec_loss: 0.39354  (0.43679)
     | > postnet_diff_spec_loss: 0.85945  (0.86785)
     | > decoder_ssim_loss: 0.29572  (0.33744)
     | > postnet_ssim_loss: 0.33249  (0.37801)
     | > loss: 6.20156  (6.53314)
     | > align_error: 0.99330  (0.99222)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.04446  (3.71381)
     | > current_lr: 0.00002 
     | > step_time: 3.74610  (3.84952)
     | > loader_time: 0.05160  (0.03503)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.96696 [0m(-0.29645)
     | > avg_decoder_loss:[92m 4.59225 [0m(-1.33655)
     | > avg_postnet_loss:[92m 6.69874 [0m(-1.54287)
     | > avg_stopnet_loss:[91m 1.48478 [0m(+0.00106)
     | > avg_decoder_coarse_loss:[92m 4.23383 [0m(-1.18225)
     | > avg_decoder_ddc_loss:[91m 0.00170 [0m(+0.00054)
     | > avg_ga_loss:[92m 0.00218 [0m(-0.00011)
     | > avg_decoder_diff_spec_loss:[92m 0.40937 [0m(-0.03400)
     | > avg_postnet_diff_spec_loss:[91m 0.84928 [0m(+0.00005)
     | > avg_decoder_ssim_loss:[92m 0.35467 [0m(-0.00585)
     | > avg_postnet_ssim_loss:[92m 0.39224 [0m(-0.00176)
     | > avg_loss:[92m 5.87867 [0m(-1.02515)
     | > avg_align_error:[92m 0.99274 [0m(-0.00077)


[4m[1m > EPOCH: 9/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 10:46:15) [0m

[1m   --> STEP: 8/87 -- GLOBAL_STEP: 270800[0m
     | > decoder_loss: 2.77042  (3.34876)
     | > postnet_loss: 6.60326  (8.07316)
     | > stopnet_loss: 2.62828  (1.67093)
     | > decoder_coarse_loss: 2.81000  (3.41356)
     | > decoder_ddc_loss: 0.00137  (0.00113)
     | > ga_loss: 0.00292  (0.00244)
     | > decoder_diff_spec_loss: 0.38508  (0.40873)
     | > postnet_diff_spec_loss: 0.84561  (0.86658)
     | > decoder_ssim_loss: 0.23810  (0.36528)
     | > postnet_ssim_loss: 0.26860  (0.40992)
     | > loss: 6.12350  (5.90491)
     | > align_error: 0.99238  (0.99194)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 8.74082  (4.67980)
     | > current_lr: 0.00002 
     | > step_time: 3.62370  (4.52924)
     | > loader_time: 0.01660  (0.04591)


[1m   --> STEP: 33/87 -- GLOBAL_STEP: 270825[0m
     | > decoder_loss: 3.18272  (3.35444)
     | > postnet_loss: 8.32297  (8.36132)
     | > stopnet_loss: 1.60680  (1.86556)
     | > decoder_coarse_loss: 3.22008  (3.40789)
     | > decoder_ddc_loss: 0.00110  (0.00106)
     | > ga_loss: 0.00237  (0.00245)
     | > decoder_diff_spec_loss: 0.41470  (0.41364)
     | > postnet_diff_spec_loss: 0.88372  (0.86683)
     | > decoder_ssim_loss: 0.35963  (0.33528)
     | > postnet_ssim_loss: 0.40431  (0.37693)
     | > loss: 5.81595  (6.15716)
     | > align_error: 0.99258  (0.99239)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.02062  (3.64075)
     | > current_lr: 0.00002 
     | > step_time: 4.32390  (4.15121)
     | > loader_time: 0.01940  (0.03865)


[1m   --> STEP: 58/87 -- GLOBAL_STEP: 270850[0m
     | > decoder_loss: 3.26352  (3.33814)
     | > postnet_loss: 8.23371  (8.11057)
     | > stopnet_loss: 1.72187  (1.85509)
     | > decoder_coarse_loss: 3.28463  (3.38248)
     | > decoder_ddc_loss: 0.00087  (0.00110)
     | > ga_loss: 0.00198  (0.00250)
     | > decoder_diff_spec_loss: 0.43691  (0.41459)
     | > postnet_diff_spec_loss: 0.88235  (0.86808)
     | > decoder_ssim_loss: 0.33481  (0.33908)
     | > postnet_ssim_loss: 0.37538  (0.38160)
     | > loss: 5.93483  (6.07651)
     | > align_error: 0.99384  (0.99210)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.06667  (3.67424)
     | > current_lr: 0.00002 
     | > step_time: 4.85700  (3.94894)
     | > loader_time: 0.01910  (0.03453)


[1m   --> STEP: 83/87 -- GLOBAL_STEP: 270875[0m
     | > decoder_loss: 3.54209  (3.34163)
     | > postnet_loss: 8.03385  (7.94922)
     | > stopnet_loss: 2.27552  (1.88707)
     | > decoder_coarse_loss: 3.56535  (3.37937)
     | > decoder_ddc_loss: 0.00100  (0.00104)
     | > ga_loss: 0.00202  (0.00243)
     | > decoder_diff_spec_loss: 0.39903  (0.41466)
     | > postnet_diff_spec_loss: 0.84199  (0.86844)
     | > decoder_ssim_loss: 0.25478  (0.33102)
     | > postnet_ssim_loss: 0.28756  (0.37253)
     | > loss: 6.51702  (6.06368)
     | > align_error: 0.99382  (0.99245)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.99819  (3.48684)
     | > current_lr: 0.00002 
     | > step_time: 3.05130  (3.91721)
     | > loader_time: 0.01780  (0.03351)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 4.10240 [0m(+0.13544)
     | > avg_decoder_loss:[92m 4.00915 [0m(-0.58311)
     | > avg_postnet_loss:[92m 5.61061 [0m(-1.08813)
     | > avg_stopnet_loss:[91m 1.48923 [0m(+0.00445)
     | > avg_decoder_coarse_loss:[92m 3.73770 [0m(-0.49613)
     | > avg_decoder_ddc_loss:[91m 0.00174 [0m(+0.00004)
     | > avg_ga_loss:[92m 0.00214 [0m(-0.00004)
     | > avg_decoder_diff_spec_loss:[92m 0.40129 [0m(-0.00808)
     | > avg_postnet_diff_spec_loss:[92m 0.84909 [0m(-0.00019)
     | > avg_decoder_ssim_loss:[92m 0.35233 [0m(-0.00233)
     | > avg_postnet_ssim_loss:[92m 0.39077 [0m(-0.00147)
     | > avg_loss:[92m 5.33808 [0m(-0.54059)
     | > avg_align_error:[92m 0.99235 [0m(-0.00039)


[4m[1m > EPOCH: 10/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 10:53:52) [0m

[1m   --> STEP: 20/87 -- GLOBAL_STEP: 270900[0m
     | > decoder_loss: 2.73390  (3.31289)
     | > postnet_loss: 6.55884  (7.10075)
     | > stopnet_loss: 2.68172  (1.93307)
     | > decoder_coarse_loss: 2.74462  (3.32883)
     | > decoder_ddc_loss: 0.00113  (0.00106)
     | > ga_loss: 0.00271  (0.00243)
     | > decoder_diff_spec_loss: 0.38951  (0.40785)
     | > postnet_diff_spec_loss: 0.83951  (0.86442)
     | > decoder_ssim_loss: 0.26014  (0.32732)
     | > postnet_ssim_loss: 0.29593  (0.36755)
     | > loss: 6.15116  (5.87288)
     | > align_error: 0.99183  (0.99239)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 6.90892  (3.77631)
     | > current_lr: 0.00002 
     | > step_time: 3.83370  (4.33675)
     | > loader_time: 0.02090  (0.03390)


[1m   --> STEP: 45/87 -- GLOBAL_STEP: 270925[0m
     | > decoder_loss: 3.41869  (3.34532)
     | > postnet_loss: 6.59743  (6.95375)
     | > stopnet_loss: 2.18490  (1.87545)
     | > decoder_coarse_loss: 3.42762  (3.36161)
     | > decoder_ddc_loss: 0.00119  (0.00107)
     | > ga_loss: 0.00243  (0.00253)
     | > decoder_diff_spec_loss: 0.41414  (0.41207)
     | > postnet_diff_spec_loss: 0.87305  (0.86870)
     | > decoder_ssim_loss: 0.27639  (0.34091)
     | > postnet_ssim_loss: 0.31102  (0.38312)
     | > loss: 6.02694  (5.80473)
     | > align_error: 0.99252  (0.99211)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.11935  (3.90541)
     | > current_lr: 0.00002 
     | > step_time: 4.77780  (4.05591)
     | > loader_time: 0.01990  (0.03159)


[1m   --> STEP: 70/87 -- GLOBAL_STEP: 270950[0m
     | > decoder_loss: 3.71787  (3.30677)
     | > postnet_loss: 9.10484  (6.83209)
     | > stopnet_loss: 1.87468  (1.88290)
     | > decoder_coarse_loss: 3.73324  (3.32196)
     | > decoder_ddc_loss: 0.00081  (0.00103)
     | > ga_loss: 0.00195  (0.00246)
     | > decoder_diff_spec_loss: 0.44616  (0.41155)
     | > postnet_diff_spec_loss: 0.86855  (0.86751)
     | > decoder_ssim_loss: 0.31767  (0.33470)
     | > postnet_ssim_loss: 0.35528  (0.37613)
     | > loss: 6.52052  (5.75813)
     | > align_error: 0.99429  (0.99237)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.79091  (3.74576)
     | > current_lr: 0.00002 
     | > step_time: 4.36980  (4.10563)
     | > loader_time: 0.01870  (0.03404)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.92487 [0m(-0.17753)
     | > avg_decoder_loss:[92m 3.48336 [0m(-0.52579)
     | > avg_postnet_loss:[92m 4.80106 [0m(-0.80956)
     | > avg_stopnet_loss:[91m 1.49183 [0m(+0.00260)
     | > avg_decoder_coarse_loss:[92m 3.34204 [0m(-0.39566)
     | > avg_decoder_ddc_loss:[92m 0.00151 [0m(-0.00023)
     | > avg_ga_loss:[92m 0.00213 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.39433 [0m(-0.00696)
     | > avg_postnet_diff_spec_loss:[92m 0.84881 [0m(-0.00028)
     | > avg_decoder_ssim_loss:[92m 0.35024 [0m(-0.00209)
     | > avg_postnet_ssim_loss:[92m 0.38952 [0m(-0.00124)
     | > avg_loss:[92m 4.90520 [0m(-0.43288)
     | > avg_align_error:[92m 0.99217 [0m(-0.00018)


[4m[1m > EPOCH: 11/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 11:01:32) [0m

[1m   --> STEP: 7/87 -- GLOBAL_STEP: 270975[0m
     | > decoder_loss: 3.52314  (3.36710)
     | > postnet_loss: 6.03634  (6.04775)
     | > stopnet_loss: 1.79590  (1.59743)
     | > decoder_coarse_loss: 3.54499  (3.38488)
     | > decoder_ddc_loss: 0.00066  (0.00104)
     | > ga_loss: 0.00231  (0.00239)
     | > decoder_diff_spec_loss: 0.38920  (0.40730)
     | > postnet_diff_spec_loss: 0.84528  (0.86945)
     | > decoder_ssim_loss: 0.34085  (0.38237)
     | > postnet_ssim_loss: 0.38065  (0.42768)
     | > loss: 5.57274  (5.33127)
     | > align_error: 0.99365  (0.99219)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.85463  (4.04279)
     | > current_lr: 0.00002 
     | > step_time: 3.14410  (4.40618)
     | > loader_time: 0.02050  (0.05569)


[1m   --> STEP: 32/87 -- GLOBAL_STEP: 271000[0m
     | > decoder_loss: 2.81862  (3.30028)
     | > postnet_loss: 4.31527  (6.11414)
     | > stopnet_loss: 1.99377  (1.88479)
     | > decoder_coarse_loss: 2.83505  (3.31723)
     | > decoder_ddc_loss: 0.00104  (0.00097)
     | > ga_loss: 0.00284  (0.00243)
     | > decoder_diff_spec_loss: 0.37633  (0.40947)
     | > postnet_diff_spec_loss: 0.84077  (0.86598)
     | > decoder_ssim_loss: 0.29711  (0.33356)
     | > postnet_ssim_loss: 0.33635  (0.37395)
     | > loss: 4.96312  (5.57584)
     | > align_error: 0.99189  (0.99268)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.95839  (3.85059)
     | > current_lr: 0.00003 
     | > step_time: 4.46150  (4.09405)
     | > loader_time: 0.01750  (0.04230)


[1m   --> STEP: 57/87 -- GLOBAL_STEP: 271025[0m
     | > decoder_loss: 3.52169  (3.27664)
     | > postnet_loss: 6.08954  (5.94749)
     | > stopnet_loss: 1.77625  (1.85204)
     | > decoder_coarse_loss: 3.55660  (3.29478)
     | > decoder_ddc_loss: 0.00095  (0.00099)
     | > ga_loss: 0.00277  (0.00250)
     | > decoder_diff_spec_loss: 0.47478  (0.41062)
     | > postnet_diff_spec_loss: 0.93210  (0.86751)
     | > decoder_ssim_loss: 0.36286  (0.33833)
     | > postnet_ssim_loss: 0.40771  (0.37955)
     | > loss: 5.62665  (5.49350)
     | > align_error: 0.99293  (0.99242)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 6.20375  (3.61726)
     | > current_lr: 0.00003 
     | > step_time: 3.05930  (3.95589)
     | > loader_time: 0.05340  (0.04024)


[1m   --> STEP: 82/87 -- GLOBAL_STEP: 271050[0m
     | > decoder_loss: 2.73967  (3.27531)
     | > postnet_loss: 4.96557  (5.85399)
     | > stopnet_loss: 2.56152  (1.87820)
     | > decoder_coarse_loss: 2.77252  (3.29549)
     | > decoder_ddc_loss: 0.00084  (0.00094)
     | > ga_loss: 0.00206  (0.00242)
     | > decoder_diff_spec_loss: 0.43932  (0.41174)
     | > postnet_diff_spec_loss: 0.88592  (0.86846)
     | > decoder_ssim_loss: 0.24424  (0.33123)
     | > postnet_ssim_loss: 0.27520  (0.37152)
     | > loss: 5.65264  (5.49249)
     | > align_error: 0.99377  (0.99280)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.03871  (3.37793)
     | > current_lr: 0.00003 
     | > step_time: 3.36480  (3.95186)
     | > loader_time: 0.02090  (0.03637)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.86864 [0m(-0.05623)
     | > avg_decoder_loss:[92m 3.45142 [0m(-0.03193)
     | > avg_postnet_loss:[92m 4.22902 [0m(-0.57203)
     | > avg_stopnet_loss:[91m 1.49734 [0m(+0.00551)
     | > avg_decoder_coarse_loss:[92m 3.31093 [0m(-0.03111)
     | > avg_decoder_ddc_loss:[91m 0.00164 [0m(+0.00013)
     | > avg_ga_loss:[91m 0.00215 [0m(+0.00002)
     | > avg_decoder_diff_spec_loss:[91m 0.39474 [0m(+0.00042)
     | > avg_postnet_diff_spec_loss:[92m 0.84872 [0m(-0.00010)
     | > avg_decoder_ssim_loss:[91m 0.35030 [0m(+0.00006)
     | > avg_postnet_ssim_loss:[92m 0.38864 [0m(-0.00089)
     | > avg_loss:[92m 4.75195 [0m(-0.15325)
     | > avg_align_error:[91m 0.99243 [0m(+0.00026)


[4m[1m > EPOCH: 12/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 11:09:05) [0m

[1m   --> STEP: 19/87 -- GLOBAL_STEP: 271075[0m
     | > decoder_loss: 3.02710  (3.26832)
     | > postnet_loss: 4.81602  (5.36183)
     | > stopnet_loss: 1.45064  (1.88845)
     | > decoder_coarse_loss: 3.07053  (3.30095)
     | > decoder_ddc_loss: 0.00088  (0.00091)
     | > ga_loss: 0.00221  (0.00243)
     | > decoder_diff_spec_loss: 0.40166  (0.40716)
     | > postnet_diff_spec_loss: 0.85664  (0.86560)
     | > decoder_ssim_loss: 0.41165  (0.33031)
     | > postnet_ssim_loss: 0.46160  (0.36952)
     | > loss: 4.72322  (5.37675)
     | > align_error: 0.99344  (0.99293)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.13741  (4.22472)
     | > current_lr: 0.00003 
     | > step_time: 3.50510  (4.15600)
     | > loader_time: 0.01830  (0.03610)


[1m   --> STEP: 44/87 -- GLOBAL_STEP: 271100[0m
     | > decoder_loss: 2.86523  (3.26173)
     | > postnet_loss: 4.29390  (5.27592)
     | > stopnet_loss: 1.33109  (1.84126)
     | > decoder_coarse_loss: 2.91925  (3.29961)
     | > decoder_ddc_loss: 0.00085  (0.00093)
     | > ga_loss: 0.00179  (0.00256)
     | > decoder_diff_spec_loss: 0.38844  (0.41065)
     | > postnet_diff_spec_loss: 0.84190  (0.86852)
     | > decoder_ssim_loss: 0.40596  (0.34176)
     | > postnet_ssim_loss: 0.45603  (0.38289)
     | > loss: 4.38293  (5.31455)
     | > align_error: 0.99436  (0.99266)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 9.45004  (4.36256)
     | > current_lr: 0.00003 
     | > step_time: 5.36380  (3.94294)
     | > loader_time: 0.02050  (0.03083)


[1m   --> STEP: 69/87 -- GLOBAL_STEP: 271125[0m
     | > decoder_loss: 3.29641  (3.21390)
     | > postnet_loss: 5.22654  (5.16415)
     | > stopnet_loss: 2.34409  (1.85414)
     | > decoder_coarse_loss: 3.35823  (3.25565)
     | > decoder_ddc_loss: 0.00063  (0.00092)
     | > ga_loss: 0.00209  (0.00250)
     | > decoder_diff_spec_loss: 0.40125  (0.41009)
     | > postnet_diff_spec_loss: 0.84395  (0.86744)
     | > decoder_ssim_loss: 0.24958  (0.33435)
     | > postnet_ssim_loss: 0.28031  (0.37467)
     | > loss: 5.76877  (5.27192)
     | > align_error: 0.99458  (0.99288)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.08019  (4.06806)
     | > current_lr: 0.00003 
     | > step_time: 4.41060  (3.93906)
     | > loader_time: 0.02310  (0.03329)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 4.23564 [0m(+0.36700)
     | > avg_decoder_loss:[92m 3.24786 [0m(-0.20356)
     | > avg_postnet_loss:[92m 3.82491 [0m(-0.40411)
     | > avg_stopnet_loss:[91m 1.50202 [0m(+0.00468)
     | > avg_decoder_coarse_loss:[92m 3.16606 [0m(-0.14487)
     | > avg_decoder_ddc_loss:[91m 0.00194 [0m(+0.00031)
     | > avg_ga_loss:[91m 0.00222 [0m(+0.00007)
     | > avg_decoder_diff_spec_loss:[92m 0.39473 [0m(-0.00001)
     | > avg_postnet_diff_spec_loss:[92m 0.84847 [0m(-0.00025)
     | > avg_decoder_ssim_loss:[92m 0.34963 [0m(-0.00067)
     | > avg_postnet_ssim_loss:[92m 0.38780 [0m(-0.00084)
     | > avg_loss:[92m 4.56846 [0m(-0.18350)
     | > avg_align_error:[91m 0.99256 [0m(+0.00013)


[4m[1m > EPOCH: 13/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 11:16:33) [0m

[1m   --> STEP: 6/87 -- GLOBAL_STEP: 271150[0m
     | > decoder_loss: 3.00071  (3.21811)
     | > postnet_loss: 4.29014  (4.74433)
     | > stopnet_loss: 1.11604  (1.53827)
     | > decoder_coarse_loss: 3.09676  (3.29896)
     | > decoder_ddc_loss: 0.00095  (0.00102)
     | > ga_loss: 0.00161  (0.00242)
     | > decoder_diff_spec_loss: 0.39117  (0.41144)
     | > postnet_diff_spec_loss: 0.85290  (0.87335)
     | > decoder_ssim_loss: 0.45806  (0.38843)
     | > postnet_ssim_loss: 0.51078  (0.43378)
     | > loss: 4.27444  (4.89271)
     | > align_error: 0.99367  (0.99257)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.54525  (4.22180)
     | > current_lr: 0.00003 
     | > step_time: 4.26000  (4.51090)
     | > loader_time: 0.10420  (0.06485)


[1m   --> STEP: 31/87 -- GLOBAL_STEP: 271175[0m
     | > decoder_loss: 3.25778  (3.19383)
     | > postnet_loss: 4.73388  (4.83243)
     | > stopnet_loss: 1.57054  (1.88566)
     | > decoder_coarse_loss: 3.35358  (3.27683)
     | > decoder_ddc_loss: 0.00101  (0.00088)
     | > ga_loss: 0.00279  (0.00248)
     | > decoder_diff_spec_loss: 0.46891  (0.41210)
     | > postnet_diff_spec_loss: 0.92937  (0.86681)
     | > decoder_ssim_loss: 0.38177  (0.33397)
     | > postnet_ssim_loss: 0.42705  (0.37362)
     | > loss: 4.97282  (5.22066)
     | > align_error: 0.99253  (0.99325)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.95287  (4.12392)
     | > current_lr: 0.00003 
     | > step_time: 2.88840  (4.03227)
     | > loader_time: 0.03410  (0.03680)


[1m   --> STEP: 56/87 -- GLOBAL_STEP: 271200[0m
     | > decoder_loss: 2.93657  (3.13681)
     | > postnet_loss: 4.39739  (4.68340)
     | > stopnet_loss: 2.25682  (1.86650)
     | > decoder_coarse_loss: 3.05042  (3.22872)
     | > decoder_ddc_loss: 0.00047  (0.00091)
     | > ga_loss: 0.00190  (0.00254)
     | > decoder_diff_spec_loss: 0.42000  (0.41175)
     | > postnet_diff_spec_loss: 0.85669  (0.86639)
     | > decoder_ssim_loss: 0.27188  (0.33710)
     | > postnet_ssim_loss: 0.30404  (0.37750)
     | > loss: 5.32568  (5.13984)
     | > align_error: 0.99585  (0.99296)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.34290  (3.86927)
     | > current_lr: 0.00003 
     | > step_time: 4.50640  (3.84661)
     | > loader_time: 0.09120  (0.03755)


[1m   --> STEP: 81/87 -- GLOBAL_STEP: 271225[0m
     | > decoder_loss: 3.26621  (3.13437)
     | > postnet_loss: 4.34677  (4.64778)
     | > stopnet_loss: 2.51507  (1.88415)
     | > decoder_coarse_loss: 3.38542  (3.23778)
     | > decoder_ddc_loss: 0.00090  (0.00088)
     | > ga_loss: 0.00242  (0.00248)
     | > decoder_diff_spec_loss: 0.47583  (0.41462)
     | > postnet_diff_spec_loss: 0.93030  (0.86824)
     | > decoder_ssim_loss: 0.25195  (0.33152)
     | > postnet_ssim_loss: 0.28117  (0.37122)
     | > loss: 5.76180  (5.14815)
     | > align_error: 0.99380  (0.99324)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.15446  (3.65194)
     | > current_lr: 0.00003 
     | > step_time: 2.99170  (3.81226)
     | > loader_time: 0.01800  (0.03455)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 4.75322 [0m(+0.51758)
     | > avg_decoder_loss:[92m 3.15345 [0m(-0.09441)
     | > avg_postnet_loss:[92m 3.55294 [0m(-0.27197)
     | > avg_stopnet_loss:[91m 1.50583 [0m(+0.00382)
     | > avg_decoder_coarse_loss:[92m 3.08836 [0m(-0.07770)
     | > avg_decoder_ddc_loss:[91m 0.00253 [0m(+0.00059)
     | > avg_ga_loss:[91m 0.00236 [0m(+0.00014)
     | > avg_decoder_diff_spec_loss:[91m 0.40217 [0m(+0.00744)
     | > avg_postnet_diff_spec_loss:[92m 0.84824 [0m(-0.00023)
     | > avg_decoder_ssim_loss:[92m 0.34947 [0m(-0.00017)
     | > avg_postnet_ssim_loss:[92m 0.38707 [0m(-0.00074)
     | > avg_loss:[92m 4.46370 [0m(-0.10476)
     | > avg_align_error:[92m 0.99244 [0m(-0.00012)


[4m[1m > EPOCH: 14/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 11:24:00) [0m

[1m   --> STEP: 18/87 -- GLOBAL_STEP: 271250[0m
     | > decoder_loss: 3.20496  (3.07210)
     | > postnet_loss: 4.35078  (4.40684)
     | > stopnet_loss: 1.52012  (1.92554)
     | > decoder_coarse_loss: 3.38272  (3.23116)
     | > decoder_ddc_loss: 0.00135  (0.00109)
     | > ga_loss: 0.00226  (0.00252)
     | > decoder_diff_spec_loss: 0.46162  (0.41659)
     | > postnet_diff_spec_loss: 0.91960  (0.86605)
     | > decoder_ssim_loss: 0.36070  (0.32485)
     | > postnet_ssim_loss: 0.40273  (0.36308)
     | > loss: 4.80253  (5.10860)
     | > align_error: 0.99277  (0.99287)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.20720  (3.86551)
     | > current_lr: 0.00003 
     | > step_time: 4.17940  (4.33485)
     | > loader_time: 0.02020  (0.04639)


[1m   --> STEP: 43/87 -- GLOBAL_STEP: 271275[0m
     | > decoder_loss: 3.06578  (3.02987)
     | > postnet_loss: 4.14881  (4.36172)
     | > stopnet_loss: 1.75937  (1.86385)
     | > decoder_coarse_loss: 3.34330  (3.20902)
     | > decoder_ddc_loss: 0.00242  (0.00139)
     | > ga_loss: 0.00302  (0.00268)
     | > decoder_diff_spec_loss: 0.42664  (0.42359)
     | > postnet_diff_spec_loss: 0.86071  (0.86908)
     | > decoder_ssim_loss: 0.34056  (0.33937)
     | > postnet_ssim_loss: 0.38225  (0.37979)
     | > loss: 4.91707  (5.03069)
     | > align_error: 0.99041  (0.99225)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.37346  (3.65505)
     | > current_lr: 0.00003 
     | > step_time: 2.46850  (3.92330)
     | > loader_time: 0.02650  (0.03590)


[1m   --> STEP: 68/87 -- GLOBAL_STEP: 271300[0m
     | > decoder_loss: 3.23702  (2.95504)
     | > postnet_loss: 4.42362  (4.26320)
     | > stopnet_loss: 1.77781  (1.86062)
     | > decoder_coarse_loss: 3.49636  (3.14284)
     | > decoder_ddc_loss: 0.00194  (0.00153)
     | > ga_loss: 0.00227  (0.00262)
     | > decoder_diff_spec_loss: 0.49493  (0.42864)
     | > postnet_diff_spec_loss: 0.93032  (0.86779)
     | > decoder_ssim_loss: 0.33396  (0.33460)
     | > postnet_ssim_loss: 0.37313  (0.37470)
     | > loss: 5.11196  (4.96581)
     | > align_error: 0.99158  (0.99208)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.55611  (3.58215)
     | > current_lr: 0.00003 
     | > step_time: 4.17620  (3.93139)
     | > loader_time: 0.02480  (0.03379)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 7.33903 [0m(+2.58581)
     | > avg_decoder_loss:[92m 2.86868 [0m(-0.28477)
     | > avg_postnet_loss:[91m 4.20262 [0m(+0.64968)
     | > avg_stopnet_loss:[91m 1.54539 [0m(+0.03955)
     | > avg_decoder_coarse_loss:[92m 2.79645 [0m(-0.29191)
     | > avg_decoder_ddc_loss:[92m 0.00191 [0m(-0.00062)
     | > avg_ga_loss:[91m 0.00278 [0m(+0.00042)
     | > avg_decoder_diff_spec_loss:[91m 0.44124 [0m(+0.03907)
     | > avg_postnet_diff_spec_loss:[91m 0.85649 [0m(+0.00826)
     | > avg_decoder_ssim_loss:[92m 0.34775 [0m(-0.00171)
     | > avg_postnet_ssim_loss:[92m 0.38522 [0m(-0.00184)
     | > avg_loss:[91m 4.53437 [0m(+0.07068)
     | > avg_align_error:[92m 0.99124 [0m(-0.00120)


[4m[1m > EPOCH: 15/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 11:31:42) [0m

[1m   --> STEP: 5/87 -- GLOBAL_STEP: 271325[0m
     | > decoder_loss: 3.06187  (2.92845)
     | > postnet_loss: 4.08958  (4.14713)
     | > stopnet_loss: 1.34173  (1.67327)
     | > decoder_coarse_loss: 3.31354  (3.11656)
     | > decoder_ddc_loss: 0.00162  (0.00156)
     | > ga_loss: 0.00245  (0.00275)
     | > decoder_diff_spec_loss: 0.42484  (0.45239)
     | > postnet_diff_spec_loss: 0.85295  (0.87782)
     | > decoder_ssim_loss: 0.44252  (0.37237)
     | > postnet_ssim_loss: 0.49618  (0.41649)
     | > loss: 4.52477  (4.76521)
     | > align_error: 0.98951  (0.99032)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.87416  (3.83291)
     | > current_lr: 0.00003 
     | > step_time: 3.02780  (4.54128)
     | > loader_time: 0.04460  (0.04261)


[1m   --> STEP: 30/87 -- GLOBAL_STEP: 271350[0m
     | > decoder_loss: 3.36307  (2.85598)
     | > postnet_loss: 5.12507  (4.10260)
     | > stopnet_loss: 1.39391  (1.89154)
     | > decoder_coarse_loss: 3.62887  (3.01816)
     | > decoder_ddc_loss: 0.00133  (0.00153)
     | > ga_loss: 0.00192  (0.00260)
     | > decoder_diff_spec_loss: 0.45163  (0.44481)
     | > postnet_diff_spec_loss: 0.86699  (0.86416)
     | > decoder_ssim_loss: 0.38646  (0.33107)
     | > postnet_ssim_loss: 0.43129  (0.36990)
     | > loss: 4.96717  (4.90161)
     | > align_error: 0.99265  (0.99139)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.49152  (3.84771)
     | > current_lr: 0.00003 
     | > step_time: 3.91850  (4.07033)
     | > loader_time: 0.03260  (0.03793)


[1m   --> STEP: 55/87 -- GLOBAL_STEP: 271375[0m
     | > decoder_loss: 2.64403  (2.80910)
     | > postnet_loss: 3.50534  (3.98110)
     | > stopnet_loss: 1.54350  (1.85784)
     | > decoder_coarse_loss: 2.73112  (2.94527)
     | > decoder_ddc_loss: 0.00230  (0.00177)
     | > ga_loss: 0.00325  (0.00273)
     | > decoder_diff_spec_loss: 0.45184  (0.44729)
     | > postnet_diff_spec_loss: 0.86662  (0.86545)
     | > decoder_ssim_loss: 0.40285  (0.33702)
     | > postnet_ssim_loss: 0.44522  (0.37635)
     | > loss: 4.32207  (4.81232)
     | > align_error: 0.98835  (0.99080)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.85875  (3.77920)
     | > current_lr: 0.00003 
     | > step_time: 2.88180  (3.82904)
     | > loader_time: 0.02230  (0.03700)


[1m   --> STEP: 80/87 -- GLOBAL_STEP: 271400[0m
     | > decoder_loss: 2.82392  (2.80257)
     | > postnet_loss: 3.78388  (3.95129)
     | > stopnet_loss: 1.33309  (1.86620)
     | > decoder_coarse_loss: 2.90697  (2.92269)
     | > decoder_ddc_loss: 0.00184  (0.00179)
     | > ga_loss: 0.00226  (0.00263)
     | > decoder_diff_spec_loss: 0.43689  (0.44857)
     | > postnet_diff_spec_loss: 0.85859  (0.86571)
     | > decoder_ssim_loss: 0.39833  (0.33117)
     | > postnet_ssim_loss: 0.44266  (0.36927)
     | > loss: 4.25768  (4.80263)
     | > align_error: 0.99211  (0.99117)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.72340  (3.60892)
     | > current_lr: 0.00004 
     | > step_time: 2.96500  (3.86933)
     | > loader_time: 0.01860  (0.03608)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 4.66268 [0m(-2.67635)
     | > avg_decoder_loss:[92m 2.41319 [0m(-0.45549)
     | > avg_postnet_loss:[92m 3.60251 [0m(-0.60012)
     | > avg_stopnet_loss:[91m 1.57949 [0m(+0.03411)
     | > avg_decoder_coarse_loss:[92m 2.49236 [0m(-0.30409)
     | > avg_decoder_ddc_loss:[91m 0.00204 [0m(+0.00013)
     | > avg_ga_loss:[92m 0.00260 [0m(-0.00018)
     | > avg_decoder_diff_spec_loss:[92m 0.42630 [0m(-0.01494)
     | > avg_postnet_diff_spec_loss:[92m 0.84173 [0m(-0.01476)
     | > avg_decoder_ssim_loss:[92m 0.34579 [0m(-0.00197)
     | > avg_postnet_ssim_loss:[92m 0.37942 [0m(-0.00581)
     | > avg_loss:[92m 4.21834 [0m(-0.31603)
     | > avg_align_error:[92m 0.99098 [0m(-0.00026)


[4m[1m > EPOCH: 16/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 11:39:11) [0m

[1m   --> STEP: 17/87 -- GLOBAL_STEP: 271425[0m
     | > decoder_loss: 3.21512  (2.77460)
     | > postnet_loss: 4.09347  (3.75077)
     | > stopnet_loss: 1.93634  (1.97695)
     | > decoder_coarse_loss: 3.27629  (2.84074)
     | > decoder_ddc_loss: 0.00228  (0.00210)
     | > ga_loss: 0.00291  (0.00267)
     | > decoder_diff_spec_loss: 0.43683  (0.44431)
     | > postnet_diff_spec_loss: 0.85675  (0.85827)
     | > decoder_ssim_loss: 0.29018  (0.32134)
     | > postnet_ssim_loss: 0.32351  (0.35487)
     | > loss: 5.07449  (4.82707)
     | > align_error: 0.99068  (0.99077)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.03688  (3.65828)
     | > current_lr: 0.00004 
     | > step_time: 3.48560  (4.23577)
     | > loader_time: 0.01580  (0.03792)


[1m   --> STEP: 42/87 -- GLOBAL_STEP: 271450[0m
     | > decoder_loss: 2.93195  (2.76771)
     | > postnet_loss: 3.75053  (3.69616)
     | > stopnet_loss: 1.55415  (1.88890)
     | > decoder_coarse_loss: 3.00868  (2.81744)
     | > decoder_ddc_loss: 0.00244  (0.00214)
     | > ga_loss: 0.00305  (0.00277)
     | > decoder_diff_spec_loss: 0.44647  (0.45077)
     | > postnet_diff_spec_loss: 0.87582  (0.86393)
     | > decoder_ssim_loss: 0.40879  (0.33795)
     | > postnet_ssim_loss: 0.44685  (0.37289)
     | > loss: 4.53730  (4.73000)
     | > align_error: 0.98901  (0.99038)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.93979  (3.84168)
     | > current_lr: 0.00004 
     | > step_time: 2.58600  (3.87429)
     | > loader_time: 0.01760  (0.03305)


[1m   --> STEP: 67/87 -- GLOBAL_STEP: 271475[0m
     | > decoder_loss: 2.47787  (2.72771)
     | > postnet_loss: 2.94741  (3.58992)
     | > stopnet_loss: 1.63563  (1.87119)
     | > decoder_coarse_loss: 2.45958  (2.76060)
     | > decoder_ddc_loss: 0.00164  (0.00211)
     | > ga_loss: 0.00243  (0.00269)
     | > decoder_diff_spec_loss: 0.41290  (0.44908)
     | > postnet_diff_spec_loss: 0.82998  (0.86041)
     | > decoder_ssim_loss: 0.35704  (0.33338)
     | > postnet_ssim_loss: 0.38884  (0.36703)
     | > loss: 4.11657  (4.65719)
     | > align_error: 0.99287  (0.99059)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.37856  (3.57445)
     | > current_lr: 0.00004 
     | > step_time: 4.03860  (3.94230)
     | > loader_time: 0.02120  (0.03274)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 4.53082 [0m(-0.13186)
     | > avg_decoder_loss:[92m 2.37026 [0m(-0.04293)
     | > avg_postnet_loss:[92m 3.03899 [0m(-0.56352)
     | > avg_stopnet_loss:[92m 1.56728 [0m(-0.01221)
     | > avg_decoder_coarse_loss:[91m 2.62797 [0m(+0.13560)
     | > avg_decoder_ddc_loss:[91m 0.00234 [0m(+0.00030)
     | > avg_ga_loss:[92m 0.00250 [0m(-0.00010)
     | > avg_decoder_diff_spec_loss:[92m 0.42201 [0m(-0.00429)
     | > avg_postnet_diff_spec_loss:[92m 0.83422 [0m(-0.00751)
     | > avg_decoder_ssim_loss:[92m 0.34555 [0m(-0.00024)
     | > avg_postnet_ssim_loss:[92m 0.37235 [0m(-0.00706)
     | > avg_loss:[92m 4.08319 [0m(-0.13515)
     | > avg_align_error:[92m 0.99092 [0m(-0.00006)


[4m[1m > EPOCH: 17/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 11:46:52) [0m

[1m   --> STEP: 4/87 -- GLOBAL_STEP: 271500[0m
     | > decoder_loss: 2.90006  (2.75682)
     | > postnet_loss: 3.42731  (3.48605)
     | > stopnet_loss: 2.01607  (1.74383)
     | > decoder_coarse_loss: 2.91021  (2.77010)
     | > decoder_ddc_loss: 0.00284  (0.00233)
     | > ga_loss: 0.00351  (0.00275)
     | > decoder_diff_spec_loss: 0.47801  (0.46461)
     | > postnet_diff_spec_loss: 0.87975  (0.87467)
     | > decoder_ssim_loss: 0.35226  (0.35357)
     | > postnet_ssim_loss: 0.37958  (0.38586)
     | > loss: 4.86613  (4.53107)
     | > align_error: 0.98713  (0.98973)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 10.17102  (4.55165)
     | > current_lr: 0.00004 
     | > step_time: 2.92530  (4.79755)
     | > loader_time: 0.01820  (0.06059)


[1m   --> STEP: 29/87 -- GLOBAL_STEP: 271525[0m
     | > decoder_loss: 2.49681  (2.73247)
     | > postnet_loss: 3.05672  (3.39808)
     | > stopnet_loss: 1.80217  (1.89317)
     | > decoder_coarse_loss: 2.49644  (2.72114)
     | > decoder_ddc_loss: 0.00257  (0.00212)
     | > ga_loss: 0.00334  (0.00262)
     | > decoder_diff_spec_loss: 0.44676  (0.45346)
     | > postnet_diff_spec_loss: 0.83400  (0.85451)
     | > decoder_ssim_loss: 0.35478  (0.32818)
     | > postnet_ssim_loss: 0.38522  (0.35699)
     | > loss: 4.33721  (4.61801)
     | > align_error: 0.98889  (0.99059)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 7.11364  (4.20699)
     | > current_lr: 0.00004 
     | > step_time: 2.53320  (4.09259)
     | > loader_time: 0.02200  (0.03932)


[1m   --> STEP: 54/87 -- GLOBAL_STEP: 271550[0m
     | > decoder_loss: 2.73573  (2.71805)
     | > postnet_loss: 3.42347  (3.32229)
     | > stopnet_loss: 1.56269  (1.83864)
     | > decoder_coarse_loss: 2.73043  (2.70287)
     | > decoder_ddc_loss: 0.00194  (0.00227)
     | > ga_loss: 0.00225  (0.00269)
     | > decoder_diff_spec_loss: 0.43300  (0.45264)
     | > postnet_diff_spec_loss: 0.82305  (0.85536)
     | > decoder_ssim_loss: 0.35923  (0.33475)
     | > postnet_ssim_loss: 0.39041  (0.36375)
     | > loss: 4.29828  (4.54007)
     | > align_error: 0.99164  (0.99015)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.39075  (4.20428)
     | > current_lr: 0.00004 
     | > step_time: 4.28040  (3.91998)
     | > loader_time: 0.04340  (0.03795)


[1m   --> STEP: 79/87 -- GLOBAL_STEP: 271575[0m
     | > decoder_loss: 2.95964  (2.71539)
     | > postnet_loss: 3.37062  (3.29438)
     | > stopnet_loss: 2.08728  (1.84615)
     | > decoder_coarse_loss: 2.90204  (2.69580)
     | > decoder_ddc_loss: 0.00163  (0.00216)
     | > ga_loss: 0.00245  (0.00261)
     | > decoder_diff_spec_loss: 0.47205  (0.45438)
     | > postnet_diff_spec_loss: 0.86812  (0.85583)
     | > decoder_ssim_loss: 0.27660  (0.32941)
     | > postnet_ssim_loss: 0.29621  (0.35709)
     | > loss: 4.88626  (4.53530)
     | > align_error: 0.99244  (0.99049)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.68775  (3.84102)
     | > current_lr: 0.00004 
     | > step_time: 4.21640  (3.93126)
     | > loader_time: 0.02220  (0.03585)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.87896 [0m(-0.65186)
     | > avg_decoder_loss:[92m 2.35628 [0m(-0.01398)
     | > avg_postnet_loss:[91m 3.12512 [0m(+0.08613)
     | > avg_stopnet_loss:[92m 1.55185 [0m(-0.01543)
     | > avg_decoder_coarse_loss:[91m 2.71316 [0m(+0.08519)
     | > avg_decoder_ddc_loss:[91m 0.00238 [0m(+0.00004)
     | > avg_ga_loss:[92m 0.00246 [0m(-0.00004)
     | > avg_decoder_diff_spec_loss:[92m 0.41721 [0m(-0.00480)
     | > avg_postnet_diff_spec_loss:[92m 0.82798 [0m(-0.00623)
     | > avg_decoder_ssim_loss:[92m 0.34544 [0m(-0.00011)
     | > avg_postnet_ssim_loss:[92m 0.36507 [0m(-0.00729)
     | > avg_loss:[91m 4.10231 [0m(+0.01912)
     | > avg_align_error:[92m 0.99076 [0m(-0.00016)


[4m[1m > EPOCH: 18/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 11:54:33) [0m

[1m   --> STEP: 16/87 -- GLOBAL_STEP: 271600[0m
     | > decoder_loss: 2.42138  (2.68471)
     | > postnet_loss: 2.66634  (3.12209)
     | > stopnet_loss: 2.51222  (1.94072)
     | > decoder_coarse_loss: 2.36959  (2.63942)
     | > decoder_ddc_loss: 0.00284  (0.00218)
     | > ga_loss: 0.00303  (0.00259)
     | > decoder_diff_spec_loss: 0.44373  (0.44956)
     | > postnet_diff_spec_loss: 0.85725  (0.84761)
     | > decoder_ssim_loss: 0.26424  (0.32255)
     | > postnet_ssim_loss: 0.28582  (0.34509)
     | > loss: 4.85517  (4.55697)
     | > align_error: 0.98763  (0.99024)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.51972  (4.18080)
     | > current_lr: 0.00004 
     | > step_time: 4.11130  (4.29253)
     | > loader_time: 0.01780  (0.03857)


[1m   --> STEP: 41/87 -- GLOBAL_STEP: 271625[0m
     | > decoder_loss: 2.41768  (2.69873)
     | > postnet_loss: 2.75387  (3.11052)
     | > stopnet_loss: 2.70732  (1.86021)
     | > decoder_coarse_loss: 2.38312  (2.66237)
     | > decoder_ddc_loss: 0.00136  (0.00228)
     | > ga_loss: 0.00207  (0.00271)
     | > decoder_diff_spec_loss: 0.42478  (0.45631)
     | > postnet_diff_spec_loss: 0.84959  (0.85349)
     | > decoder_ssim_loss: 0.22550  (0.33531)
     | > postnet_ssim_loss: 0.24217  (0.35944)
     | > loss: 5.04218  (4.49339)
     | > align_error: 0.99329  (0.98984)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.53647  (4.47643)
     | > current_lr: 0.00004 
     | > step_time: 6.15430  (4.05460)
     | > loader_time: 0.02140  (0.03776)


[1m   --> STEP: 66/87 -- GLOBAL_STEP: 271650[0m
     | > decoder_loss: 2.85445  (2.66741)
     | > postnet_loss: 3.16879  (3.03778)
     | > stopnet_loss: 1.44913  (1.84671)
     | > decoder_coarse_loss: 2.78852  (2.62903)
     | > decoder_ddc_loss: 0.00216  (0.00222)
     | > ga_loss: 0.00254  (0.00265)
     | > decoder_diff_spec_loss: 0.46941  (0.45424)
     | > postnet_diff_spec_loss: 0.88100  (0.85107)
     | > decoder_ssim_loss: 0.37844  (0.33212)
     | > postnet_ssim_loss: 0.40734  (0.35578)
     | > loss: 4.19934  (4.44238)
     | > align_error: 0.98974  (0.98999)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.69924  (4.13410)
     | > current_lr: 0.00004 
     | > step_time: 4.17060  (4.01585)
     | > loader_time: 0.06520  (0.03588)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 4.98503 [0m(+1.10607)
     | > avg_decoder_loss:[92m 2.35249 [0m(-0.00379)
     | > avg_postnet_loss:[92m 2.46483 [0m(-0.66029)
     | > avg_stopnet_loss:[92m 1.54428 [0m(-0.00756)
     | > avg_decoder_coarse_loss:[91m 2.73036 [0m(+0.01720)
     | > avg_decoder_ddc_loss:[92m 0.00229 [0m(-0.00010)
     | > avg_ga_loss:[92m 0.00244 [0m(-0.00002)
     | > avg_decoder_diff_spec_loss:[92m 0.41129 [0m(-0.00592)
     | > avg_postnet_diff_spec_loss:[92m 0.82446 [0m(-0.00352)
     | > avg_decoder_ssim_loss:[92m 0.34506 [0m(-0.00038)
     | > avg_postnet_ssim_loss:[92m 0.36050 [0m(-0.00457)
     | > avg_loss:[92m 3.92929 [0m(-0.17302)
     | > avg_align_error:[91m 0.99078 [0m(+0.00002)


[4m[1m > EPOCH: 19/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 12:02:27) [0m

[1m   --> STEP: 3/87 -- GLOBAL_STEP: 271675[0m
     | > decoder_loss: 2.58570  (2.65841)
     | > postnet_loss: 2.68738  (3.02267)
     | > stopnet_loss: 2.01291  (1.69895)
     | > decoder_coarse_loss: 2.56934  (2.62718)
     | > decoder_ddc_loss: 0.00206  (0.00207)
     | > ga_loss: 0.00274  (0.00248)
     | > decoder_diff_spec_loss: 0.44915  (0.46528)
     | > postnet_diff_spec_loss: 0.85882  (0.86349)
     | > decoder_ssim_loss: 0.30721  (0.35402)
     | > postnet_ssim_loss: 0.32359  (0.37608)
     | > loss: 4.47241  (4.30366)
     | > align_error: 0.99016  (0.99007)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.36897  (3.27236)
     | > current_lr: 0.00004 
     | > step_time: 5.06590  (5.35176)
     | > loader_time: 0.01580  (0.06773)


[1m   --> STEP: 28/87 -- GLOBAL_STEP: 271700[0m
     | > decoder_loss: 2.43940  (2.66828)
     | > postnet_loss: 2.72746  (2.94899)
     | > stopnet_loss: 2.46885  (1.89230)
     | > decoder_coarse_loss: 2.36989  (2.62452)
     | > decoder_ddc_loss: 0.00162  (0.00202)
     | > ga_loss: 0.00254  (0.00257)
     | > decoder_diff_spec_loss: 0.46045  (0.45542)
     | > postnet_diff_spec_loss: 0.84136  (0.84655)
     | > decoder_ssim_loss: 0.24922  (0.32664)
     | > postnet_ssim_loss: 0.26614  (0.34663)
     | > loss: 4.82046  (4.45993)
     | > align_error: 0.99127  (0.99018)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.76274  (4.13245)
     | > current_lr: 0.00004 
     | > step_time: 5.08290  (4.08681)
     | > loader_time: 0.02450  (0.03728)


[1m   --> STEP: 53/87 -- GLOBAL_STEP: 271725[0m
     | > decoder_loss: 2.52896  (2.64652)
     | > postnet_loss: 2.53076  (2.88652)
     | > stopnet_loss: 1.83193  (1.85291)
     | > decoder_coarse_loss: 2.43769  (2.60259)
     | > decoder_ddc_loss: 0.00146  (0.00218)
     | > ga_loss: 0.00184  (0.00267)
     | > decoder_diff_spec_loss: 0.44137  (0.45677)
     | > postnet_diff_spec_loss: 0.82658  (0.84751)
     | > decoder_ssim_loss: 0.31415  (0.33379)
     | > postnet_ssim_loss: 0.32999  (0.35427)
     | > loss: 4.19385  (4.39880)
     | > align_error: 0.99240  (0.98967)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.30857  (4.21089)
     | > current_lr: 0.00004 
     | > step_time: 4.39890  (3.86704)
     | > loader_time: 0.02650  (0.03143)


[1m   --> STEP: 78/87 -- GLOBAL_STEP: 271750[0m
     | > decoder_loss: 2.97513  (2.63719)
     | > postnet_loss: 3.16741  (2.86821)
     | > stopnet_loss: 1.72697  (1.85246)
     | > decoder_coarse_loss: 2.88634  (2.59590)
     | > decoder_ddc_loss: 0.00160  (0.00205)
     | > ga_loss: 0.00211  (0.00259)
     | > decoder_diff_spec_loss: 0.49072  (0.45819)
     | > postnet_diff_spec_loss: 0.87927  (0.84741)
     | > decoder_ssim_loss: 0.32626  (0.32949)
     | > postnet_ssim_loss: 0.34542  (0.34936)
     | > loss: 4.50554  (4.38734)
     | > align_error: 0.99232  (0.99006)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.12454  (3.91178)
     | > current_lr: 0.00004 
     | > step_time: 3.94580  (3.91241)
     | > loader_time: 0.02050  (0.03224)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.97899 [0m(-1.00605)
     | > avg_decoder_loss:[92m 2.35119 [0m(-0.00130)
     | > avg_postnet_loss:[92m 2.41013 [0m(-0.05470)
     | > avg_stopnet_loss:[91m 1.54588 [0m(+0.00160)
     | > avg_decoder_coarse_loss:[91m 2.85031 [0m(+0.11995)
     | > avg_decoder_ddc_loss:[92m 0.00224 [0m(-0.00005)
     | > avg_ga_loss:[92m 0.00242 [0m(-0.00002)
     | > avg_decoder_diff_spec_loss:[92m 0.41104 [0m(-0.00025)
     | > avg_postnet_diff_spec_loss:[92m 0.82290 [0m(-0.00156)
     | > avg_decoder_ssim_loss:[92m 0.34488 [0m(-0.00018)
     | > avg_postnet_ssim_loss:[92m 0.35714 [0m(-0.00336)
     | > avg_loss:[91m 3.94542 [0m(+0.01613)
     | > avg_align_error:[92m 0.99075 [0m(-0.00003)


[4m[1m > EPOCH: 20/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 12:10:06) [0m

[1m   --> STEP: 15/87 -- GLOBAL_STEP: 271775[0m
     | > decoder_loss: 2.51901  (2.62839)
     | > postnet_loss: 2.61638  (2.80812)
     | > stopnet_loss: 2.52896  (1.91126)
     | > decoder_coarse_loss: 2.50190  (2.58668)
     | > decoder_ddc_loss: 0.00154  (0.00197)
     | > ga_loss: 0.00244  (0.00254)
     | > decoder_diff_spec_loss: 0.45001  (0.45614)
     | > postnet_diff_spec_loss: 0.85134  (0.83949)
     | > decoder_ssim_loss: 0.24027  (0.32567)
     | > postnet_ssim_loss: 0.25318  (0.34152)
     | > loss: 4.89959  (4.42093)
     | > align_error: 0.99256  (0.99015)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.86712  (3.67487)
     | > current_lr: 0.00004 
     | > step_time: 4.42090  (4.24710)
     | > loader_time: 0.02760  (0.04055)


[1m   --> STEP: 40/87 -- GLOBAL_STEP: 271800[0m
     | > decoder_loss: 2.49951  (2.63026)
     | > postnet_loss: 2.81333  (2.78770)
     | > stopnet_loss: 1.47214  (1.84114)
     | > decoder_coarse_loss: 2.46085  (2.59184)
     | > decoder_ddc_loss: 0.00260  (0.00206)
     | > ga_loss: 0.00312  (0.00270)
     | > decoder_diff_spec_loss: 0.44558  (0.46324)
     | > postnet_diff_spec_loss: 0.83396  (0.84630)
     | > decoder_ssim_loss: 0.45025  (0.33727)
     | > postnet_ssim_loss: 0.47820  (0.35513)
     | > loss: 3.98382  (4.35810)
     | > align_error: 0.98781  (0.98949)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.04102  (4.24566)
     | > current_lr: 0.00005 
     | > step_time: 2.59470  (3.80645)
     | > loader_time: 0.01990  (0.03188)


[1m   --> STEP: 65/87 -- GLOBAL_STEP: 271825[0m
     | > decoder_loss: 2.31900  (2.59660)
     | > postnet_loss: 2.36408  (2.71260)
     | > stopnet_loss: 1.75535  (1.84905)
     | > decoder_coarse_loss: 2.30089  (2.55616)
     | > decoder_ddc_loss: 0.00136  (0.00197)
     | > ga_loss: 0.00209  (0.00263)
     | > decoder_diff_spec_loss: 0.41985  (0.45926)
     | > postnet_diff_spec_loss: 0.83904  (0.84372)
     | > decoder_ssim_loss: 0.31378  (0.33067)
     | > postnet_ssim_loss: 0.33280  (0.34804)
     | > loss: 3.98850  (4.32447)
     | > align_error: 0.99267  (0.98975)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.89321  (4.06489)
     | > current_lr: 0.00005 
     | > step_time: 4.85990  (3.85570)
     | > loader_time: 0.02490  (0.03157)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 4.35719 [0m(+0.37820)
     | > avg_decoder_loss:[91m 2.41909 [0m(+0.06790)
     | > avg_postnet_loss:[92m 2.18333 [0m(-0.22680)
     | > avg_stopnet_loss:[91m 1.54866 [0m(+0.00278)
     | > avg_decoder_coarse_loss:[91m 2.98600 [0m(+0.13569)
     | > avg_decoder_ddc_loss:[92m 0.00205 [0m(-0.00019)
     | > avg_ga_loss:[92m 0.00241 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[92m 0.40965 [0m(-0.00139)
     | > avg_postnet_diff_spec_loss:[92m 0.82162 [0m(-0.00128)
     | > avg_decoder_ssim_loss:[92m 0.34477 [0m(-0.00011)
     | > avg_postnet_ssim_loss:[92m 0.35430 [0m(-0.00284)
     | > avg_loss:[92m 3.94092 [0m(-0.00450)
     | > avg_align_error:[91m 0.99095 [0m(+0.00019)


[4m[1m > EPOCH: 21/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 12:17:39) [0m

[1m   --> STEP: 2/87 -- GLOBAL_STEP: 271850[0m
     | > decoder_loss: 2.77234  (2.61688)
     | > postnet_loss: 2.94714  (2.84756)
     | > stopnet_loss: 1.49095  (1.46097)
     | > decoder_coarse_loss: 2.73872  (2.58233)
     | > decoder_ddc_loss: 0.00201  (0.00184)
     | > ga_loss: 0.00249  (0.00234)
     | > decoder_diff_spec_loss: 0.48832  (0.47443)
     | > postnet_diff_spec_loss: 0.87926  (0.86030)
     | > decoder_ssim_loss: 0.37037  (0.37613)
     | > postnet_ssim_loss: 0.39026  (0.39562)
     | > loss: 4.15051  (4.01143)
     | > align_error: 0.98915  (0.99001)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.14987  (2.45562)
     | > current_lr: 0.00005 
     | > step_time: 5.54100  (6.60794)
     | > loader_time: 0.02480  (0.02801)


[1m   --> STEP: 27/87 -- GLOBAL_STEP: 271875[0m
     | > decoder_loss: 2.55096  (2.61161)
     | > postnet_loss: 2.41296  (2.67606)
     | > stopnet_loss: 1.65746  (1.86692)
     | > decoder_coarse_loss: 2.46151  (2.55833)
     | > decoder_ddc_loss: 0.00153  (0.00178)
     | > ga_loss: 0.00240  (0.00255)
     | > decoder_diff_spec_loss: 0.46149  (0.46035)
     | > postnet_diff_spec_loss: 0.84395  (0.84061)
     | > decoder_ssim_loss: 0.35641  (0.32855)
     | > postnet_ssim_loss: 0.36943  (0.34336)
     | > loss: 4.03400  (4.33483)
     | > align_error: 0.99143  (0.99004)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.87362  (4.22313)
     | > current_lr: 0.00005 
     | > step_time: 3.32000  (4.16585)
     | > loader_time: 0.01910  (0.03846)


[1m   --> STEP: 52/87 -- GLOBAL_STEP: 271900[0m
     | > decoder_loss: 2.62237  (2.58780)
     | > postnet_loss: 2.57673  (2.62549)
     | > stopnet_loss: 1.73707  (1.84578)
     | > decoder_coarse_loss: 2.67099  (2.53699)
     | > decoder_ddc_loss: 0.00214  (0.00187)
     | > ga_loss: 0.00237  (0.00267)
     | > decoder_diff_spec_loss: 0.47892  (0.46115)
     | > postnet_diff_spec_loss: 0.83545  (0.84223)
     | > decoder_ssim_loss: 0.33361  (0.33326)
     | > postnet_ssim_loss: 0.34503  (0.34875)
     | > loss: 4.21525  (4.29349)
     | > align_error: 0.98869  (0.98952)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.18534  (4.24913)
     | > current_lr: 0.00005 
     | > step_time: 3.39460  (3.93433)
     | > loader_time: 0.06600  (0.04071)


[1m   --> STEP: 77/87 -- GLOBAL_STEP: 271925[0m
     | > decoder_loss: 2.49887  (2.57538)
     | > postnet_loss: 2.57658  (2.60447)
     | > stopnet_loss: 1.66722  (1.84308)
     | > decoder_coarse_loss: 2.48684  (2.52074)
     | > decoder_ddc_loss: 0.00130  (0.00176)
     | > ga_loss: 0.00233  (0.00257)
     | > decoder_diff_spec_loss: 0.46457  (0.46205)
     | > postnet_diff_spec_loss: 0.87239  (0.84149)
     | > decoder_ssim_loss: 0.34025  (0.32877)
     | > postnet_ssim_loss: 0.35597  (0.34359)
     | > loss: 4.07805  (4.27551)
     | > align_error: 0.99249  (0.98996)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.64639  (4.03039)
     | > current_lr: 0.00005 
     | > step_time: 3.38630  (3.92558)
     | > loader_time: 0.07340  (0.04099)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.96604 [0m(-0.39114)
     | > avg_decoder_loss:[92m 2.38049 [0m(-0.03860)
     | > avg_postnet_loss:[92m 2.08706 [0m(-0.09626)
     | > avg_stopnet_loss:[92m 1.54762 [0m(-0.00104)
     | > avg_decoder_coarse_loss:[92m 2.88005 [0m(-0.10595)
     | > avg_decoder_ddc_loss:[92m 0.00191 [0m(-0.00014)
     | > avg_ga_loss:[92m 0.00240 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[91m 0.41113 [0m(+0.00148)
     | > avg_postnet_diff_spec_loss:[92m 0.81954 [0m(-0.00208)
     | > avg_decoder_ssim_loss:[92m 0.34412 [0m(-0.00065)
     | > avg_postnet_ssim_loss:[92m 0.35219 [0m(-0.00210)
     | > avg_loss:[92m 3.87875 [0m(-0.06217)
     | > avg_align_error:[92m 0.99088 [0m(-0.00007)


[4m[1m > EPOCH: 22/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 12:25:26) [0m

[1m   --> STEP: 14/87 -- GLOBAL_STEP: 271950[0m
     | > decoder_loss: 2.86024  (2.57482)
     | > postnet_loss: 3.08269  (2.58174)
     | > stopnet_loss: 1.97565  (1.85759)
     | > decoder_coarse_loss: 2.81532  (2.50920)
     | > decoder_ddc_loss: 0.00155  (0.00174)
     | > ga_loss: 0.00232  (0.00253)
     | > decoder_diff_spec_loss: 0.47257  (0.46058)
     | > postnet_diff_spec_loss: 0.83293  (0.83457)
     | > decoder_ssim_loss: 0.30810  (0.33151)
     | > postnet_ssim_loss: 0.32280  (0.34315)
     | > loss: 4.66129  (4.27958)
     | > align_error: 0.99132  (0.98999)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.48596  (3.68549)
     | > current_lr: 0.00005 
     | > step_time: 3.37200  (4.39984)
     | > loader_time: 0.06570  (0.03136)


[1m   --> STEP: 39/87 -- GLOBAL_STEP: 271975[0m
     | > decoder_loss: 2.45629  (2.58222)
     | > postnet_loss: 2.31673  (2.57898)
     | > stopnet_loss: 1.59152  (1.83954)
     | > decoder_coarse_loss: 2.40931  (2.52361)
     | > decoder_ddc_loss: 0.00221  (0.00180)
     | > ga_loss: 0.00331  (0.00268)
     | > decoder_diff_spec_loss: 0.46355  (0.46711)
     | > postnet_diff_spec_loss: 0.84533  (0.84213)
     | > decoder_ssim_loss: 0.38909  (0.33365)
     | > postnet_ssim_loss: 0.40137  (0.34685)
     | > loss: 3.92903  (4.27206)
     | > align_error: 0.98652  (0.98956)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.58868  (4.21198)
     | > current_lr: 0.00005 
     | > step_time: 2.83370  (4.04652)
     | > loader_time: 0.01900  (0.03168)


[1m   --> STEP: 64/87 -- GLOBAL_STEP: 272000[0m
     | > decoder_loss: 2.55733  (2.54931)
     | > postnet_loss: 2.34403  (2.50896)
     | > stopnet_loss: 1.51488  (1.84965)
     | > decoder_coarse_loss: 2.50751  (2.48943)
     | > decoder_ddc_loss: 0.00191  (0.00175)
     | > ga_loss: 0.00253  (0.00263)
     | > decoder_diff_spec_loss: 0.47765  (0.46376)
     | > postnet_diff_spec_loss: 0.84026  (0.83961)
     | > decoder_ssim_loss: 0.36033  (0.33016)
     | > postnet_ssim_loss: 0.37369  (0.34340)
     | > loss: 3.89318  (4.24440)
     | > align_error: 0.98866  (0.98973)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.99803  (4.09180)
     | > current_lr: 0.00005 
     | > step_time: 3.78530  (4.03143)
     | > loader_time: 0.01920  (0.03188)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.99322 [0m(+0.02718)
     | > avg_decoder_loss:[91m 2.40339 [0m(+0.02290)
     | > avg_postnet_loss:[92m 2.06709 [0m(-0.01997)
     | > avg_stopnet_loss:[92m 1.54235 [0m(-0.00527)
     | > avg_decoder_coarse_loss:[92m 2.83003 [0m(-0.05002)
     | > avg_decoder_ddc_loss:[92m 0.00175 [0m(-0.00016)
     | > avg_ga_loss:[92m 0.00239 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.41052 [0m(-0.00060)
     | > avg_postnet_diff_spec_loss:[92m 0.81950 [0m(-0.00004)
     | > avg_decoder_ssim_loss:[92m 0.34383 [0m(-0.00029)
     | > avg_postnet_ssim_loss:[91m 0.35314 [0m(+0.00095)
     | > avg_loss:[92m 3.86161 [0m(-0.01714)
     | > avg_align_error:[91m 0.99095 [0m(+0.00007)


[4m[1m > EPOCH: 23/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 12:33:13) [0m

[1m   --> STEP: 1/87 -- GLOBAL_STEP: 272025[0m
     | > decoder_loss: 2.40835  (2.40835)
     | > postnet_loss: 2.52312  (2.52312)
     | > stopnet_loss: 1.39054  (1.39054)
     | > decoder_coarse_loss: 2.40925  (2.40925)
     | > decoder_ddc_loss: 0.00150  (0.00150)
     | > ga_loss: 0.00215  (0.00215)
     | > decoder_diff_spec_loss: 0.46451  (0.46451)
     | > postnet_diff_spec_loss: 0.83695  (0.83695)
     | > decoder_ssim_loss: 0.38030  (0.38030)
     | > postnet_ssim_loss: 0.39453  (0.39453)
     | > loss: 3.75590  (3.75590)
     | > align_error: 0.99086  (0.99086)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.83761  (3.83761)
     | > current_lr: 0.00005 
     | > step_time: 6.88680  (6.88676)
     | > loader_time: 0.07820  (0.07820)


[1m   --> STEP: 26/87 -- GLOBAL_STEP: 272050[0m
     | > decoder_loss: 2.80060  (2.55531)
     | > postnet_loss: 3.18293  (2.49664)
     | > stopnet_loss: 1.88353  (1.85941)
     | > decoder_coarse_loss: 2.73531  (2.48725)
     | > decoder_ddc_loss: 0.00111  (0.00164)
     | > ga_loss: 0.00201  (0.00254)
     | > decoder_diff_spec_loss: 0.48427  (0.46507)
     | > postnet_diff_spec_loss: 0.83613  (0.83608)
     | > decoder_ssim_loss: 0.29591  (0.32646)
     | > postnet_ssim_loss: 0.31020  (0.33754)
     | > loss: 4.55519  (4.24859)
     | > align_error: 0.99286  (0.99007)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.33007  (4.48663)
     | > current_lr: 0.00005 
     | > step_time: 4.58240  (4.30352)
     | > loader_time: 0.06410  (0.03741)


[1m   --> STEP: 51/87 -- GLOBAL_STEP: 272075[0m
     | > decoder_loss: 2.37000  (2.53007)
     | > postnet_loss: 2.39060  (2.44900)
     | > stopnet_loss: 1.91966  (1.84219)
     | > decoder_coarse_loss: 2.34610  (2.46382)
     | > decoder_ddc_loss: 0.00219  (0.00173)
     | > ga_loss: 0.00280  (0.00266)
     | > decoder_diff_spec_loss: 0.44224  (0.46552)
     | > postnet_diff_spec_loss: 0.83057  (0.83823)
     | > decoder_ssim_loss: 0.31899  (0.33202)
     | > postnet_ssim_loss: 0.33573  (0.34413)
     | > loss: 4.19277  (4.21161)
     | > align_error: 0.98837  (0.98965)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.95536  (4.23899)
     | > current_lr: 0.00005 
     | > step_time: 3.20510  (3.97606)
     | > loader_time: 0.01570  (0.03649)


[1m   --> STEP: 76/87 -- GLOBAL_STEP: 272100[0m
     | > decoder_loss: 2.51928  (2.51883)
     | > postnet_loss: 2.26010  (2.43093)
     | > stopnet_loss: 2.10076  (1.84063)
     | > decoder_coarse_loss: 2.47115  (2.45051)
     | > decoder_ddc_loss: 0.00122  (0.00164)
     | > ga_loss: 0.00219  (0.00257)
     | > decoder_diff_spec_loss: 0.47741  (0.46662)
     | > postnet_diff_spec_loss: 0.82871  (0.83700)
     | > decoder_ssim_loss: 0.27689  (0.32737)
     | > postnet_ssim_loss: 0.28325  (0.33898)
     | > loss: 4.39121  (4.19644)
     | > align_error: 0.99176  (0.99003)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.43560  (3.94001)
     | > current_lr: 0.00005 
     | > step_time: 4.55450  (3.99683)
     | > loader_time: 0.02750  (0.03823)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.92380 [0m(-0.06942)
     | > avg_decoder_loss:[92m 2.34049 [0m(-0.06290)
     | > avg_postnet_loss:[92m 2.01679 [0m(-0.05030)
     | > avg_stopnet_loss:[91m 1.54235 [0m(+0.00000)
     | > avg_decoder_coarse_loss:[92m 2.77644 [0m(-0.05359)
     | > avg_decoder_ddc_loss:[92m 0.00166 [0m(-0.00008)
     | > avg_ga_loss:[92m 0.00238 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[91m 0.41191 [0m(+0.00139)
     | > avg_postnet_diff_spec_loss:[92m 0.81656 [0m(-0.00294)
     | > avg_decoder_ssim_loss:[92m 0.34270 [0m(-0.00113)
     | > avg_postnet_ssim_loss:[92m 0.35160 [0m(-0.00154)
     | > avg_loss:[92m 3.81881 [0m(-0.04280)
     | > avg_align_error:[92m 0.99090 [0m(-0.00004)


[4m[1m > EPOCH: 24/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 12:40:52) [0m

[1m   --> STEP: 13/87 -- GLOBAL_STEP: 272125[0m
     | > decoder_loss: 2.72490  (2.49413)
     | > postnet_loss: 2.61835  (2.38821)
     | > stopnet_loss: 2.34909  (1.85041)
     | > decoder_coarse_loss: 2.60560  (2.43117)
     | > decoder_ddc_loss: 0.00191  (0.00162)
     | > ga_loss: 0.00288  (0.00254)
     | > decoder_diff_spec_loss: 0.46402  (0.46267)
     | > postnet_diff_spec_loss: 0.83189  (0.83149)
     | > decoder_ssim_loss: 0.26083  (0.33175)
     | > postnet_ssim_loss: 0.27091  (0.34129)
     | > loss: 4.80810  (4.18370)
     | > align_error: 0.98777  (0.98995)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.02131  (3.73999)
     | > current_lr: 0.00005 
     | > step_time: 4.01460  (4.40122)
     | > loader_time: 0.01750  (0.04427)


[1m   --> STEP: 38/87 -- GLOBAL_STEP: 272150[0m
     | > decoder_loss: 2.84238  (2.52510)
     | > postnet_loss: 2.88013  (2.42665)
     | > stopnet_loss: 1.84897  (1.84418)
     | > decoder_coarse_loss: 2.78297  (2.45960)
     | > decoder_ddc_loss: 0.00175  (0.00167)
     | > ga_loss: 0.00243  (0.00265)
     | > decoder_diff_spec_loss: 0.53086  (0.47200)
     | > postnet_diff_spec_loss: 0.87056  (0.83845)
     | > decoder_ssim_loss: 0.30874  (0.33073)
     | > postnet_ssim_loss: 0.31728  (0.34167)
     | > loss: 4.49477  (4.20639)
     | > align_error: 0.98850  (0.98969)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.22441  (4.83935)
     | > current_lr: 0.00005 
     | > step_time: 4.23270  (3.99663)
     | > loader_time: 0.01910  (0.03785)


[1m   --> STEP: 63/87 -- GLOBAL_STEP: 272175[0m
     | > decoder_loss: 2.30229  (2.48801)
     | > postnet_loss: 2.23497  (2.36137)
     | > stopnet_loss: 2.35048  (1.84935)
     | > decoder_coarse_loss: 2.27043  (2.42056)
     | > decoder_ddc_loss: 0.00154  (0.00165)
     | > ga_loss: 0.00286  (0.00262)
     | > decoder_diff_spec_loss: 0.46843  (0.46901)
     | > postnet_diff_spec_loss: 0.82621  (0.83579)
     | > decoder_ssim_loss: 0.25607  (0.32831)
     | > postnet_ssim_loss: 0.26431  (0.33926)
     | > loss: 4.52085  (4.17342)
     | > align_error: 0.98974  (0.98981)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 6.25974  (4.27585)
     | > current_lr: 0.00005 
     | > step_time: 4.15570  (3.96455)
     | > loader_time: 0.02290  (0.03381)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 4.30816 [0m(+0.38436)
     | > avg_decoder_loss:[92m 2.32303 [0m(-0.01746)
     | > avg_postnet_loss:[92m 1.96452 [0m(-0.05227)
     | > avg_stopnet_loss:[91m 1.54571 [0m(+0.00336)
     | > avg_decoder_coarse_loss:[92m 2.70619 [0m(-0.07025)
     | > avg_decoder_ddc_loss:[92m 0.00159 [0m(-0.00008)
     | > avg_ga_loss:[92m 0.00238 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.41498 [0m(+0.00307)
     | > avg_postnet_diff_spec_loss:[92m 0.81528 [0m(-0.00128)
     | > avg_decoder_ssim_loss:[92m 0.34177 [0m(-0.00093)
     | > avg_postnet_ssim_loss:[92m 0.35001 [0m(-0.00159)
     | > avg_loss:[92m 3.78695 [0m(-0.03186)
     | > avg_align_error:[91m 0.99098 [0m(+0.00008)


[4m[1m > EPOCH: 25/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 12:48:28) [0m

[1m   --> STEP: 0/87 -- GLOBAL_STEP: 272200[0m
     | > decoder_loss: 2.37356  (2.37356)
     | > postnet_loss: 2.23778  (2.23778)
     | > stopnet_loss: 1.12580  (1.12580)
     | > decoder_coarse_loss: 2.34307  (2.34307)
     | > decoder_ddc_loss: 0.00197  (0.00197)
     | > ga_loss: 0.00261  (0.00261)
     | > decoder_diff_spec_loss: 0.45185  (0.45185)
     | > postnet_diff_spec_loss: 0.83306  (0.83306)
     | > decoder_ssim_loss: 0.46765  (0.46765)
     | > postnet_ssim_loss: 0.48280  (0.48280)
     | > loss: 3.43677  (3.43677)
     | > align_error: 0.98849  (0.98849)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.57895  (1.57895)
     | > current_lr: 0.00006 
     | > step_time: 5.20490  (5.20489)
     | > loader_time: 6.70430  (6.70429)


[1m   --> STEP: 25/87 -- GLOBAL_STEP: 272225[0m
     | > decoder_loss: 2.86243  (2.47319)
     | > postnet_loss: 2.46843  (2.32789)
     | > stopnet_loss: 1.94944  (1.87255)
     | > decoder_coarse_loss: 2.73668  (2.41811)
     | > decoder_ddc_loss: 0.00138  (0.00160)
     | > ga_loss: 0.00232  (0.00255)
     | > decoder_diff_spec_loss: 0.51815  (0.47145)
     | > postnet_diff_spec_loss: 0.85276  (0.83227)
     | > decoder_ssim_loss: 0.28449  (0.32628)
     | > postnet_ssim_loss: 0.28624  (0.33576)
     | > loss: 4.46370  (4.18193)
     | > align_error: 0.99107  (0.98998)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.39710  (4.53738)
     | > current_lr: 0.00006 
     | > step_time: 4.43430  (4.19314)
     | > loader_time: 0.06930  (0.03447)


[1m   --> STEP: 50/87 -- GLOBAL_STEP: 272250[0m
     | > decoder_loss: 2.28199  (2.46281)
     | > postnet_loss: 2.06507  (2.31143)
     | > stopnet_loss: 1.69052  (1.84670)
     | > decoder_coarse_loss: 2.23045  (2.40510)
     | > decoder_ddc_loss: 0.00183  (0.00166)
     | > ga_loss: 0.00277  (0.00264)
     | > decoder_diff_spec_loss: 0.44809  (0.47457)
     | > postnet_diff_spec_loss: 0.82652  (0.83460)
     | > decoder_ssim_loss: 0.33071  (0.33078)
     | > postnet_ssim_loss: 0.34047  (0.34084)
     | > loss: 3.83567  (4.15037)
     | > align_error: 0.98923  (0.98967)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.03196  (4.60569)
     | > current_lr: 0.00006 
     | > step_time: 3.35700  (3.92287)
     | > loader_time: 0.02390  (0.03158)


[1m   --> STEP: 75/87 -- GLOBAL_STEP: 272275[0m
     | > decoder_loss: 2.53284  (2.44538)
     | > postnet_loss: 2.28798  (2.29372)
     | > stopnet_loss: 2.16581  (1.83360)
     | > decoder_coarse_loss: 2.44783  (2.38505)
     | > decoder_ddc_loss: 0.00129  (0.00159)
     | > ga_loss: 0.00239  (0.00256)
     | > decoder_diff_spec_loss: 0.49978  (0.47527)
     | > postnet_diff_spec_loss: 0.84541  (0.83336)
     | > decoder_ssim_loss: 0.26412  (0.32653)
     | > postnet_ssim_loss: 0.27025  (0.33634)
     | > loss: 4.46511  (4.12073)
     | > align_error: 0.99111  (0.99000)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.11047  (4.26347)
     | > current_lr: 0.00006 
     | > step_time: 4.58480  (3.98408)
     | > loader_time: 0.03920  (0.03279)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 4.33667 [0m(+0.02851)
     | > avg_decoder_loss:[92m 2.30173 [0m(-0.02130)
     | > avg_postnet_loss:[91m 2.00991 [0m(+0.04539)
     | > avg_stopnet_loss:[92m 1.54256 [0m(-0.00316)
     | > avg_decoder_coarse_loss:[91m 2.76661 [0m(+0.06042)
     | > avg_decoder_ddc_loss:[91m 0.00159 [0m(+0.00000)
     | > avg_ga_loss:[92m 0.00238 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.42101 [0m(+0.00603)
     | > avg_postnet_diff_spec_loss:[91m 0.81565 [0m(+0.00037)
     | > avg_decoder_ssim_loss:[92m 0.34046 [0m(-0.00132)
     | > avg_postnet_ssim_loss:[92m 0.34970 [0m(-0.00030)
     | > avg_loss:[91m 3.80610 [0m(+0.01915)
     | > avg_align_error:[91m 0.99102 [0m(+0.00004)


[4m[1m > EPOCH: 26/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 12:56:05) [0m

[1m   --> STEP: 12/87 -- GLOBAL_STEP: 272300[0m
     | > decoder_loss: 2.51735  (2.39789)
     | > postnet_loss: 2.26288  (2.22569)
     | > stopnet_loss: 2.41683  (1.80189)
     | > decoder_coarse_loss: 2.44802  (2.33228)
     | > decoder_ddc_loss: 0.00127  (0.00156)
     | > ga_loss: 0.00253  (0.00250)
     | > decoder_diff_spec_loss: 0.52252  (0.47301)
     | > postnet_diff_spec_loss: 0.84396  (0.82721)
     | > decoder_ssim_loss: 0.24625  (0.33587)
     | > postnet_ssim_loss: 0.24783  (0.34357)
     | > loss: 4.70198  (4.04866)
     | > align_error: 0.99220  (0.99015)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.64829  (4.00877)
     | > current_lr: 0.00006 
     | > step_time: 4.94280  (4.51951)
     | > loader_time: 0.02260  (0.05480)


[1m   --> STEP: 37/87 -- GLOBAL_STEP: 272325[0m
     | > decoder_loss: 2.67897  (2.43741)
     | > postnet_loss: 2.42316  (2.26856)
     | > stopnet_loss: 1.64181  (1.83051)
     | > decoder_coarse_loss: 2.60799  (2.37842)
     | > decoder_ddc_loss: 0.00143  (0.00162)
     | > ga_loss: 0.00264  (0.00265)
     | > decoder_diff_spec_loss: 0.48284  (0.48176)
     | > postnet_diff_spec_loss: 0.83420  (0.83284)
     | > decoder_ssim_loss: 0.34593  (0.32975)
     | > postnet_ssim_loss: 0.36023  (0.33859)
     | > loss: 4.08871  (4.11101)
     | > align_error: 0.99089  (0.98974)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.55187  (5.16509)
     | > current_lr: 0.00006 
     | > step_time: 3.14250  (4.01699)
     | > loader_time: 0.01390  (0.03970)


[1m   --> STEP: 62/87 -- GLOBAL_STEP: 272350[0m
     | > decoder_loss: 2.08712  (2.40986)
     | > postnet_loss: 1.74509  (2.21858)
     | > stopnet_loss: 2.10845  (1.82970)
     | > decoder_coarse_loss: 2.04684  (2.35152)
     | > decoder_ddc_loss: 0.00130  (0.00161)
     | > ga_loss: 0.00228  (0.00261)
     | > decoder_diff_spec_loss: 0.44084  (0.47964)
     | > postnet_diff_spec_loss: 0.79793  (0.83145)
     | > decoder_ssim_loss: 0.27563  (0.32761)
     | > postnet_ssim_loss: 0.28264  (0.33687)
     | > loss: 4.03918  (4.08204)
     | > align_error: 0.99202  (0.98983)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.30579  (4.63152)
     | > current_lr: 0.00006 
     | > step_time: 4.45950  (3.94525)
     | > loader_time: 0.02450  (0.03445)


[1m   --> STEP: 87/87 -- GLOBAL_STEP: 272375[0m
     | > decoder_loss: 2.08092  (2.40048)
     | > postnet_loss: 1.92055  (2.21533)
     | > stopnet_loss: 1.04891  (1.82699)
     | > decoder_coarse_loss: 2.04101  (2.34875)
     | > decoder_ddc_loss: 0.00153  (0.00155)
     | > ga_loss: 0.00182  (0.00252)
     | > decoder_diff_spec_loss: 0.48122  (0.48280)
     | > postnet_diff_spec_loss: 0.83720  (0.83186)
     | > decoder_ssim_loss: 0.42732  (0.32649)
     | > postnet_ssim_loss: 0.42978  (0.33552)
     | > loss: 3.11289  (4.07528)
     | > align_error: 0.99044  (0.99017)
     | > amp_scaler: 65536.00000  (33521.28736)
     | > grad_norm: 3.72413  (4.30856)
     | > current_lr: 0.00006 
     | > step_time: 2.51300  (3.83734)
     | > loader_time: 0.00700  (0.03385)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 4.70502 [0m(+0.36835)
     | > avg_decoder_loss:[92m 2.21003 [0m(-0.09169)
     | > avg_postnet_loss:[92m 1.98186 [0m(-0.02805)
     | > avg_stopnet_loss:[91m 1.54963 [0m(+0.00707)
     | > avg_decoder_coarse_loss:[92m 2.63870 [0m(-0.12791)
     | > avg_decoder_ddc_loss:[92m 0.00155 [0m(-0.00003)
     | > avg_ga_loss:[92m 0.00237 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.42724 [0m(+0.00623)
     | > avg_postnet_diff_spec_loss:[92m 0.81283 [0m(-0.00282)
     | > avg_decoder_ssim_loss:[92m 0.33808 [0m(-0.00238)
     | > avg_postnet_ssim_loss:[92m 0.34761 [0m(-0.00209)
     | > avg_loss:[92m 3.75098 [0m(-0.05512)
     | > avg_align_error:[92m 0.99099 [0m(-0.00003)


[4m[1m > EPOCH: 27/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 13:03:44) [0m

[1m   --> STEP: 24/87 -- GLOBAL_STEP: 272400[0m
     | > decoder_loss: 2.63548  (2.35835)
     | > postnet_loss: 2.18671  (2.17818)
     | > stopnet_loss: 1.96944  (1.85182)
     | > decoder_coarse_loss: 2.61745  (2.33476)
     | > decoder_ddc_loss: 0.00134  (0.00159)
     | > ga_loss: 0.00266  (0.00256)
     | > decoder_diff_spec_loss: 0.53715  (0.48001)
     | > postnet_diff_spec_loss: 0.87088  (0.82682)
     | > decoder_ssim_loss: 0.28309  (0.32532)
     | > postnet_ssim_loss: 0.28540  (0.33389)
     | > loss: 4.33710  (4.07434)
     | > align_error: 0.99033  (0.98995)
     | > amp_scaler: 65536.00000  (65536.00000)
     | > grad_norm: 2.65959  (4.44563)
     | > current_lr: 0.00006 
     | > step_time: 4.06790  (4.07474)
     | > loader_time: 0.02330  (0.03790)


[1m   --> STEP: 49/87 -- GLOBAL_STEP: 272425[0m
     | > decoder_loss: 2.28933  (2.36508)
     | > postnet_loss: 1.99330  (2.16970)
     | > stopnet_loss: 1.78766  (1.83562)
     | > decoder_coarse_loss: 2.21974  (2.33640)
     | > decoder_ddc_loss: 0.00149  (0.00163)
     | > ga_loss: 0.00247  (0.00264)
     | > decoder_diff_spec_loss: 0.47010  (0.48648)
     | > postnet_diff_spec_loss: 0.80998  (0.83009)
     | > decoder_ssim_loss: 0.31187  (0.32799)
     | > postnet_ssim_loss: 0.32182  (0.33696)
     | > loss: 3.90443  (4.06239)
     | > align_error: 0.99062  (0.98972)
     | > amp_scaler: 32768.00000  (56173.71429)
     | > grad_norm: 2.40160  (4.06717)
     | > current_lr: 0.00006 
     | > step_time: 3.81400  (3.91541)
     | > loader_time: 0.02420  (0.03649)


[1m   --> STEP: 74/87 -- GLOBAL_STEP: 272450[0m
     | > decoder_loss: 2.38262  (2.33926)
     | > postnet_loss: 2.11076  (2.14618)
     | > stopnet_loss: 1.51067  (1.82150)
     | > decoder_coarse_loss: 2.31653  (2.31408)
     | > decoder_ddc_loss: 0.00118  (0.00157)
     | > ga_loss: 0.00208  (0.00256)
     | > decoder_diff_spec_loss: 0.49221  (0.48649)
     | > postnet_diff_spec_loss: 0.81270  (0.82864)
     | > decoder_ssim_loss: 0.32867  (0.32444)
     | > postnet_ssim_loss: 0.33807  (0.33328)
     | > loss: 3.71674  (4.02778)
     | > align_error: 0.99284  (0.99003)
     | > amp_scaler: 32768.00000  (48266.37838)
     | > grad_norm: 2.91634  (3.84990)
     | > current_lr: 0.00006 
     | > step_time: 5.05600  (3.94623)
     | > loader_time: 0.02670  (0.03592)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 4.17138 [0m(-0.53364)
     | > avg_decoder_loss:[92m 2.17393 [0m(-0.03610)
     | > avg_postnet_loss:[91m 1.98495 [0m(+0.00309)
     | > avg_stopnet_loss:[92m 1.54336 [0m(-0.00627)
     | > avg_decoder_coarse_loss:[91m 2.66369 [0m(+0.02499)
     | > avg_decoder_ddc_loss:[92m 0.00152 [0m(-0.00004)
     | > avg_ga_loss:[92m 0.00237 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.43422 [0m(+0.00697)
     | > avg_postnet_diff_spec_loss:[92m 0.81124 [0m(-0.00159)
     | > avg_decoder_ssim_loss:[92m 0.33621 [0m(-0.00186)
     | > avg_postnet_ssim_loss:[92m 0.34646 [0m(-0.00115)
     | > avg_loss:[92m 3.74328 [0m(-0.00770)
     | > avg_align_error:[92m 0.99097 [0m(-0.00002)


[4m[1m > EPOCH: 28/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 13:11:28) [0m

[1m   --> STEP: 11/87 -- GLOBAL_STEP: 272475[0m
     | > decoder_loss: 2.32524  (2.27652)
     | > postnet_loss: 2.23491  (2.07193)
     | > stopnet_loss: 1.31252  (1.71699)
     | > decoder_coarse_loss: 2.26349  (2.26475)
     | > decoder_ddc_loss: 0.00157  (0.00155)
     | > ga_loss: 0.00237  (0.00250)
     | > decoder_diff_spec_loss: 0.46395  (0.48094)
     | > postnet_diff_spec_loss: 0.79891  (0.82195)
     | > decoder_ssim_loss: 0.40415  (0.34050)
     | > postnet_ssim_loss: 0.42080  (0.34834)
     | > loss: 3.55265  (3.88111)
     | > align_error: 0.99057  (0.98997)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.68132  (3.76244)
     | > current_lr: 0.00006 
     | > step_time: 4.18040  (4.50421)
     | > loader_time: 0.02100  (0.04381)


[1m   --> STEP: 36/87 -- GLOBAL_STEP: 272500[0m
     | > decoder_loss: 2.27364  (2.31324)
     | > postnet_loss: 1.92951  (2.11832)
     | > stopnet_loss: 1.66841  (1.83078)
     | > decoder_coarse_loss: 2.25063  (2.30552)
     | > decoder_ddc_loss: 0.00350  (0.00161)
     | > ga_loss: 0.00503  (0.00264)
     | > decoder_diff_spec_loss: 0.49318  (0.49374)
     | > postnet_diff_spec_loss: 0.83550  (0.82847)
     | > decoder_ssim_loss: 0.44646  (0.32578)
     | > postnet_ssim_loss: 0.45744  (0.33379)
     | > loss: 3.86601  (4.02412)
     | > align_error: 0.97918  (0.98970)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 14.25918  (4.83428)
     | > current_lr: 0.00006 
     | > step_time: 1.89800  (4.01963)
     | > loader_time: 0.02280  (0.03573)


[1m   --> STEP: 61/87 -- GLOBAL_STEP: 272525[0m
     | > decoder_loss: 2.10705  (2.29461)
     | > postnet_loss: 1.87540  (2.08713)
     | > stopnet_loss: 1.74995  (1.82322)
     | > decoder_coarse_loss: 2.11419  (2.28849)
     | > decoder_ddc_loss: 0.00168  (0.00158)
     | > ga_loss: 0.00300  (0.00261)
     | > decoder_diff_spec_loss: 0.44029  (0.49159)
     | > postnet_diff_spec_loss: 0.79539  (0.82740)
     | > decoder_ssim_loss: 0.31354  (0.32495)
     | > postnet_ssim_loss: 0.32302  (0.33363)
     | > loss: 3.75757  (3.99860)
     | > align_error: 0.98853  (0.98979)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.04733  (4.26983)
     | > current_lr: 0.00006 
     | > step_time: 3.63750  (3.95185)
     | > loader_time: 0.03490  (0.03445)


[1m   --> STEP: 86/87 -- GLOBAL_STEP: 272550[0m
     | > decoder_loss: 2.22962  (2.28647)
     | > postnet_loss: 1.96773  (2.08335)
     | > stopnet_loss: 0.80070  (1.82675)
     | > decoder_coarse_loss: 2.26745  (2.28044)
     | > decoder_ddc_loss: 0.00214  (0.00151)
     | > ga_loss: 0.00275  (0.00252)
     | > decoder_diff_spec_loss: 0.49531  (0.49378)
     | > postnet_diff_spec_loss: 0.81799  (0.82731)
     | > decoder_ssim_loss: 0.56584  (0.32195)
     | > postnet_ssim_loss: 0.58619  (0.33035)
     | > loss: 3.04753  (3.99564)
     | > align_error: 0.98740  (0.99016)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.21337  (4.05529)
     | > current_lr: 0.00006 
     | > step_time: 1.88290  (3.85477)
     | > loader_time: 0.01510  (0.03290)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 4.40969 [0m(+0.23831)
     | > avg_decoder_loss:[92m 2.05829 [0m(-0.11564)
     | > avg_postnet_loss:[92m 1.97905 [0m(-0.00590)
     | > avg_stopnet_loss:[91m 1.54883 [0m(+0.00547)
     | > avg_decoder_coarse_loss:[92m 2.53645 [0m(-0.12725)
     | > avg_decoder_ddc_loss:[92m 0.00149 [0m(-0.00003)
     | > avg_ga_loss:[92m 0.00237 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[91m 0.43667 [0m(+0.00245)
     | > avg_postnet_diff_spec_loss:[92m 0.81088 [0m(-0.00036)
     | > avg_decoder_ssim_loss:[92m 0.33390 [0m(-0.00231)
     | > avg_postnet_ssim_loss:[92m 0.34479 [0m(-0.00167)
     | > avg_loss:[92m 3.68604 [0m(-0.05724)
     | > avg_align_error:[92m 0.99087 [0m(-0.00010)


[4m[1m > EPOCH: 29/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 13:19:06) [0m

[1m   --> STEP: 23/87 -- GLOBAL_STEP: 272575[0m
     | > decoder_loss: 2.29321  (2.24456)
     | > postnet_loss: 2.23030  (2.04925)
     | > stopnet_loss: 1.14651  (1.83627)
     | > decoder_coarse_loss: 2.24353  (2.24994)
     | > decoder_ddc_loss: 0.00156  (0.00151)
     | > ga_loss: 0.00258  (0.00255)
     | > decoder_diff_spec_loss: 0.47548  (0.48804)
     | > postnet_diff_spec_loss: 0.81842  (0.82108)
     | > decoder_ssim_loss: 0.46163  (0.32409)
     | > postnet_ssim_loss: 0.47966  (0.33218)
     | > loss: 3.41037  (3.97670)
     | > align_error: 0.98960  (0.98987)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 6.70015  (4.55917)
     | > current_lr: 0.00006 
     | > step_time: 2.57890  (4.14489)
     | > loader_time: 0.02170  (0.03375)


[1m   --> STEP: 48/87 -- GLOBAL_STEP: 272600[0m
     | > decoder_loss: 2.25901  (2.26247)
     | > postnet_loss: 1.96063  (2.05311)
     | > stopnet_loss: 2.55585  (1.82295)
     | > decoder_coarse_loss: 2.26570  (2.26529)
     | > decoder_ddc_loss: 0.00141  (0.00156)
     | > ga_loss: 0.00282  (0.00264)
     | > decoder_diff_spec_loss: 0.52644  (0.49544)
     | > postnet_diff_spec_loss: 0.86557  (0.82636)
     | > decoder_ssim_loss: 0.23805  (0.32515)
     | > postnet_ssim_loss: 0.23941  (0.33361)
     | > loss: 4.65899  (3.97689)
     | > align_error: 0.98989  (0.98962)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.10739  (4.66935)
     | > current_lr: 0.00006 
     | > step_time: 3.99430  (3.97219)
     | > loader_time: 0.13550  (0.03904)


[1m   --> STEP: 73/87 -- GLOBAL_STEP: 272625[0m
     | > decoder_loss: 2.22908  (2.23881)
     | > postnet_loss: 1.94447  (2.03684)
     | > stopnet_loss: 1.42401  (1.81179)
     | > decoder_coarse_loss: 2.20409  (2.23896)
     | > decoder_ddc_loss: 0.00189  (0.00151)
     | > ga_loss: 0.00281  (0.00256)
     | > decoder_diff_spec_loss: 0.50387  (0.49471)
     | > postnet_diff_spec_loss: 0.82331  (0.82470)
     | > decoder_ssim_loss: 0.39992  (0.32132)
     | > postnet_ssim_loss: 0.40503  (0.32968)
     | > loss: 3.56598  (3.94624)
     | > align_error: 0.98763  (0.98992)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.47029  (4.30675)
     | > current_lr: 0.00007 
     | > step_time: 3.61590  (3.99511)
     | > loader_time: 0.05590  (0.03846)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 4.27487 [0m(-0.13482)
     | > avg_decoder_loss:[92m 2.04040 [0m(-0.01788)
     | > avg_postnet_loss:[92m 1.92283 [0m(-0.05622)
     | > avg_stopnet_loss:[92m 1.54312 [0m(-0.00571)
     | > avg_decoder_coarse_loss:[92m 2.47725 [0m(-0.05919)
     | > avg_decoder_ddc_loss:[92m 0.00145 [0m(-0.00004)
     | > avg_ga_loss:[92m 0.00237 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.43706 [0m(+0.00039)
     | > avg_postnet_diff_spec_loss:[92m 0.80840 [0m(-0.00248)
     | > avg_decoder_ssim_loss:[92m 0.33264 [0m(-0.00127)
     | > avg_postnet_ssim_loss:[91m 0.34485 [0m(+0.00006)
     | > avg_loss:[92m 3.64617 [0m(-0.03987)
     | > avg_align_error:[91m 0.99096 [0m(+0.00009)


[4m[1m > EPOCH: 30/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 13:26:45) [0m

[1m   --> STEP: 10/87 -- GLOBAL_STEP: 272650[0m
     | > decoder_loss: 2.07820  (2.18174)
     | > postnet_loss: 1.97222  (1.96860)
     | > stopnet_loss: 2.23789  (1.77251)
     | > decoder_coarse_loss: 2.11697  (2.17555)
     | > decoder_ddc_loss: 0.00142  (0.00146)
     | > ga_loss: 0.00246  (0.00251)
     | > decoder_diff_spec_loss: 0.48820  (0.48771)
     | > postnet_diff_spec_loss: 0.79892  (0.82086)
     | > decoder_ssim_loss: 0.26091  (0.33043)
     | > postnet_ssim_loss: 0.26774  (0.33788)
     | > loss: 4.24636  (3.86112)
     | > align_error: 0.98945  (0.98990)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.01003  (3.82874)
     | > current_lr: 0.00007 
     | > step_time: 4.83860  (4.32824)
     | > loader_time: 0.08110  (0.05622)


[1m   --> STEP: 35/87 -- GLOBAL_STEP: 272675[0m
     | > decoder_loss: 2.46572  (2.23234)
     | > postnet_loss: 2.17676  (2.03692)
     | > stopnet_loss: 1.72374  (1.83956)
     | > decoder_coarse_loss: 2.45515  (2.22840)
     | > decoder_ddc_loss: 0.00152  (0.00150)
     | > ga_loss: 0.00254  (0.00258)
     | > decoder_diff_spec_loss: 0.55268  (0.50004)
     | > postnet_diff_spec_loss: 0.88666  (0.82512)
     | > decoder_ssim_loss: 0.33829  (0.31947)
     | > postnet_ssim_loss: 0.34615  (0.32745)
     | > loss: 4.04215  (3.97025)
     | > align_error: 0.98911  (0.98991)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.60329  (4.76447)
     | > current_lr: 0.00007 
     | > step_time: 4.44420  (4.08358)
     | > loader_time: 0.02820  (0.03807)


[1m   --> STEP: 60/87 -- GLOBAL_STEP: 272700[0m
     | > decoder_loss: 2.24173  (2.21502)
     | > postnet_loss: 2.24068  (2.00737)
     | > stopnet_loss: 1.65921  (1.81687)
     | > decoder_coarse_loss: 2.25233  (2.21416)
     | > decoder_ddc_loss: 0.00115  (0.00153)
     | > ga_loss: 0.00203  (0.00260)
     | > decoder_diff_spec_loss: 0.51988  (0.49813)
     | > postnet_diff_spec_loss: 0.83351  (0.82472)
     | > decoder_ssim_loss: 0.30901  (0.32236)
     | > postnet_ssim_loss: 0.32140  (0.33102)
     | > loss: 3.84926  (3.93345)
     | > align_error: 0.99142  (0.98970)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.38052  (4.49612)
     | > current_lr: 0.00007 
     | > step_time: 4.58960  (3.87617)
     | > loader_time: 0.01990  (0.03429)


[1m   --> STEP: 85/87 -- GLOBAL_STEP: 272725[0m
     | > decoder_loss: 2.33758  (2.20875)
     | > postnet_loss: 2.37783  (2.00274)
     | > stopnet_loss: 2.06634  (1.83267)
     | > decoder_coarse_loss: 2.37490  (2.20596)
     | > decoder_ddc_loss: 0.00127  (0.00145)
     | > ga_loss: 0.00208  (0.00252)
     | > decoder_diff_spec_loss: 0.51182  (0.49925)
     | > postnet_diff_spec_loss: 0.81739  (0.82423)
     | > decoder_ssim_loss: 0.27660  (0.31639)
     | > postnet_ssim_loss: 0.28635  (0.32473)
     | > loss: 4.32267  (3.94113)
     | > align_error: 0.99102  (0.99010)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.92128  (4.14402)
     | > current_lr: 0.00007 
     | > step_time: 3.14780  (3.86034)
     | > loader_time: 0.01860  (0.03165)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 5.87443 [0m(+1.59956)
     | > avg_decoder_loss:[91m 2.04728 [0m(+0.00688)
     | > avg_postnet_loss:[92m 1.88072 [0m(-0.04210)
     | > avg_stopnet_loss:[92m 1.53876 [0m(-0.00435)
     | > avg_decoder_coarse_loss:[91m 2.48920 [0m(+0.01195)
     | > avg_decoder_ddc_loss:[92m 0.00135 [0m(-0.00010)
     | > avg_ga_loss:[92m 0.00236 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.44009 [0m(+0.00303)
     | > avg_postnet_diff_spec_loss:[92m 0.80782 [0m(-0.00058)
     | > avg_decoder_ssim_loss:[92m 0.33158 [0m(-0.00106)
     | > avg_postnet_ssim_loss:[92m 0.34303 [0m(-0.00182)
     | > avg_loss:[92m 3.63585 [0m(-0.01031)
     | > avg_align_error:[92m 0.99086 [0m(-0.00010)


[4m[1m > EPOCH: 31/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 13:34:20) [0m

[1m   --> STEP: 22/87 -- GLOBAL_STEP: 272750[0m
     | > decoder_loss: 1.86902  (2.19013)
     | > postnet_loss: 1.96587  (1.98259)
     | > stopnet_loss: 1.92045  (1.86433)
     | > decoder_coarse_loss: 1.90849  (2.16154)
     | > decoder_ddc_loss: 0.00100  (0.00144)
     | > ga_loss: 0.00209  (0.00254)
     | > decoder_diff_spec_loss: 0.43942  (0.49196)
     | > postnet_diff_spec_loss: 0.76593  (0.81905)
     | > decoder_ssim_loss: 0.28331  (0.31589)
     | > postnet_ssim_loss: 0.29505  (0.32333)
     | > loss: 3.81291  (3.94850)
     | > align_error: 0.99255  (0.98985)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 8.76903  (4.72603)
     | > current_lr: 0.00007 
     | > step_time: 5.70090  (4.22714)
     | > loader_time: 0.06850  (0.04084)


[1m   --> STEP: 47/87 -- GLOBAL_STEP: 272775[0m
     | > decoder_loss: 2.07049  (2.20565)
     | > postnet_loss: 1.76797  (1.99635)
     | > stopnet_loss: 1.75195  (1.79588)
     | > decoder_coarse_loss: 2.12739  (2.17711)
     | > decoder_ddc_loss: 0.00174  (0.00148)
     | > ga_loss: 0.00271  (0.00262)
     | > decoder_diff_spec_loss: 0.46956  (0.49839)
     | > postnet_diff_spec_loss: 0.79405  (0.82331)
     | > decoder_ssim_loss: 0.31441  (0.32486)
     | > postnet_ssim_loss: 0.32344  (0.33371)
     | > loss: 3.73278  (3.89919)
     | > align_error: 0.98724  (0.98958)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.40102  (4.99221)
     | > current_lr: 0.00007 
     | > step_time: 3.20060  (3.98832)
     | > loader_time: 0.07300  (0.04068)


[1m   --> STEP: 72/87 -- GLOBAL_STEP: 272800[0m
     | > decoder_loss: 2.07422  (2.18047)
     | > postnet_loss: 2.25199  (1.97885)
     | > stopnet_loss: 1.92387  (1.81204)
     | > decoder_coarse_loss: 2.08834  (2.15416)
     | > decoder_ddc_loss: 0.00113  (0.00142)
     | > ga_loss: 0.00202  (0.00255)
     | > decoder_diff_spec_loss: 0.49330  (0.49846)
     | > postnet_diff_spec_loss: 0.78809  (0.82236)
     | > decoder_ssim_loss: 0.28801  (0.31808)
     | > postnet_ssim_loss: 0.30328  (0.32670)
     | > loss: 4.00607  (3.89490)
     | > align_error: 0.99194  (0.98991)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.14536  (4.50956)
     | > current_lr: 0.00007 
     | > step_time: 4.68590  (4.03324)
     | > loader_time: 0.12450  (0.03936)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 4.19513 [0m(-1.67930)
     | > avg_decoder_loss:[92m 1.99642 [0m(-0.05086)
     | > avg_postnet_loss:[92m 1.86413 [0m(-0.01660)
     | > avg_stopnet_loss:[92m 1.53490 [0m(-0.00386)
     | > avg_decoder_coarse_loss:[92m 2.36921 [0m(-0.11999)
     | > avg_decoder_ddc_loss:[92m 0.00129 [0m(-0.00006)
     | > avg_ga_loss:[92m 0.00236 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[92m 0.43780 [0m(-0.00229)
     | > avg_postnet_diff_spec_loss:[92m 0.80639 [0m(-0.00143)
     | > avg_decoder_ssim_loss:[92m 0.33038 [0m(-0.00120)
     | > avg_postnet_ssim_loss:[92m 0.34117 [0m(-0.00186)
     | > avg_loss:[92m 3.58341 [0m(-0.05244)
     | > avg_align_error:[91m 0.99088 [0m(+0.00002)


[4m[1m > EPOCH: 32/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 13:42:04) [0m

[1m   --> STEP: 9/87 -- GLOBAL_STEP: 272825[0m
     | > decoder_loss: 2.11264  (2.13642)
     | > postnet_loss: 1.93670  (1.90880)
     | > stopnet_loss: 2.12866  (1.70097)
     | > decoder_coarse_loss: 2.08499  (2.10347)
     | > decoder_ddc_loss: 0.00115  (0.00138)
     | > ga_loss: 0.00225  (0.00251)
     | > decoder_diff_spec_loss: 0.50464  (0.49183)
     | > postnet_diff_spec_loss: 0.80378  (0.82062)
     | > decoder_ssim_loss: 0.25680  (0.33672)
     | > postnet_ssim_loss: 0.26183  (0.34338)
     | > loss: 4.13056  (3.74917)
     | > align_error: 0.99191  (0.98986)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.85245  (4.59203)
     | > current_lr: 0.00007 
     | > step_time: 3.96130  (4.20892)
     | > loader_time: 0.05950  (0.04615)


[1m   --> STEP: 34/87 -- GLOBAL_STEP: 272850[0m
     | > decoder_loss: 1.98370  (2.17139)
     | > postnet_loss: 1.71480  (1.97288)
     | > stopnet_loss: 2.23736  (1.82635)
     | > decoder_coarse_loss: 2.01262  (2.14211)
     | > decoder_ddc_loss: 0.00180  (0.00137)
     | > ga_loss: 0.00376  (0.00257)
     | > decoder_diff_spec_loss: 0.49744  (0.50025)
     | > postnet_diff_spec_loss: 0.81184  (0.82099)
     | > decoder_ssim_loss: 0.28102  (0.31713)
     | > postnet_ssim_loss: 0.28888  (0.32506)
     | > loss: 4.15417  (3.90198)
     | > align_error: 0.98627  (0.98988)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 12.81303  (4.94289)
     | > current_lr: 0.00007 
     | > step_time: 2.60850  (4.13122)
     | > loader_time: 0.08060  (0.04131)


[1m   --> STEP: 59/87 -- GLOBAL_STEP: 272875[0m
     | > decoder_loss: 2.11622  (2.16719)
     | > postnet_loss: 1.90847  (1.94811)
     | > stopnet_loss: 2.30850  (1.80254)
     | > decoder_coarse_loss: 2.08563  (2.13364)
     | > decoder_ddc_loss: 0.00160  (0.00141)
     | > ga_loss: 0.00317  (0.00260)
     | > decoder_diff_spec_loss: 0.50113  (0.49997)
     | > postnet_diff_spec_loss: 0.83248  (0.82246)
     | > decoder_ssim_loss: 0.25501  (0.32086)
     | > postnet_ssim_loss: 0.26143  (0.32933)
     | > loss: 4.31485  (3.87127)
     | > align_error: 0.98839  (0.98963)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 8.29317  (4.57813)
     | > current_lr: 0.00007 
     | > step_time: 3.32760  (3.92600)
     | > loader_time: 0.07650  (0.03738)


[1m   --> STEP: 84/87 -- GLOBAL_STEP: 272900[0m
     | > decoder_loss: 2.08754  (2.16157)
     | > postnet_loss: 1.82044  (1.94421)
     | > stopnet_loss: 1.36573  (1.81559)
     | > decoder_coarse_loss: 2.05792  (2.12396)
     | > decoder_ddc_loss: 0.00137  (0.00134)
     | > ga_loss: 0.00242  (0.00251)
     | > decoder_diff_spec_loss: 0.49512  (0.50181)
     | > postnet_diff_spec_loss: 0.80272  (0.82231)
     | > decoder_ssim_loss: 0.38252  (0.31521)
     | > postnet_ssim_loss: 0.39551  (0.32351)
     | > loss: 3.38860  (3.87662)
     | > align_error: 0.98982  (0.99003)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.68612  (4.28248)
     | > current_lr: 0.00007 
     | > step_time: 2.49920  (3.91262)
     | > loader_time: 0.01630  (0.03479)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 4.17406 [0m(-0.02107)
     | > avg_decoder_loss:[91m 1.99725 [0m(+0.00082)
     | > avg_postnet_loss:[92m 1.83608 [0m(-0.02804)
     | > avg_stopnet_loss:[91m 1.54148 [0m(+0.00658)
     | > avg_decoder_coarse_loss:[92m 2.33513 [0m(-0.03408)
     | > avg_decoder_ddc_loss:[92m 0.00124 [0m(-0.00005)
     | > avg_ga_loss:[92m 0.00236 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.43590 [0m(-0.00190)
     | > avg_postnet_diff_spec_loss:[92m 0.80540 [0m(-0.00099)
     | > avg_decoder_ssim_loss:[92m 0.32960 [0m(-0.00078)
     | > avg_postnet_ssim_loss:[91m 0.34133 [0m(+0.00016)
     | > avg_loss:[92m 3.57375 [0m(-0.00966)
     | > avg_align_error:[91m 0.99092 [0m(+0.00005)


[4m[1m > EPOCH: 33/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 13:49:43) [0m

[1m   --> STEP: 21/87 -- GLOBAL_STEP: 272925[0m
     | > decoder_loss: 2.26264  (2.15926)
     | > postnet_loss: 1.89107  (1.92548)
     | > stopnet_loss: 1.27760  (1.82895)
     | > decoder_coarse_loss: 2.17811  (2.09711)
     | > decoder_ddc_loss: 0.00181  (0.00137)
     | > ga_loss: 0.00305  (0.00255)
     | > decoder_diff_spec_loss: 0.49916  (0.49538)
     | > postnet_diff_spec_loss: 0.82821  (0.81867)
     | > decoder_ssim_loss: 0.41535  (0.31572)
     | > postnet_ssim_loss: 0.42428  (0.32275)
     | > loss: 3.41799  (3.87564)
     | > align_error: 0.98509  (0.98962)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 6.99679  (4.53487)
     | > current_lr: 0.00007 
     | > step_time: 2.84570  (4.10560)
     | > loader_time: 0.01690  (0.04026)


[1m   --> STEP: 46/87 -- GLOBAL_STEP: 272950[0m
     | > decoder_loss: 2.03698  (2.16420)
     | > postnet_loss: 2.10100  (1.94627)
     | > stopnet_loss: 1.57333  (1.79583)
     | > decoder_coarse_loss: 1.99753  (2.10594)
     | > decoder_ddc_loss: 0.00106  (0.00139)
     | > ga_loss: 0.00189  (0.00261)
     | > decoder_diff_spec_loss: 0.48834  (0.49952)
     | > postnet_diff_spec_loss: 0.81019  (0.82176)
     | > decoder_ssim_loss: 0.31716  (0.32349)
     | > postnet_ssim_loss: 0.33226  (0.33206)
     | > loss: 3.60392  (3.85753)
     | > align_error: 0.99238  (0.98951)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 6.97338  (5.07285)
     | > current_lr: 0.00007 
     | > step_time: 4.73480  (3.93327)
     | > loader_time: 0.02370  (0.03794)


[1m   --> STEP: 71/87 -- GLOBAL_STEP: 272975[0m
     | > decoder_loss: 1.96981  (2.14063)
     | > postnet_loss: 1.81543  (1.92414)
     | > stopnet_loss: 1.92247  (1.80105)
     | > decoder_coarse_loss: 1.91899  (2.08616)
     | > decoder_ddc_loss: 0.00114  (0.00135)
     | > ga_loss: 0.00250  (0.00255)
     | > decoder_diff_spec_loss: 0.46772  (0.49922)
     | > postnet_diff_spec_loss: 0.81200  (0.82083)
     | > decoder_ssim_loss: 0.27591  (0.31699)
     | > postnet_ssim_loss: 0.28556  (0.32545)
     | > loss: 3.82162  (3.84247)
     | > align_error: 0.99066  (0.98975)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.65746  (4.46817)
     | > current_lr: 0.00007 
     | > step_time: 4.34700  (3.95575)
     | > loader_time: 0.02460  (0.03818)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 4.43863 [0m(+0.26457)
     | > avg_decoder_loss:[91m 2.04306 [0m(+0.04582)
     | > avg_postnet_loss:[91m 2.01373 [0m(+0.17764)
     | > avg_stopnet_loss:[92m 1.53578 [0m(-0.00570)
     | > avg_decoder_coarse_loss:[91m 2.34623 [0m(+0.01110)
     | > avg_decoder_ddc_loss:[92m 0.00112 [0m(-0.00013)
     | > avg_ga_loss:[92m 0.00236 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.43767 [0m(+0.00177)
     | > avg_postnet_diff_spec_loss:[91m 0.80615 [0m(+0.00075)
     | > avg_decoder_ssim_loss:[92m 0.32913 [0m(-0.00047)
     | > avg_postnet_ssim_loss:[91m 0.34179 [0m(+0.00046)
     | > avg_loss:[91m 3.62728 [0m(+0.05353)
     | > avg_align_error:[91m 0.99093 [0m(+0.00000)


[4m[1m > EPOCH: 34/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 13:57:21) [0m

[1m   --> STEP: 8/87 -- GLOBAL_STEP: 273000[0m
     | > decoder_loss: 1.81145  (2.10840)
     | > postnet_loss: 1.71699  (1.86995)
     | > stopnet_loss: 2.59993  (1.64731)
     | > decoder_coarse_loss: 1.80478  (2.04308)
     | > decoder_ddc_loss: 0.00135  (0.00135)
     | > ga_loss: 0.00302  (0.00251)
     | > decoder_diff_spec_loss: 0.45012  (0.49186)
     | > postnet_diff_spec_loss: 0.80572  (0.82084)
     | > decoder_ssim_loss: 0.22361  (0.34482)
     | > postnet_ssim_loss: 0.22980  (0.35233)
     | > loss: 4.37597  (3.66803)
     | > align_error: 0.98935  (0.98946)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 8.40288  (4.93806)
     | > current_lr: 0.00007 
     | > step_time: 3.21080  (4.50310)
     | > loader_time: 0.01600  (0.05523)


[1m   --> STEP: 33/87 -- GLOBAL_STEP: 273025[0m
     | > decoder_loss: 2.13156  (2.13929)
     | > postnet_loss: 2.05031  (1.93598)
     | > stopnet_loss: 1.60243  (1.83155)
     | > decoder_coarse_loss: 2.04321  (2.08124)
     | > decoder_ddc_loss: 0.00126  (0.00132)
     | > ga_loss: 0.00230  (0.00251)
     | > decoder_diff_spec_loss: 0.50121  (0.50045)
     | > postnet_diff_spec_loss: 0.84063  (0.81944)
     | > decoder_ssim_loss: 0.34207  (0.31668)
     | > postnet_ssim_loss: 0.35153  (0.32476)
     | > loss: 3.67940  (3.87389)
     | > align_error: 0.99029  (0.98986)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.34837  (5.00758)
     | > current_lr: 0.00008 
     | > step_time: 4.26480  (4.28198)
     | > loader_time: 0.04450  (0.04743)


[1m   --> STEP: 58/87 -- GLOBAL_STEP: 273050[0m
     | > decoder_loss: 2.13616  (2.12945)
     | > postnet_loss: 1.95067  (1.90882)
     | > stopnet_loss: 1.50899  (1.80050)
     | > decoder_coarse_loss: 2.10802  (2.07190)
     | > decoder_ddc_loss: 0.00107  (0.00137)
     | > ga_loss: 0.00201  (0.00257)
     | > decoder_diff_spec_loss: 0.53139  (0.50076)
     | > postnet_diff_spec_loss: 0.83442  (0.82031)
     | > decoder_ssim_loss: 0.31688  (0.32032)
     | > postnet_ssim_loss: 0.32725  (0.32906)
     | > loss: 3.57049  (3.83386)
     | > align_error: 0.99183  (0.98952)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.73958  (4.73029)
     | > current_lr: 0.00008 
     | > step_time: 5.22100  (3.97938)
     | > loader_time: 0.02310  (0.04094)


[1m   --> STEP: 83/87 -- GLOBAL_STEP: 273075[0m
     | > decoder_loss: 2.10440  (2.12369)
     | > postnet_loss: 1.83788  (1.90891)
     | > stopnet_loss: 2.38696  (1.82921)
     | > decoder_coarse_loss: 2.05416  (2.06747)
     | > decoder_ddc_loss: 0.00113  (0.00132)
     | > ga_loss: 0.00215  (0.00249)
     | > decoder_diff_spec_loss: 0.48063  (0.50185)
     | > postnet_diff_spec_loss: 0.79749  (0.82079)
     | > decoder_ssim_loss: 0.24001  (0.31270)
     | > postnet_ssim_loss: 0.25086  (0.32130)
     | > loss: 4.33935  (3.85618)
     | > align_error: 0.99111  (0.98992)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.03367  (4.49459)
     | > current_lr: 0.00008 
     | > step_time: 3.21280  (3.93605)
     | > loader_time: 0.02000  (0.03840)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 4.55063 [0m(+0.11200)
     | > avg_decoder_loss:[92m 1.96287 [0m(-0.08019)
     | > avg_postnet_loss:[92m 1.88572 [0m(-0.12801)
     | > avg_stopnet_loss:[92m 1.53496 [0m(-0.00082)
     | > avg_decoder_coarse_loss:[92m 2.20744 [0m(-0.13879)
     | > avg_decoder_ddc_loss:[91m 0.00114 [0m(+0.00003)
     | > avg_ga_loss:[92m 0.00235 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[92m 0.43515 [0m(-0.00253)
     | > avg_postnet_diff_spec_loss:[92m 0.80519 [0m(-0.00097)
     | > avg_decoder_ssim_loss:[92m 0.32794 [0m(-0.00120)
     | > avg_postnet_ssim_loss:[92m 0.33977 [0m(-0.00202)
     | > avg_loss:[92m 3.53803 [0m(-0.08925)
     | > avg_align_error:[92m 0.99083 [0m(-0.00009)


[4m[1m > EPOCH: 35/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 14:05:02) [0m

[1m   --> STEP: 20/87 -- GLOBAL_STEP: 273100[0m
     | > decoder_loss: 1.90926  (2.11571)
     | > postnet_loss: 1.70111  (1.89269)
     | > stopnet_loss: 2.27651  (1.84302)
     | > decoder_coarse_loss: 1.90734  (2.05333)
     | > decoder_ddc_loss: 0.00146  (0.00133)
     | > ga_loss: 0.00292  (0.00252)
     | > decoder_diff_spec_loss: 0.46812  (0.49659)
     | > postnet_diff_spec_loss: 0.79568  (0.81735)
     | > decoder_ssim_loss: 0.24465  (0.30897)
     | > postnet_ssim_loss: 0.25232  (0.31715)
     | > loss: 4.11110  (3.85638)
     | > align_error: 0.98855  (0.98967)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.63643  (4.67508)
     | > current_lr: 0.00008 
     | > step_time: 3.44460  (4.09976)
     | > loader_time: 0.05040  (0.03514)


[1m   --> STEP: 45/87 -- GLOBAL_STEP: 273125[0m
     | > decoder_loss: 2.13227  (2.12342)
     | > postnet_loss: 1.74233  (1.90250)
     | > stopnet_loss: 2.11255  (1.78039)
     | > decoder_coarse_loss: 2.08038  (2.06091)
     | > decoder_ddc_loss: 0.00128  (0.00138)
     | > ga_loss: 0.00262  (0.00261)
     | > decoder_diff_spec_loss: 0.50402  (0.50034)
     | > postnet_diff_spec_loss: 0.81879  (0.82041)
     | > decoder_ssim_loss: 0.26005  (0.32174)
     | > postnet_ssim_loss: 0.26551  (0.33090)
     | > loss: 4.07684  (3.80884)
     | > align_error: 0.98962  (0.98932)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.10755  (4.99419)
     | > current_lr: 0.00008 
     | > step_time: 4.47190  (3.85994)
     | > loader_time: 0.04430  (0.03321)


[1m   --> STEP: 70/87 -- GLOBAL_STEP: 273150[0m
     | > decoder_loss: 2.29935  (2.10266)
     | > postnet_loss: 2.45083  (1.88870)
     | > stopnet_loss: 1.75952  (1.78986)
     | > decoder_coarse_loss: 2.18371  (2.04053)
     | > decoder_ddc_loss: 0.00111  (0.00135)
     | > ga_loss: 0.00210  (0.00253)
     | > decoder_diff_spec_loss: 0.55117  (0.49992)
     | > postnet_diff_spec_loss: 0.82157  (0.81951)
     | > decoder_ssim_loss: 0.29841  (0.31576)
     | > postnet_ssim_loss: 0.31181  (0.32499)
     | > loss: 3.99951  (3.80088)
     | > align_error: 0.99181  (0.98961)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.58250  (4.54879)
     | > current_lr: 0.00008 
     | > step_time: 4.05260  (3.90770)
     | > loader_time: 0.02390  (0.03321)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 5.06656 [0m(+0.51593)
     | > avg_decoder_loss:[91m 2.03935 [0m(+0.07648)
     | > avg_postnet_loss:[91m 2.01009 [0m(+0.12437)
     | > avg_stopnet_loss:[92m 1.53176 [0m(-0.00320)
     | > avg_decoder_coarse_loss:[91m 2.22426 [0m(+0.01682)
     | > avg_decoder_ddc_loss:[92m 0.00108 [0m(-0.00006)
     | > avg_ga_loss:[92m 0.00235 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[91m 0.43572 [0m(+0.00057)
     | > avg_postnet_diff_spec_loss:[91m 0.80576 [0m(+0.00058)
     | > avg_decoder_ssim_loss:[92m 0.32759 [0m(-0.00035)
     | > avg_postnet_ssim_loss:[91m 0.34168 [0m(+0.00191)
     | > avg_loss:[91m 3.58987 [0m(+0.05184)
     | > avg_align_error:[91m 0.99090 [0m(+0.00007)


[4m[1m > EPOCH: 36/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 14:12:41) [0m

[1m   --> STEP: 7/87 -- GLOBAL_STEP: 273175[0m
     | > decoder_loss: 2.12495  (2.09367)
     | > postnet_loss: 1.81367  (1.85175)
     | > stopnet_loss: 1.81372  (1.49510)
     | > decoder_coarse_loss: 2.04888  (2.02333)
     | > decoder_ddc_loss: 0.00117  (0.00135)
     | > ga_loss: 0.00235  (0.00241)
     | > decoder_diff_spec_loss: 0.47980  (0.49492)
     | > postnet_diff_spec_loss: 0.79868  (0.82175)
     | > decoder_ssim_loss: 0.32443  (0.36006)
     | > postnet_ssim_loss: 0.33143  (0.36936)
     | > loss: 3.80624  (3.51122)
     | > align_error: 0.99075  (0.98939)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.85890  (4.86894)
     | > current_lr: 0.00008 
     | > step_time: 3.82020  (4.62894)
     | > loader_time: 0.02280  (0.02425)


[1m   --> STEP: 32/87 -- GLOBAL_STEP: 273200[0m
     | > decoder_loss: 1.78886  (2.10479)
     | > postnet_loss: 1.80153  (1.88847)
     | > stopnet_loss: 1.89035  (1.81405)
     | > decoder_coarse_loss: 1.77074  (2.03605)
     | > decoder_ddc_loss: 0.00129  (0.00131)
     | > ga_loss: 0.00292  (0.00250)
     | > decoder_diff_spec_loss: 0.44354  (0.50074)
     | > postnet_diff_spec_loss: 0.80566  (0.81735)
     | > decoder_ssim_loss: 0.27941  (0.31426)
     | > postnet_ssim_loss: 0.29249  (0.32287)
     | > loss: 3.70081  (3.82300)
     | > align_error: 0.98893  (0.98976)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.70344  (5.40439)
     | > current_lr: 0.00008 
     | > step_time: 3.48020  (4.05968)
     | > loader_time: 0.02440  (0.03215)


[1m   --> STEP: 57/87 -- GLOBAL_STEP: 273225[0m
     | > decoder_loss: 2.29024  (2.09159)
     | > postnet_loss: 1.90993  (1.86990)
     | > stopnet_loss: 1.64895  (1.78141)
     | > decoder_coarse_loss: 2.24825  (2.02768)
     | > decoder_ddc_loss: 0.00146  (0.00137)
     | > ga_loss: 0.00277  (0.00256)
     | > decoder_diff_spec_loss: 0.57552  (0.50088)
     | > postnet_diff_spec_loss: 0.86688  (0.81872)
     | > decoder_ssim_loss: 0.34261  (0.31873)
     | > postnet_ssim_loss: 0.34994  (0.32804)
     | > loss: 3.80901  (3.78346)
     | > align_error: 0.98951  (0.98938)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.82405  (4.89232)
     | > current_lr: 0.00008 
     | > step_time: 2.51660  (3.85740)
     | > loader_time: 0.02900  (0.03444)


[1m   --> STEP: 82/87 -- GLOBAL_STEP: 273250[0m
     | > decoder_loss: 2.00332  (2.09066)
     | > postnet_loss: 1.83157  (1.87257)
     | > stopnet_loss: 2.42140  (1.80708)
     | > decoder_coarse_loss: 1.97476  (2.02578)
     | > decoder_ddc_loss: 0.00123  (0.00131)
     | > ga_loss: 0.00203  (0.00248)
     | > decoder_diff_spec_loss: 0.52368  (0.50271)
     | > postnet_diff_spec_loss: 0.83693  (0.81971)
     | > decoder_ssim_loss: 0.23287  (0.31200)
     | > postnet_ssim_loss: 0.23812  (0.32115)
     | > loss: 4.34218  (3.80597)
     | > align_error: 0.99093  (0.98981)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.86711  (4.53723)
     | > current_lr: 0.00008 
     | > step_time: 3.60570  (3.89657)
     | > loader_time: 0.02230  (0.03424)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 4.33177 [0m(-0.73479)
     | > avg_decoder_loss:[92m 2.03356 [0m(-0.00580)
     | > avg_postnet_loss:[91m 2.06067 [0m(+0.05058)
     | > avg_stopnet_loss:[92m 1.52699 [0m(-0.00477)
     | > avg_decoder_coarse_loss:[92m 2.21695 [0m(-0.00730)
     | > avg_decoder_ddc_loss:[91m 0.00108 [0m(+0.00000)
     | > avg_ga_loss:[92m 0.00234 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.43438 [0m(-0.00133)
     | > avg_postnet_diff_spec_loss:[92m 0.80463 [0m(-0.00113)
     | > avg_decoder_ssim_loss:[92m 0.32690 [0m(-0.00069)
     | > avg_postnet_ssim_loss:[92m 0.34113 [0m(-0.00055)
     | > avg_loss:[91m 3.59350 [0m(+0.00363)
     | > avg_align_error:[92m 0.99083 [0m(-0.00007)


[4m[1m > EPOCH: 37/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 14:20:19) [0m

[1m   --> STEP: 19/87 -- GLOBAL_STEP: 273275[0m
     | > decoder_loss: 2.04781  (2.09454)
     | > postnet_loss: 1.78567  (1.87529)
     | > stopnet_loss: 1.25237  (1.83204)
     | > decoder_coarse_loss: 2.02263  (2.02943)
     | > decoder_ddc_loss: 0.00115  (0.00129)
     | > ga_loss: 0.00234  (0.00248)
     | > decoder_diff_spec_loss: 0.49134  (0.49787)
     | > postnet_diff_spec_loss: 0.81589  (0.81732)
     | > decoder_ssim_loss: 0.39268  (0.31121)
     | > postnet_ssim_loss: 0.40426  (0.31946)
     | > loss: 3.25443  (3.83102)
     | > align_error: 0.99047  (0.98971)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 7.69972  (4.55908)
     | > current_lr: 0.00008 
     | > step_time: 3.26250  (4.15807)
     | > loader_time: 0.07360  (0.03048)


[1m   --> STEP: 44/87 -- GLOBAL_STEP: 273300[0m
     | > decoder_loss: 1.92466  (2.08718)
     | > postnet_loss: 1.64442  (1.87687)
     | > stopnet_loss: 1.22544  (1.76555)
     | > decoder_coarse_loss: 1.92438  (2.02588)
     | > decoder_ddc_loss: 0.00102  (0.00135)
     | > ga_loss: 0.00183  (0.00259)
     | > decoder_diff_spec_loss: 0.47105  (0.50157)
     | > postnet_diff_spec_loss: 0.79669  (0.81961)
     | > decoder_ssim_loss: 0.38441  (0.32152)
     | > postnet_ssim_loss: 0.39566  (0.33147)
     | > loss: 3.12017  (3.76988)
     | > align_error: 0.99185  (0.98924)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 9.60232  (4.83737)
     | > current_lr: 0.00008 
     | > step_time: 4.83420  (3.83878)
     | > loader_time: 0.02290  (0.03084)


[1m   --> STEP: 69/87 -- GLOBAL_STEP: 273325[0m
     | > decoder_loss: 2.02635  (2.06855)
     | > postnet_loss: 1.79267  (1.84964)
     | > stopnet_loss: 2.32573  (1.78112)
     | > decoder_coarse_loss: 1.92910  (2.00608)
     | > decoder_ddc_loss: 0.00104  (0.00131)
     | > ga_loss: 0.00207  (0.00252)
     | > decoder_diff_spec_loss: 0.48646  (0.50060)
     | > postnet_diff_spec_loss: 0.80080  (0.81833)
     | > decoder_ssim_loss: 0.23524  (0.31468)
     | > postnet_ssim_loss: 0.24576  (0.32417)
     | > loss: 4.21542  (3.76459)
     | > align_error: 0.99150  (0.98951)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.82603  (4.44989)
     | > current_lr: 0.00008 
     | > step_time: 3.75170  (3.90810)
     | > loader_time: 0.06030  (0.03456)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 4.06016 [0m(-0.27161)
     | > avg_decoder_loss:[92m 2.01939 [0m(-0.01417)
     | > avg_postnet_loss:[92m 1.96166 [0m(-0.09900)
     | > avg_stopnet_loss:[91m 1.52902 [0m(+0.00204)
     | > avg_decoder_coarse_loss:[92m 2.16716 [0m(-0.04980)
     | > avg_decoder_ddc_loss:[92m 0.00100 [0m(-0.00008)
     | > avg_ga_loss:[92m 0.00233 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.43325 [0m(-0.00113)
     | > avg_postnet_diff_spec_loss:[92m 0.80392 [0m(-0.00071)
     | > avg_decoder_ssim_loss:[92m 0.32623 [0m(-0.00067)
     | > avg_postnet_ssim_loss:[92m 0.34015 [0m(-0.00098)
     | > avg_loss:[92m 3.55388 [0m(-0.03962)
     | > avg_align_error:[92m 0.99075 [0m(-0.00008)


[4m[1m > EPOCH: 38/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 14:27:52) [0m

[1m   --> STEP: 6/87 -- GLOBAL_STEP: 273350[0m
     | > decoder_loss: 1.92666  (2.07617)
     | > postnet_loss: 1.63851  (1.82239)
     | > stopnet_loss: 1.03189  (1.45645)
     | > decoder_coarse_loss: 1.82411  (1.98997)
     | > decoder_ddc_loss: 0.00107  (0.00130)
     | > ga_loss: 0.00165  (0.00243)
     | > decoder_diff_spec_loss: 0.48556  (0.49837)
     | > postnet_diff_spec_loss: 0.80526  (0.82497)
     | > decoder_ssim_loss: 0.43531  (0.36514)
     | > postnet_ssim_loss: 0.44153  (0.37458)
     | > loss: 2.92966  (3.45682)
     | > align_error: 0.99164  (0.98908)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.66932  (4.35271)
     | > current_lr: 0.00008 
     | > step_time: 4.46830  (4.55868)
     | > loader_time: 0.02260  (0.05491)


[1m   --> STEP: 31/87 -- GLOBAL_STEP: 273375[0m
     | > decoder_loss: 2.23383  (2.09166)
     | > postnet_loss: 1.87218  (1.86531)
     | > stopnet_loss: 1.49242  (1.78512)
     | > decoder_coarse_loss: 2.18077  (2.01414)
     | > decoder_ddc_loss: 0.00141  (0.00126)
     | > ga_loss: 0.00275  (0.00248)
     | > decoder_diff_spec_loss: 0.56474  (0.50355)
     | > postnet_diff_spec_loss: 0.86773  (0.81665)
     | > decoder_ssim_loss: 0.35467  (0.31415)
     | > postnet_ssim_loss: 0.36250  (0.32302)
     | > loss: 3.61562  (3.77996)
     | > align_error: 0.98846  (0.98967)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.08305  (4.88826)
     | > current_lr: 0.00008 
     | > step_time: 3.23230  (4.03220)
     | > loader_time: 0.05600  (0.03773)


[1m   --> STEP: 56/87 -- GLOBAL_STEP: 273400[0m
     | > decoder_loss: 1.99439  (2.06618)
     | > postnet_loss: 1.79859  (1.84421)
     | > stopnet_loss: 2.16156  (1.76811)
     | > decoder_coarse_loss: 1.91083  (1.99018)
     | > decoder_ddc_loss: 0.00084  (0.00133)
     | > ga_loss: 0.00189  (0.00255)
     | > decoder_diff_spec_loss: 0.50672  (0.50012)
     | > postnet_diff_spec_loss: 0.80906  (0.81693)
     | > decoder_ssim_loss: 0.25550  (0.31708)
     | > postnet_ssim_loss: 0.26371  (0.32690)
     | > loss: 4.05595  (3.74660)
     | > align_error: 0.99330  (0.98926)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.61969  (4.55964)
     | > current_lr: 0.00008 
     | > step_time: 5.12310  (3.86403)
     | > loader_time: 0.02560  (0.03509)


[1m   --> STEP: 81/87 -- GLOBAL_STEP: 273425[0m
     | > decoder_loss: 2.21986  (2.06781)
     | > postnet_loss: 1.80626  (1.84406)
     | > stopnet_loss: 2.31876  (1.78123)
     | > decoder_coarse_loss: 2.16390  (1.99354)
     | > decoder_ddc_loss: 0.00101  (0.00127)
     | > ga_loss: 0.00238  (0.00248)
     | > decoder_diff_spec_loss: 0.58709  (0.50309)
     | > postnet_diff_spec_loss: 0.86328  (0.81844)
     | > decoder_ssim_loss: 0.23427  (0.31177)
     | > postnet_ssim_loss: 0.23818  (0.32136)
     | > loss: 4.35914  (3.75896)
     | > align_error: 0.99035  (0.98970)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.20702  (4.37555)
     | > current_lr: 0.00009 
     | > step_time: 3.31370  (3.87628)
     | > loader_time: 0.02080  (0.03397)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.97404 [0m(-0.08612)
     | > avg_decoder_loss:[91m 2.01984 [0m(+0.00045)
     | > avg_postnet_loss:[91m 1.99765 [0m(+0.03598)
     | > avg_stopnet_loss:[92m 1.51930 [0m(-0.00973)
     | > avg_decoder_coarse_loss:[92m 2.10249 [0m(-0.06467)
     | > avg_decoder_ddc_loss:[92m 0.00098 [0m(-0.00002)
     | > avg_ga_loss:[91m 0.00233 [0m(+0.00000)
     | > avg_decoder_diff_spec_loss:[92m 0.43252 [0m(-0.00073)
     | > avg_postnet_diff_spec_loss:[92m 0.80328 [0m(-0.00064)
     | > avg_decoder_ssim_loss:[92m 0.32557 [0m(-0.00066)
     | > avg_postnet_ssim_loss:[92m 0.33974 [0m(-0.00041)
     | > avg_loss:[92m 3.53649 [0m(-0.01739)
     | > avg_align_error:[92m 0.99073 [0m(-0.00003)


[4m[1m > EPOCH: 39/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 14:35:22) [0m

[1m   --> STEP: 18/87 -- GLOBAL_STEP: 273450[0m
     | > decoder_loss: 2.18800  (2.07055)
     | > postnet_loss: 1.88402  (1.83926)
     | > stopnet_loss: 1.50469  (1.85854)
     | > decoder_coarse_loss: 2.07689  (1.98904)
     | > decoder_ddc_loss: 0.00125  (0.00124)
     | > ga_loss: 0.00224  (0.00249)
     | > decoder_diff_spec_loss: 0.55570  (0.49912)
     | > postnet_diff_spec_loss: 0.86260  (0.81578)
     | > decoder_ssim_loss: 0.33917  (0.30546)
     | > postnet_ssim_loss: 0.34505  (0.31376)
     | > loss: 3.57906  (3.82954)
     | > align_error: 0.98983  (0.98956)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 8.38599  (4.53801)
     | > current_lr: 0.00009 
     | > step_time: 3.91490  (4.17345)
     | > loader_time: 0.02580  (0.03180)


[1m   --> STEP: 43/87 -- GLOBAL_STEP: 273475[0m
     | > decoder_loss: 2.04875  (2.06537)
     | > postnet_loss: 1.67981  (1.84760)
     | > stopnet_loss: 1.73793  (1.77532)
     | > decoder_coarse_loss: 2.00008  (1.99169)
     | > decoder_ddc_loss: 0.00146  (0.00131)
     | > ga_loss: 0.00289  (0.00261)
     | > decoder_diff_spec_loss: 0.50159  (0.50223)
     | > postnet_diff_spec_loss: 0.81045  (0.81894)
     | > decoder_ssim_loss: 0.32412  (0.31890)
     | > postnet_ssim_loss: 0.33184  (0.32911)
     | > loss: 3.67689  (3.75715)
     | > align_error: 0.98767  (0.98904)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.17680  (5.07276)
     | > current_lr: 0.00009 
     | > step_time: 3.16130  (3.85241)
     | > loader_time: 0.01970  (0.02990)


[1m   --> STEP: 68/87 -- GLOBAL_STEP: 273500[0m
     | > decoder_loss: 2.26587  (2.04659)
     | > postnet_loss: 1.89483  (1.82091)
     | > stopnet_loss: 1.74949  (1.76891)
     | > decoder_coarse_loss: 2.14915  (1.97433)
     | > decoder_ddc_loss: 0.00122  (0.00127)
     | > ga_loss: 0.00202  (0.00253)
     | > decoder_diff_spec_loss: 0.55807  (0.50052)
     | > postnet_diff_spec_loss: 0.86616  (0.81776)
     | > decoder_ssim_loss: 0.31520  (0.31467)
     | > postnet_ssim_loss: 0.32254  (0.32465)
     | > loss: 3.85286  (3.73173)
     | > align_error: 0.98991  (0.98935)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.75432  (4.58371)
     | > current_lr: 0.00009 
     | > step_time: 4.29890  (3.90986)
     | > loader_time: 0.05020  (0.03159)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 4.17671 [0m(+0.20267)
     | > avg_decoder_loss:[91m 2.05237 [0m(+0.03253)
     | > avg_postnet_loss:[92m 1.96002 [0m(-0.03763)
     | > avg_stopnet_loss:[91m 1.52220 [0m(+0.00291)
     | > avg_decoder_coarse_loss:[91m 2.12981 [0m(+0.02732)
     | > avg_decoder_ddc_loss:[92m 0.00098 [0m(-0.00000)
     | > avg_ga_loss:[92m 0.00233 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.43446 [0m(+0.00194)
     | > avg_postnet_diff_spec_loss:[92m 0.80239 [0m(-0.00089)
     | > avg_decoder_ssim_loss:[92m 0.32512 [0m(-0.00045)
     | > avg_postnet_ssim_loss:[92m 0.33765 [0m(-0.00208)
     | > avg_loss:[91m 3.54457 [0m(+0.00809)
     | > avg_align_error:[92m 0.99057 [0m(-0.00016)


[4m[1m > EPOCH: 40/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 14:42:44) [0m

[1m   --> STEP: 5/87 -- GLOBAL_STEP: 273525[0m
     | > decoder_loss: 2.05877  (2.07485)
     | > postnet_loss: 1.79703  (1.84138)
     | > stopnet_loss: 1.19131  (1.50844)
     | > decoder_coarse_loss: 1.96120  (1.98467)
     | > decoder_ddc_loss: 0.00140  (0.00137)
     | > ga_loss: 0.00242  (0.00256)
     | > decoder_diff_spec_loss: 0.48133  (0.50902)
     | > postnet_diff_spec_loss: 0.80613  (0.82763)
     | > decoder_ssim_loss: 0.41660  (0.34996)
     | > postnet_ssim_loss: 0.43405  (0.36086)
     | > loss: 3.19254  (3.50868)
     | > align_error: 0.98782  (0.98840)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.56549  (5.07487)
     | > current_lr: 0.00009 
     | > step_time: 3.44430  (4.70042)
     | > loader_time: 0.01850  (0.04814)


[1m   --> STEP: 30/87 -- GLOBAL_STEP: 273550[0m
     | > decoder_loss: 2.22369  (2.06026)
     | > postnet_loss: 2.01855  (1.83458)
     | > stopnet_loss: 1.29044  (1.77451)
     | > decoder_coarse_loss: 2.14586  (1.97988)
     | > decoder_ddc_loss: 0.00102  (0.00123)
     | > ga_loss: 0.00180  (0.00247)
     | > decoder_diff_spec_loss: 0.53256  (0.50235)
     | > postnet_diff_spec_loss: 0.81032  (0.81393)
     | > decoder_ssim_loss: 0.36110  (0.31181)
     | > postnet_ssim_loss: 0.37339  (0.32086)
     | > loss: 3.41607  (3.74308)
     | > align_error: 0.99110  (0.98962)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.86656  (5.01766)
     | > current_lr: 0.00009 
     | > step_time: 4.03050  (4.03070)
     | > loader_time: 0.03840  (0.03420)


[1m   --> STEP: 55/87 -- GLOBAL_STEP: 273575[0m
     | > decoder_loss: 1.96878  (2.04206)
     | > postnet_loss: 1.71980  (1.81111)
     | > stopnet_loss: 1.46757  (1.74903)
     | > decoder_coarse_loss: 1.90091  (1.96738)
     | > decoder_ddc_loss: 0.00166  (0.00130)
     | > ga_loss: 0.00300  (0.00256)
     | > decoder_diff_spec_loss: 0.51049  (0.50037)
     | > postnet_diff_spec_loss: 0.81778  (0.81595)
     | > decoder_ssim_loss: 0.37879  (0.31733)
     | > postnet_ssim_loss: 0.38721  (0.32730)
     | > loss: 3.40394  (3.70754)
     | > align_error: 0.98621  (0.98912)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.43902  (4.73005)
     | > current_lr: 0.00009 
     | > step_time: 2.53570  (3.76747)
     | > loader_time: 0.01910  (0.03754)


[1m   --> STEP: 80/87 -- GLOBAL_STEP: 273600[0m
     | > decoder_loss: 2.00070  (2.03955)
     | > postnet_loss: 1.82678  (1.80928)
     | > stopnet_loss: 1.35324  (1.75846)
     | > decoder_coarse_loss: 1.89595  (1.96414)
     | > decoder_ddc_loss: 0.00115  (0.00123)
     | > ga_loss: 0.00207  (0.00248)
     | > decoder_diff_spec_loss: 0.48554  (0.50246)
     | > postnet_diff_spec_loss: 0.81410  (0.81690)
     | > decoder_ssim_loss: 0.37516  (0.31188)
     | > postnet_ssim_loss: 0.38613  (0.32173)
     | > loss: 3.31000  (3.71264)
     | > align_error: 0.99107  (0.98962)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.35726  (4.47203)
     | > current_lr: 0.00009 
     | > step_time: 2.86210  (3.79210)
     | > loader_time: 0.01800  (0.03404)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 4.06632 [0m(-0.11039)
     | > avg_decoder_loss:[91m 2.06110 [0m(+0.00873)
     | > avg_postnet_loss:[91m 1.99088 [0m(+0.03087)
     | > avg_stopnet_loss:[92m 1.51739 [0m(-0.00481)
     | > avg_decoder_coarse_loss:[92m 2.09152 [0m(-0.03829)
     | > avg_decoder_ddc_loss:[92m 0.00098 [0m(-0.00001)
     | > avg_ga_loss:[92m 0.00233 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[92m 0.43235 [0m(-0.00211)
     | > avg_postnet_diff_spec_loss:[92m 0.80207 [0m(-0.00032)
     | > avg_decoder_ssim_loss:[92m 0.32450 [0m(-0.00062)
     | > avg_postnet_ssim_loss:[91m 0.33894 [0m(+0.00129)
     | > avg_loss:[92m 3.53962 [0m(-0.00495)
     | > avg_align_error:[91m 0.99059 [0m(+0.00002)


[4m[1m > EPOCH: 41/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 14:49:58) [0m

[1m   --> STEP: 17/87 -- GLOBAL_STEP: 273625[0m
     | > decoder_loss: 2.20990  (2.03427)
     | > postnet_loss: 1.86082  (1.80500)
     | > stopnet_loss: 2.02712  (1.84659)
     | > decoder_coarse_loss: 2.11534  (1.96292)
     | > decoder_ddc_loss: 0.00117  (0.00124)
     | > ga_loss: 0.00274  (0.00248)
     | > decoder_diff_spec_loss: 0.49350  (0.49663)
     | > postnet_diff_spec_loss: 0.81011  (0.81232)
     | > decoder_ssim_loss: 0.27257  (0.30251)
     | > postnet_ssim_loss: 0.28202  (0.31198)
     | > loss: 4.05217  (3.79068)
     | > align_error: 0.98893  (0.98941)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.75945  (3.87858)
     | > current_lr: 0.00009 
     | > step_time: 3.73620  (4.26834)
     | > loader_time: 0.02040  (0.03869)


[1m   --> STEP: 42/87 -- GLOBAL_STEP: 273650[0m
     | > decoder_loss: 2.07166  (2.04657)
     | > postnet_loss: 1.88849  (1.81718)
     | > stopnet_loss: 1.36097  (1.75549)
     | > decoder_coarse_loss: 1.98622  (1.96656)
     | > decoder_ddc_loss: 0.00163  (0.00131)
     | > ga_loss: 0.00290  (0.00258)
     | > decoder_diff_spec_loss: 0.49058  (0.50382)
     | > postnet_diff_spec_loss: 0.83403  (0.81852)
     | > decoder_ssim_loss: 0.38287  (0.31793)
     | > postnet_ssim_loss: 0.40054  (0.32851)
     | > loss: 3.38950  (3.71852)
     | > align_error: 0.98752  (0.98896)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.70390  (4.23815)
     | > current_lr: 0.00009 
     | > step_time: 2.59290  (3.87953)
     | > loader_time: 0.10230  (0.03400)


[1m   --> STEP: 67/87 -- GLOBAL_STEP: 273675[0m
     | > decoder_loss: 1.87076  (2.02183)
     | > postnet_loss: 1.60908  (1.78651)
     | > stopnet_loss: 1.49898  (1.75444)
     | > decoder_coarse_loss: 1.81621  (1.94557)
     | > decoder_ddc_loss: 0.00097  (0.00128)
     | > ga_loss: 0.00226  (0.00252)
     | > decoder_diff_spec_loss: 0.45437  (0.50089)
     | > postnet_diff_spec_loss: 0.78888  (0.81601)
     | > decoder_ssim_loss: 0.33818  (0.31370)
     | > postnet_ssim_loss: 0.34571  (0.32408)
     | > loss: 3.31632  (3.69452)
     | > align_error: 0.99214  (0.98923)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.97161  (3.97665)
     | > current_lr: 0.00009 
     | > step_time: 4.63660  (3.91697)
     | > loader_time: 0.02130  (0.03344)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.87091 [0m(-0.19541)
     | > avg_decoder_loss:[91m 2.10987 [0m(+0.04877)
     | > avg_postnet_loss:[91m 2.21658 [0m(+0.22570)
     | > avg_stopnet_loss:[92m 1.51287 [0m(-0.00452)
     | > avg_decoder_coarse_loss:[91m 2.09216 [0m(+0.00065)
     | > avg_decoder_ddc_loss:[92m 0.00093 [0m(-0.00004)
     | > avg_ga_loss:[92m 0.00232 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.43401 [0m(+0.00166)
     | > avg_postnet_diff_spec_loss:[91m 0.80229 [0m(+0.00022)
     | > avg_decoder_ssim_loss:[92m 0.32421 [0m(-0.00029)
     | > avg_postnet_ssim_loss:[91m 0.34078 [0m(+0.00184)
     | > avg_loss:[91m 3.60470 [0m(+0.06508)
     | > avg_align_error:[91m 0.99062 [0m(+0.00003)


[4m[1m > EPOCH: 42/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 14:57:32) [0m

[1m   --> STEP: 4/87 -- GLOBAL_STEP: 273700[0m
     | > decoder_loss: 2.15471  (2.06107)
     | > postnet_loss: 1.75702  (1.79743)
     | > stopnet_loss: 1.89912  (1.63938)
     | > decoder_coarse_loss: 2.06420  (1.97104)
     | > decoder_ddc_loss: 0.00160  (0.00126)
     | > ga_loss: 0.00335  (0.00260)
     | > decoder_diff_spec_loss: 0.52582  (0.51777)
     | > postnet_diff_spec_loss: 0.83263  (0.83201)
     | > decoder_ssim_loss: 0.33349  (0.33218)
     | > postnet_ssim_loss: 0.33565  (0.34230)
     | > loss: 3.91712  (3.61615)
     | > align_error: 0.98536  (0.98853)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 11.15982  (5.79543)
     | > current_lr: 0.00009 
     | > step_time: 2.69010  (5.20510)
     | > loader_time: 0.01780  (0.02509)


[1m   --> STEP: 29/87 -- GLOBAL_STEP: 273725[0m
     | > decoder_loss: 1.91079  (2.03075)
     | > postnet_loss: 1.74445  (1.78755)
     | > stopnet_loss: 1.73485  (1.81546)
     | > decoder_coarse_loss: 1.85181  (1.95230)
     | > decoder_ddc_loss: 0.00142  (0.00119)
     | > ga_loss: 0.00318  (0.00247)
     | > decoder_diff_spec_loss: 0.49388  (0.50166)
     | > postnet_diff_spec_loss: 0.79423  (0.81342)
     | > decoder_ssim_loss: 0.33562  (0.30903)
     | > postnet_ssim_loss: 0.34096  (0.31833)
     | > loss: 3.61904  (3.75639)
     | > align_error: 0.98774  (0.98951)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 9.90486  (4.66335)
     | > current_lr: 0.00009 
     | > step_time: 2.40230  (4.24249)
     | > loader_time: 0.04030  (0.03101)


[1m   --> STEP: 54/87 -- GLOBAL_STEP: 273750[0m
     | > decoder_loss: 1.99464  (2.02235)
     | > postnet_loss: 1.72536  (1.77611)
     | > stopnet_loss: 1.47074  (1.75492)
     | > decoder_coarse_loss: 1.89721  (1.93995)
     | > decoder_ddc_loss: 0.00113  (0.00126)
     | > ga_loss: 0.00214  (0.00253)
     | > decoder_diff_spec_loss: 0.48507  (0.50095)
     | > postnet_diff_spec_loss: 0.78979  (0.81534)
     | > decoder_ssim_loss: 0.33837  (0.31501)
     | > postnet_ssim_loss: 0.35611  (0.32548)
     | > loss: 3.37838  (3.69171)
     | > align_error: 0.99096  (0.98908)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.96839  (4.41076)
     | > current_lr: 0.00009 
     | > step_time: 5.01720  (3.99410)
     | > loader_time: 0.02830  (0.02901)


[1m   --> STEP: 79/87 -- GLOBAL_STEP: 273775[0m
     | > decoder_loss: 2.20000  (2.02004)
     | > postnet_loss: 1.87305  (1.77068)
     | > stopnet_loss: 1.96210  (1.76524)
     | > decoder_coarse_loss: 2.11262  (1.93742)
     | > decoder_ddc_loss: 0.00091  (0.00121)
     | > ga_loss: 0.00231  (0.00247)
     | > decoder_diff_spec_loss: 0.51403  (0.50330)
     | > postnet_diff_spec_loss: 0.82431  (0.81623)
     | > decoder_ssim_loss: 0.25896  (0.30987)
     | > postnet_ssim_loss: 0.26572  (0.32026)
     | > loss: 3.98606  (3.69733)
     | > align_error: 0.99187  (0.98950)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.41452  (4.11485)
     | > current_lr: 0.00009 
     | > step_time: 3.85930  (3.98019)
     | > loader_time: 0.02290  (0.02852)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 4.52702 [0m(+0.65611)
     | > avg_decoder_loss:[92m 2.05475 [0m(-0.05512)
     | > avg_postnet_loss:[92m 2.05528 [0m(-0.16130)
     | > avg_stopnet_loss:[91m 1.52003 [0m(+0.00716)
     | > avg_decoder_coarse_loss:[92m 2.08992 [0m(-0.00224)
     | > avg_decoder_ddc_loss:[91m 0.00094 [0m(+0.00000)
     | > avg_ga_loss:[92m 0.00231 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.43195 [0m(-0.00206)
     | > avg_postnet_diff_spec_loss:[92m 0.80113 [0m(-0.00115)
     | > avg_decoder_ssim_loss:[92m 0.32322 [0m(-0.00099)
     | > avg_postnet_ssim_loss:[92m 0.33851 [0m(-0.00227)
     | > avg_loss:[92m 3.55553 [0m(-0.04918)
     | > avg_align_error:[92m 0.99057 [0m(-0.00005)


[4m[1m > EPOCH: 43/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 15:05:23) [0m

[1m   --> STEP: 16/87 -- GLOBAL_STEP: 273800[0m
     | > decoder_loss: 1.84752  (1.99690)
     | > postnet_loss: 1.69495  (1.76147)
     | > stopnet_loss: 2.30054  (1.84596)
     | > decoder_coarse_loss: 1.78868  (1.93201)
     | > decoder_ddc_loss: 0.00154  (0.00120)
     | > ga_loss: 0.00287  (0.00245)
     | > decoder_diff_spec_loss: 0.48019  (0.49859)
     | > postnet_diff_spec_loss: 0.82222  (0.81165)
     | > decoder_ssim_loss: 0.24787  (0.30333)
     | > postnet_ssim_loss: 0.25582  (0.31289)
     | > loss: 4.09958  (3.76273)
     | > align_error: 0.98659  (0.98944)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.11974  (4.24729)
     | > current_lr: 0.00009 
     | > step_time: 4.12260  (4.21870)
     | > loader_time: 0.02460  (0.03619)


[1m   --> STEP: 41/87 -- GLOBAL_STEP: 273825[0m
     | > decoder_loss: 1.87350  (2.02040)
     | > postnet_loss: 1.68189  (1.77518)
     | > stopnet_loss: 2.50296  (1.75488)
     | > decoder_coarse_loss: 1.81609  (1.94263)
     | > decoder_ddc_loss: 0.00081  (0.00127)
     | > ga_loss: 0.00200  (0.00257)
     | > decoder_diff_spec_loss: 0.46806  (0.50462)
     | > postnet_diff_spec_loss: 0.81502  (0.81720)
     | > decoder_ssim_loss: 0.20942  (0.31515)
     | > postnet_ssim_loss: 0.21972  (0.32582)
     | > loss: 4.28409  (3.69329)
     | > align_error: 0.99277  (0.98903)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.04376  (4.52614)
     | > current_lr: 0.00010 
     | > step_time: 4.49960  (3.76184)
     | > loader_time: 0.07440  (0.03202)


[1m   --> STEP: 66/87 -- GLOBAL_STEP: 273850[0m
     | > decoder_loss: 2.16051  (2.00188)
     | > postnet_loss: 1.84061  (1.75283)
     | > stopnet_loss: 1.40612  (1.74815)
     | > decoder_coarse_loss: 2.06455  (1.92403)
     | > decoder_ddc_loss: 0.00133  (0.00126)
     | > ga_loss: 0.00234  (0.00252)
     | > decoder_diff_spec_loss: 0.52188  (0.50152)
     | > postnet_diff_spec_loss: 0.84443  (0.81567)
     | > decoder_ssim_loss: 0.35497  (0.31210)
     | > postnet_ssim_loss: 0.36865  (0.32295)
     | > loss: 3.45704  (3.66881)
     | > align_error: 0.98903  (0.98922)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.46623  (4.36649)
     | > current_lr: 0.00010 
     | > step_time: 4.24020  (3.82684)
     | > loader_time: 0.02740  (0.03255)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 5.77194 [0m(+1.24493)
     | > avg_decoder_loss:[92m 2.02214 [0m(-0.03261)
     | > avg_postnet_loss:[91m 2.09796 [0m(+0.04268)
     | > avg_stopnet_loss:[92m 1.50783 [0m(-0.01220)
     | > avg_decoder_coarse_loss:[92m 2.05900 [0m(-0.03092)
     | > avg_decoder_ddc_loss:[91m 0.00095 [0m(+0.00002)
     | > avg_ga_loss:[91m 0.00232 [0m(+0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.43074 [0m(-0.00121)
     | > avg_postnet_diff_spec_loss:[92m 0.80048 [0m(-0.00066)
     | > avg_decoder_ssim_loss:[92m 0.32221 [0m(-0.00101)
     | > avg_postnet_ssim_loss:[92m 0.33796 [0m(-0.00055)
     | > avg_loss:[92m 3.53729 [0m(-0.01824)
     | > avg_align_error:[91m 0.99062 [0m(+0.00005)


[4m[1m > EPOCH: 44/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 15:13:10) [0m

[1m   --> STEP: 3/87 -- GLOBAL_STEP: 273875[0m
     | > decoder_loss: 1.97673  (2.00854)
     | > postnet_loss: 1.71251  (1.77724)
     | > stopnet_loss: 1.80861  (1.50976)
     | > decoder_coarse_loss: 1.88992  (1.92069)
     | > decoder_ddc_loss: 0.00115  (0.00117)
     | > ga_loss: 0.00259  (0.00234)
     | > decoder_diff_spec_loss: 0.49335  (0.50864)
     | > postnet_diff_spec_loss: 0.82717  (0.83036)
     | > decoder_ssim_loss: 0.28585  (0.32976)
     | > postnet_ssim_loss: 0.29534  (0.34448)
     | > loss: 3.69206  (3.45166)
     | > align_error: 0.98960  (0.98947)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.90395  (3.82020)
     | > current_lr: 0.00010 
     | > step_time: 6.41920  (5.92258)
     | > loader_time: 0.02160  (0.04166)


[1m   --> STEP: 28/87 -- GLOBAL_STEP: 273900[0m
     | > decoder_loss: 1.83224  (2.01660)
     | > postnet_loss: 1.58537  (1.74701)
     | > stopnet_loss: 2.24995  (1.78482)
     | > decoder_coarse_loss: 1.77587  (1.93141)
     | > decoder_ddc_loss: 0.00111  (0.00119)
     | > ga_loss: 0.00242  (0.00245)
     | > decoder_diff_spec_loss: 0.49937  (0.50246)
     | > postnet_diff_spec_loss: 0.80872  (0.81319)
     | > decoder_ssim_loss: 0.23606  (0.30669)
     | > postnet_ssim_loss: 0.24163  (0.31685)
     | > loss: 4.00713  (3.70591)
     | > align_error: 0.99075  (0.98953)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.89853  (4.44440)
     | > current_lr: 0.00010 
     | > step_time: 3.90560  (4.07261)
     | > loader_time: 0.02630  (0.03782)


[1m   --> STEP: 53/87 -- GLOBAL_STEP: 273925[0m
     | > decoder_loss: 1.89882  (2.00294)
     | > postnet_loss: 1.62523  (1.73501)
     | > stopnet_loss: 1.75105  (1.73637)
     | > decoder_coarse_loss: 1.79877  (1.92272)
     | > decoder_ddc_loss: 0.00083  (0.00125)
     | > ga_loss: 0.00175  (0.00254)
     | > decoder_diff_spec_loss: 0.47370  (0.50150)
     | > postnet_diff_spec_loss: 0.79602  (0.81481)
     | > decoder_ssim_loss: 0.29562  (0.31330)
     | > postnet_ssim_loss: 0.30319  (0.32404)
     | > loss: 3.55786  (3.65297)
     | > align_error: 0.99208  (0.98903)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.34241  (4.26515)
     | > current_lr: 0.00010 
     | > step_time: 5.07510  (3.82016)
     | > loader_time: 0.03510  (0.03187)


[1m   --> STEP: 78/87 -- GLOBAL_STEP: 273950[0m
     | > decoder_loss: 2.18184  (1.99879)
     | > postnet_loss: 1.86478  (1.72836)
     | > stopnet_loss: 1.64810  (1.73719)
     | > decoder_coarse_loss: 2.14069  (1.91754)
     | > decoder_ddc_loss: 0.00095  (0.00119)
     | > ga_loss: 0.00201  (0.00246)
     | > decoder_diff_spec_loss: 0.54604  (0.50316)
     | > postnet_diff_spec_loss: 0.84302  (0.81521)
     | > decoder_ssim_loss: 0.30348  (0.30932)
     | > postnet_ssim_loss: 0.31482  (0.32005)
     | > loss: 3.70703  (3.64792)
     | > align_error: 0.99184  (0.98948)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.44703  (4.05682)
     | > current_lr: 0.00010 
     | > step_time: 3.15280  (3.81433)
     | > loader_time: 0.05970  (0.03290)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.51437 [0m(-2.25757)
     | > avg_decoder_loss:[91m 2.10611 [0m(+0.08397)
     | > avg_postnet_loss:[91m 2.14624 [0m(+0.04828)
     | > avg_stopnet_loss:[91m 1.50912 [0m(+0.00129)
     | > avg_decoder_coarse_loss:[91m 2.10348 [0m(+0.04447)
     | > avg_decoder_ddc_loss:[91m 0.00096 [0m(+0.00000)
     | > avg_ga_loss:[92m 0.00231 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[91m 0.43361 [0m(+0.00287)
     | > avg_postnet_diff_spec_loss:[92m 0.80017 [0m(-0.00031)
     | > avg_decoder_ssim_loss:[92m 0.32204 [0m(-0.00017)
     | > avg_postnet_ssim_loss:[92m 0.33693 [0m(-0.00103)
     | > avg_loss:[91m 3.58308 [0m(+0.04579)
     | > avg_align_error:[91m 0.99063 [0m(+0.00002)


[4m[1m > EPOCH: 45/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 15:20:20) [0m

[1m   --> STEP: 15/87 -- GLOBAL_STEP: 273975[0m
     | > decoder_loss: 1.93180  (1.99108)
     | > postnet_loss: 1.76984  (1.71780)
     | > stopnet_loss: 2.52364  (1.81453)
     | > decoder_coarse_loss: 1.90708  (1.91951)
     | > decoder_ddc_loss: 0.00091  (0.00118)
     | > ga_loss: 0.00233  (0.00241)
     | > decoder_diff_spec_loss: 0.48724  (0.49840)
     | > postnet_diff_spec_loss: 0.81398  (0.81016)
     | > decoder_ssim_loss: 0.22569  (0.30581)
     | > postnet_ssim_loss: 0.22930  (0.31613)
     | > loss: 4.37677  (3.71660)
     | > align_error: 0.99193  (0.98956)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 6.15042  (4.37147)
     | > current_lr: 0.00010 
     | > step_time: 3.88960  (4.27042)
     | > loader_time: 0.02300  (0.04376)


[1m   --> STEP: 40/87 -- GLOBAL_STEP: 274000[0m
     | > decoder_loss: 1.92513  (2.00737)
     | > postnet_loss: 1.68279  (1.72898)
     | > stopnet_loss: 1.33725  (1.74733)
     | > decoder_coarse_loss: 1.80427  (1.92704)
     | > decoder_ddc_loss: 0.00167  (0.00128)
     | > ga_loss: 0.00295  (0.00256)
     | > decoder_diff_spec_loss: 0.48598  (0.50508)
     | > postnet_diff_spec_loss: 0.80813  (0.81645)
     | > decoder_ssim_loss: 0.41905  (0.31646)
     | > postnet_ssim_loss: 0.44715  (0.32766)
     | > loss: 3.24556  (3.66773)
     | > align_error: 0.98687  (0.98884)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 6.17437  (5.19849)
     | > current_lr: 0.00010 
     | > step_time: 2.62300  (3.99777)
     | > loader_time: 0.01950  (0.03677)


[1m   --> STEP: 65/87 -- GLOBAL_STEP: 274025[0m
     | > decoder_loss: 1.78066  (1.97849)
     | > postnet_loss: 1.65450  (1.70233)
     | > stopnet_loss: 1.67772  (1.76631)
     | > decoder_coarse_loss: 1.73960  (1.90523)
     | > decoder_ddc_loss: 0.00087  (0.00125)
     | > ga_loss: 0.00204  (0.00250)
     | > decoder_diff_spec_loss: 0.45920  (0.50103)
     | > postnet_diff_spec_loss: 0.81566  (0.81448)
     | > decoder_ssim_loss: 0.29306  (0.31025)
     | > postnet_ssim_loss: 0.30775  (0.32147)
     | > loss: 3.45074  (3.66244)
     | > align_error: 0.99238  (0.98914)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.24647  (4.83500)
     | > current_lr: 0.00010 
     | > step_time: 4.17020  (4.00929)
     | > loader_time: 0.04610  (0.03722)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 5.10244 [0m(+1.58807)
     | > avg_decoder_loss:[92m 2.07526 [0m(-0.03085)
     | > avg_postnet_loss:[91m 2.40730 [0m(+0.26106)
     | > avg_stopnet_loss:[92m 1.50781 [0m(-0.00132)
     | > avg_decoder_coarse_loss:[92m 2.05071 [0m(-0.05276)
     | > avg_decoder_ddc_loss:[92m 0.00095 [0m(-0.00000)
     | > avg_ga_loss:[92m 0.00230 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.43306 [0m(-0.00056)
     | > avg_postnet_diff_spec_loss:[91m 0.80223 [0m(+0.00206)
     | > avg_decoder_ssim_loss:[92m 0.32120 [0m(-0.00084)
     | > avg_postnet_ssim_loss:[91m 0.34043 [0m(+0.00350)
     | > avg_loss:[91m 3.62712 [0m(+0.04404)
     | > avg_align_error:[92m 0.99052 [0m(-0.00011)


[4m[1m > EPOCH: 46/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 15:28:03) [0m

[1m   --> STEP: 2/87 -- GLOBAL_STEP: 274050[0m
     | > decoder_loss: 2.09118  (1.98665)
     | > postnet_loss: 1.81708  (1.77926)
     | > stopnet_loss: 1.40455  (1.38282)
     | > decoder_coarse_loss: 2.01480  (1.91004)
     | > decoder_ddc_loss: 0.00126  (0.00118)
     | > ga_loss: 0.00236  (0.00223)
     | > decoder_diff_spec_loss: 0.53666  (0.52160)
     | > postnet_diff_spec_loss: 0.85094  (0.83312)
     | > decoder_ssim_loss: 0.34287  (0.35064)
     | > postnet_ssim_loss: 0.36267  (0.36766)
     | > loss: 3.42073  (3.33152)
     | > align_error: 0.98839  (0.98936)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.51431  (2.90733)
     | > current_lr: 0.00010 
     | > step_time: 4.30770  (5.64113)
     | > loader_time: 0.05170  (0.03626)


[1m   --> STEP: 27/87 -- GLOBAL_STEP: 274075[0m
     | > decoder_loss: 1.95317  (1.99131)
     | > postnet_loss: 1.63238  (1.72305)
     | > stopnet_loss: 1.47327  (1.74320)
     | > decoder_coarse_loss: 1.92789  (1.92061)
     | > decoder_ddc_loss: 0.00109  (0.00117)
     | > ga_loss: 0.00227  (0.00244)
     | > decoder_diff_spec_loss: 0.50087  (0.50154)
     | > postnet_diff_spec_loss: 0.81359  (0.81254)
     | > decoder_ssim_loss: 0.33230  (0.30790)
     | > postnet_ssim_loss: 0.33886  (0.31877)
     | > loss: 3.35966  (3.64960)
     | > align_error: 0.99090  (0.98944)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.37791  (4.10704)
     | > current_lr: 0.00010 
     | > step_time: 3.57220  (4.07970)
     | > loader_time: 0.02850  (0.03601)


[1m   --> STEP: 52/87 -- GLOBAL_STEP: 274100[0m
     | > decoder_loss: 2.05606  (1.97792)
     | > postnet_loss: 1.62628  (1.70654)
     | > stopnet_loss: 1.55845  (1.72888)
     | > decoder_coarse_loss: 2.02295  (1.90425)
     | > decoder_ddc_loss: 0.00137  (0.00125)
     | > ga_loss: 0.00231  (0.00254)
     | > decoder_diff_spec_loss: 0.52835  (0.50113)
     | > postnet_diff_spec_loss: 0.80987  (0.81475)
     | > decoder_ssim_loss: 0.31341  (0.31221)
     | > postnet_ssim_loss: 0.32165  (0.32385)
     | > loss: 3.48998  (3.62706)
     | > align_error: 0.98824  (0.98895)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.92214  (4.13866)
     | > current_lr: 0.00010 
     | > step_time: 3.62940  (3.88848)
     | > loader_time: 0.01950  (0.03682)


[1m   --> STEP: 77/87 -- GLOBAL_STEP: 274125[0m
     | > decoder_loss: 1.95957  (1.97123)
     | > postnet_loss: 1.77300  (1.69096)
     | > stopnet_loss: 1.58818  (1.74020)
     | > decoder_coarse_loss: 1.86987  (1.89464)
     | > decoder_ddc_loss: 0.00088  (0.00120)
     | > ga_loss: 0.00223  (0.00246)
     | > decoder_diff_spec_loss: 0.50853  (0.50207)
     | > postnet_diff_spec_loss: 0.84091  (0.81427)
     | > decoder_ssim_loss: 0.31849  (0.30803)
     | > postnet_ssim_loss: 0.32788  (0.31954)
     | > loss: 3.49913  (3.62796)
     | > align_error: 0.99213  (0.98943)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.53835  (3.97214)
     | > current_lr: 0.00010 
     | > step_time: 3.97970  (3.89405)
     | > loader_time: 0.01760  (0.03705)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 4.89155 [0m(-0.21090)
     | > avg_decoder_loss:[92m 2.01120 [0m(-0.06406)
     | > avg_postnet_loss:[92m 2.37101 [0m(-0.03629)
     | > avg_stopnet_loss:[91m 1.51119 [0m(+0.00338)
     | > avg_decoder_coarse_loss:[91m 2.05644 [0m(+0.00573)
     | > avg_decoder_ddc_loss:[91m 0.00098 [0m(+0.00002)
     | > avg_ga_loss:[92m 0.00230 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.42944 [0m(-0.00362)
     | > avg_postnet_diff_spec_loss:[92m 0.80171 [0m(-0.00052)
     | > avg_decoder_ssim_loss:[92m 0.32025 [0m(-0.00096)
     | > avg_postnet_ssim_loss:[92m 0.33981 [0m(-0.00062)
     | > avg_loss:[92m 3.60540 [0m(-0.02172)
     | > avg_align_error:[91m 0.99058 [0m(+0.00006)


[4m[1m > EPOCH: 47/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 15:35:28) [0m

[1m   --> STEP: 14/87 -- GLOBAL_STEP: 274150[0m
     | > decoder_loss: 2.20249  (1.97296)
     | > postnet_loss: 1.93507  (1.67610)
     | > stopnet_loss: 1.87153  (1.73335)
     | > decoder_coarse_loss: 2.09639  (1.90308)
     | > decoder_ddc_loss: 0.00106  (0.00119)
     | > ga_loss: 0.00223  (0.00241)
     | > decoder_diff_spec_loss: 0.52462  (0.49798)
     | > postnet_diff_spec_loss: 0.80316  (0.80887)
     | > decoder_ssim_loss: 0.28868  (0.31034)
     | > postnet_ssim_loss: 0.30117  (0.32116)
     | > loss: 3.92085  (3.61830)
     | > align_error: 0.99075  (0.98942)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.51898  (4.44106)
     | > current_lr: 0.00010 
     | > step_time: 2.98410  (4.20577)
     | > loader_time: 0.02160  (0.04620)


[1m   --> STEP: 39/87 -- GLOBAL_STEP: 274175[0m
     | > decoder_loss: 1.91941  (1.98843)
     | > postnet_loss: 1.59610  (1.69547)
     | > stopnet_loss: 1.53067  (1.71615)
     | > decoder_coarse_loss: 1.85062  (1.91448)
     | > decoder_ddc_loss: 0.00160  (0.00126)
     | > ga_loss: 0.00310  (0.00254)
     | > decoder_diff_spec_loss: 0.50085  (0.50456)
     | > postnet_diff_spec_loss: 0.81787  (0.81583)
     | > decoder_ssim_loss: 0.36434  (0.31253)
     | > postnet_ssim_loss: 0.37711  (0.32369)
     | > loss: 3.40313  (3.61792)
     | > align_error: 0.98587  (0.98891)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.81009  (4.53943)
     | > current_lr: 0.00010 
     | > step_time: 2.96960  (3.85647)
     | > loader_time: 0.02020  (0.03570)


[1m   --> STEP: 64/87 -- GLOBAL_STEP: 274200[0m
     | > decoder_loss: 1.94055  (1.96465)
     | > postnet_loss: 1.70388  (1.67064)
     | > stopnet_loss: 1.42899  (1.73012)
     | > decoder_coarse_loss: 1.86769  (1.89090)
     | > decoder_ddc_loss: 0.00132  (0.00123)
     | > ga_loss: 0.00243  (0.00250)
     | > decoder_diff_spec_loss: 0.50867  (0.50129)
     | > postnet_diff_spec_loss: 0.81652  (0.81372)
     | > decoder_ssim_loss: 0.33720  (0.30931)
     | > postnet_ssim_loss: 0.34795  (0.32090)
     | > loss: 3.32210  (3.61077)
     | > align_error: 0.98834  (0.98913)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.67117  (4.37549)
     | > current_lr: 0.00010 
     | > step_time: 3.84790  (3.86642)
     | > loader_time: 0.01730  (0.03289)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 5.25216 [0m(+0.36062)
     | > avg_decoder_loss:[92m 2.00504 [0m(-0.00617)
     | > avg_postnet_loss:[92m 2.23517 [0m(-0.13584)
     | > avg_stopnet_loss:[92m 1.50539 [0m(-0.00580)
     | > avg_decoder_coarse_loss:[92m 1.95360 [0m(-0.10284)
     | > avg_decoder_ddc_loss:[91m 0.00098 [0m(+0.00000)
     | > avg_ga_loss:[92m 0.00230 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[92m 0.42879 [0m(-0.00065)
     | > avg_postnet_diff_spec_loss:[92m 0.80007 [0m(-0.00164)
     | > avg_decoder_ssim_loss:[92m 0.31924 [0m(-0.00101)
     | > avg_postnet_ssim_loss:[92m 0.33785 [0m(-0.00196)
     | > avg_loss:[92m 3.53706 [0m(-0.06834)
     | > avg_align_error:[91m 0.99063 [0m(+0.00005)


[4m[1m > EPOCH: 48/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 15:43:09) [0m

[1m   --> STEP: 1/87 -- GLOBAL_STEP: 274225[0m
     | > decoder_loss: 1.87165  (1.87165)
     | > postnet_loss: 1.63658  (1.63658)
     | > stopnet_loss: 1.28631  (1.28631)
     | > decoder_coarse_loss: 1.80965  (1.80965)
     | > decoder_ddc_loss: 0.00113  (0.00113)
     | > ga_loss: 0.00207  (0.00207)
     | > decoder_diff_spec_loss: 0.50347  (0.50347)
     | > postnet_diff_spec_loss: 0.81403  (0.81403)
     | > decoder_ssim_loss: 0.35737  (0.35737)
     | > postnet_ssim_loss: 0.37201  (0.37201)
     | > loss: 3.13813  (3.13813)
     | > align_error: 0.99045  (0.99045)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.27659  (2.27659)
     | > current_lr: 0.00010 
     | > step_time: 6.75130  (6.75128)
     | > loader_time: 0.02200  (0.02204)


[1m   --> STEP: 26/87 -- GLOBAL_STEP: 274250[0m
     | > decoder_loss: 2.12744  (1.98587)
     | > postnet_loss: 1.80045  (1.67299)
     | > stopnet_loss: 1.75520  (1.75413)
     | > decoder_coarse_loss: 2.02121  (1.89194)
     | > decoder_ddc_loss: 0.00091  (0.00118)
     | > ga_loss: 0.00193  (0.00242)
     | > decoder_diff_spec_loss: 0.53422  (0.50103)
     | > postnet_diff_spec_loss: 0.80986  (0.81118)
     | > decoder_ssim_loss: 0.27607  (0.30631)
     | > postnet_ssim_loss: 0.29215  (0.31690)
     | > loss: 3.73042  (3.63806)
     | > align_error: 0.99207  (0.98943)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.84229  (3.98778)
     | > current_lr: 0.00010 
     | > step_time: 4.66690  (4.13953)
     | > loader_time: 0.02170  (0.04452)


[1m   --> STEP: 51/87 -- GLOBAL_STEP: 274275[0m
     | > decoder_loss: 1.85354  (1.96658)
     | > postnet_loss: 1.67162  (1.66305)
     | > stopnet_loss: 1.67234  (1.72183)
     | > decoder_coarse_loss: 1.77131  (1.87764)
     | > decoder_ddc_loss: 0.00151  (0.00127)
     | > ga_loss: 0.00262  (0.00252)
     | > decoder_diff_spec_loss: 0.47413  (0.49949)
     | > postnet_diff_spec_loss: 0.80402  (0.81371)
     | > decoder_ssim_loss: 0.29994  (0.31128)
     | > postnet_ssim_loss: 0.31495  (0.32292)
     | > loss: 3.48320  (3.59842)
     | > align_error: 0.98746  (0.98895)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.10648  (4.26891)
     | > current_lr: 0.00010 
     | > step_time: 3.69540  (3.90476)
     | > loader_time: 0.02420  (0.03935)


[1m   --> STEP: 76/87 -- GLOBAL_STEP: 274300[0m
     | > decoder_loss: 1.96628  (1.95885)
     | > postnet_loss: 1.55189  (1.65048)
     | > stopnet_loss: 2.03976  (1.72582)
     | > decoder_coarse_loss: 1.85752  (1.87190)
     | > decoder_ddc_loss: 0.00090  (0.00121)
     | > ga_loss: 0.00210  (0.00244)
     | > decoder_diff_spec_loss: 0.51921  (0.50104)
     | > postnet_diff_spec_loss: 0.80653  (0.81293)
     | > decoder_ssim_loss: 0.25933  (0.30683)
     | > postnet_ssim_loss: 0.26692  (0.31854)
     | > loss: 3.85741  (3.59347)
     | > align_error: 0.99132  (0.98938)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.23546  (4.00126)
     | > current_lr: 0.00010 
     | > step_time: 3.98220  (3.96525)
     | > loader_time: 0.02260  (0.03705)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.99388 [0m(-1.25829)
     | > avg_decoder_loss:[92m 1.97313 [0m(-0.03191)
     | > avg_postnet_loss:[92m 2.15277 [0m(-0.08240)
     | > avg_stopnet_loss:[91m 1.50868 [0m(+0.00330)
     | > avg_decoder_coarse_loss:[91m 1.99216 [0m(+0.03855)
     | > avg_decoder_ddc_loss:[91m 0.00098 [0m(+0.00000)
     | > avg_ga_loss:[92m 0.00229 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.42682 [0m(-0.00197)
     | > avg_postnet_diff_spec_loss:[92m 0.79931 [0m(-0.00076)
     | > avg_decoder_ssim_loss:[92m 0.31851 [0m(-0.00073)
     | > avg_postnet_ssim_loss:[91m 0.33806 [0m(+0.00021)
     | > avg_loss:[92m 3.52056 [0m(-0.01650)
     | > avg_align_error:[92m 0.99058 [0m(-0.00005)


[4m[1m > EPOCH: 49/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 15:50:38) [0m

[1m   --> STEP: 13/87 -- GLOBAL_STEP: 274325[0m
     | > decoder_loss: 2.09438  (1.94082)
     | > postnet_loss: 1.77884  (1.62764)
     | > stopnet_loss: 2.14024  (1.71599)
     | > decoder_coarse_loss: 2.00010  (1.86840)
     | > decoder_ddc_loss: 0.00151  (0.00122)
     | > ga_loss: 0.00265  (0.00241)
     | > decoder_diff_spec_loss: 0.48890  (0.49532)
     | > postnet_diff_spec_loss: 0.80679  (0.80887)
     | > decoder_ssim_loss: 0.24331  (0.31110)
     | > postnet_ssim_loss: 0.25425  (0.32242)
     | > loss: 4.07051  (3.57198)
     | > align_error: 0.98706  (0.98928)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.10592  (4.43626)
     | > current_lr: 0.00010 
     | > step_time: 4.50770  (4.27209)
     | > loader_time: 0.05130  (0.02781)


[1m   --> STEP: 38/87 -- GLOBAL_STEP: 274350[0m
     | > decoder_loss: 2.16101  (1.96744)
     | > postnet_loss: 1.83587  (1.66270)
     | > stopnet_loss: 1.82108  (1.73146)
     | > decoder_coarse_loss: 2.11187  (1.89299)
     | > decoder_ddc_loss: 0.00146  (0.00127)
     | > ga_loss: 0.00230  (0.00252)
     | > decoder_diff_spec_loss: 0.55890  (0.50373)
     | > postnet_diff_spec_loss: 0.84671  (0.81510)
     | > decoder_ssim_loss: 0.28569  (0.30993)
     | > postnet_ssim_loss: 0.30156  (0.32159)
     | > loss: 3.85838  (3.61272)
     | > align_error: 0.98765  (0.98894)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.87327  (4.33854)
     | > current_lr: 0.00010 
     | > step_time: 4.61840  (3.87698)
     | > loader_time: 0.02220  (0.02652)


 > CHECKPOINT : /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf/checkpoint_274350.pth.tar

[1m   --> STEP: 63/87 -- GLOBAL_STEP: 274375[0m
     | > decoder_loss: 1.84296  (1.94386)
     | > postnet_loss: 1.58292  (1.64014)
     | > stopnet_loss: 2.34423  (1.73420)
     | > decoder_coarse_loss: 1.74434  (1.87020)
     | > decoder_ddc_loss: 0.00120  (0.00127)
     | > ga_loss: 0.00271  (0.00249)
     | > decoder_diff_spec_loss: 0.48863  (0.50007)
     | > postnet_diff_spec_loss: 0.80349  (0.81295)
     | > decoder_ssim_loss: 0.24084  (0.30754)
     | > postnet_ssim_loss: 0.24795  (0.31968)
     | > loss: 4.09585  (3.59555)
     | > align_error: 0.98924  (0.98907)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 6.96150  (4.24484)
     | > current_lr: 0.00010 
     | > step_time: 4.25690  (3.87990)
     | > loader_time: 0.01860  (0.03750)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 4.22227 [0m(+0.22839)
     | > avg_decoder_loss:[91m 1.99926 [0m(+0.02614)
     | > avg_postnet_loss:[91m 2.41661 [0m(+0.26384)
     | > avg_stopnet_loss:[92m 1.50317 [0m(-0.00551)
     | > avg_decoder_coarse_loss:[92m 1.92107 [0m(-0.07109)
     | > avg_decoder_ddc_loss:[92m 0.00096 [0m(-0.00002)
     | > avg_ga_loss:[92m 0.00228 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[91m 0.42872 [0m(+0.00190)
     | > avg_postnet_diff_spec_loss:[91m 0.80014 [0m(+0.00083)
     | > avg_decoder_ssim_loss:[92m 0.31780 [0m(-0.00071)
     | > avg_postnet_ssim_loss:[91m 0.33910 [0m(+0.00104)
     | > avg_loss:[91m 3.57050 [0m(+0.04994)
     | > avg_align_error:[91m 0.99064 [0m(+0.00006)


[4m[1m > EPOCH: 50/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 15:58:32) [0m

[1m   --> STEP: 0/87 -- GLOBAL_STEP: 274400[0m
     | > decoder_loss: 1.92477  (1.92477)
     | > postnet_loss: 1.67928  (1.67928)
     | > stopnet_loss: 1.01646  (1.01646)
     | > decoder_coarse_loss: 1.86843  (1.86843)
     | > decoder_ddc_loss: 0.00148  (0.00148)
     | > ga_loss: 0.00251  (0.00251)
     | > decoder_diff_spec_loss: 0.48242  (0.48242)
     | > postnet_diff_spec_loss: 0.81120  (0.81120)
     | > decoder_ssim_loss: 0.43460  (0.43460)
     | > postnet_ssim_loss: 0.45428  (0.45428)
     | > loss: 2.94310  (2.94310)
     | > align_error: 0.98769  (0.98769)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.34354  (2.34354)
     | > current_lr: 0.00010 
     | > step_time: 3.83370  (3.83370)
     | > loader_time: 9.26030  (9.26026)


[1m   --> STEP: 25/87 -- GLOBAL_STEP: 274425[0m
     | > decoder_loss: 2.18288  (1.95468)
     | > postnet_loss: 1.69056  (1.64541)
     | > stopnet_loss: 1.95183  (1.73364)
     | > decoder_coarse_loss: 2.11442  (1.87191)
     | > decoder_ddc_loss: 0.00109  (0.00121)
     | > ga_loss: 0.00220  (0.00243)
     | > decoder_diff_spec_loss: 0.55518  (0.49833)
     | > postnet_diff_spec_loss: 0.82782  (0.81103)
     | > decoder_ssim_loss: 0.26651  (0.30607)
     | > postnet_ssim_loss: 0.26993  (0.31746)
     | > loss: 3.93994  (3.59732)
     | > align_error: 0.99030  (0.98922)
     | > amp_scaler: 65536.00000  (51118.08000)
     | > grad_norm: 2.76009  (3.87299)
     | > current_lr: 0.00010 
     | > step_time: 4.60640  (4.22266)
     | > loader_time: 0.02780  (0.04899)


[1m   --> STEP: 50/87 -- GLOBAL_STEP: 274450[0m
     | > decoder_loss: 1.81380  (1.94535)
     | > postnet_loss: 1.63975  (1.63980)
     | > stopnet_loss: 1.60774  (1.71464)
     | > decoder_coarse_loss: 1.76552  (1.86315)
     | > decoder_ddc_loss: 0.00164  (0.00129)
     | > ga_loss: 0.00263  (0.00251)
     | > decoder_diff_spec_loss: 0.46262  (0.49925)
     | > postnet_diff_spec_loss: 0.80713  (0.81316)
     | > decoder_ssim_loss: 0.31079  (0.31012)
     | > postnet_ssim_loss: 0.32317  (0.32248)
     | > loss: 3.40202  (3.57585)
     | > align_error: 0.98840  (0.98890)
     | > amp_scaler: 32768.00000  (48496.64000)
     | > grad_norm: 2.30590  (3.63484)
     | > current_lr: 0.00009 
     | > step_time: 3.92810  (3.90621)
     | > loader_time: 0.05750  (0.03895)


[1m   --> STEP: 75/87 -- GLOBAL_STEP: 274475[0m
     | > decoder_loss: 2.00135  (1.93651)
     | > postnet_loss: 1.61017  (1.62712)
     | > stopnet_loss: 2.06973  (1.71698)
     | > decoder_coarse_loss: 1.90671  (1.85737)
     | > decoder_ddc_loss: 0.00108  (0.00124)
     | > ga_loss: 0.00226  (0.00243)
     | > decoder_diff_spec_loss: 0.52084  (0.49953)
     | > postnet_diff_spec_loss: 0.82314  (0.81210)
     | > decoder_ssim_loss: 0.24687  (0.30615)
     | > postnet_ssim_loss: 0.25760  (0.31844)
     | > loss: 3.92297  (3.56877)
     | > align_error: 0.99041  (0.98927)
     | > amp_scaler: 32768.00000  (43253.76000)
     | > grad_norm: 3.21263  (3.70622)
     | > current_lr: 0.00009 
     | > step_time: 4.59840  (3.93736)
     | > loader_time: 0.02160  (0.03603)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 4.58899 [0m(+0.36672)
     | > avg_decoder_loss:[92m 1.99027 [0m(-0.00899)
     | > avg_postnet_loss:[92m 2.28607 [0m(-0.13054)
     | > avg_stopnet_loss:[92m 1.49973 [0m(-0.00344)
     | > avg_decoder_coarse_loss:[91m 1.95237 [0m(+0.03131)
     | > avg_decoder_ddc_loss:[91m 0.00098 [0m(+0.00002)
     | > avg_ga_loss:[92m 0.00228 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.42880 [0m(+0.00008)
     | > avg_postnet_diff_spec_loss:[92m 0.79923 [0m(-0.00091)
     | > avg_decoder_ssim_loss:[92m 0.31697 [0m(-0.00084)
     | > avg_postnet_ssim_loss:[92m 0.33812 [0m(-0.00097)
     | > avg_loss:[92m 3.53933 [0m(-0.03117)
     | > avg_align_error:[91m 0.99065 [0m(+0.00001)


[4m[1m > EPOCH: 51/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 16:06:15) [0m

[1m   --> STEP: 12/87 -- GLOBAL_STEP: 274500[0m
     | > decoder_loss: 2.05769  (1.91760)
     | > postnet_loss: 1.58302  (1.59920)
     | > stopnet_loss: 2.20285  (1.67940)
     | > decoder_coarse_loss: 1.96658  (1.83956)
     | > decoder_ddc_loss: 0.00106  (0.00121)
     | > ga_loss: 0.00238  (0.00238)
     | > decoder_diff_spec_loss: 0.54385  (0.49687)
     | > postnet_diff_spec_loss: 0.81929  (0.80781)
     | > decoder_ssim_loss: 0.23093  (0.31550)
     | > postnet_ssim_loss: 0.23452  (0.32729)
     | > loss: 4.07399  (3.51755)
     | > align_error: 0.99135  (0.98940)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.55529  (4.29894)
     | > current_lr: 0.00009 
     | > step_time: 4.80850  (4.46280)
     | > loader_time: 0.02670  (0.04014)


[1m   --> STEP: 37/87 -- GLOBAL_STEP: 274525[0m
     | > decoder_loss: 2.07804  (1.95078)
     | > postnet_loss: 1.74837  (1.63884)
     | > stopnet_loss: 1.58358  (1.70642)
     | > decoder_coarse_loss: 1.97086  (1.87189)
     | > decoder_ddc_loss: 0.00117  (0.00128)
     | > ga_loss: 0.00249  (0.00251)
     | > decoder_diff_spec_loss: 0.50152  (0.50118)
     | > postnet_diff_spec_loss: 0.81358  (0.81312)
     | > decoder_ssim_loss: 0.32534  (0.30948)
     | > postnet_ssim_loss: 0.34024  (0.32130)
     | > loss: 3.54082  (3.57093)
     | > align_error: 0.99008  (0.98895)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.28971  (4.31495)
     | > current_lr: 0.00009 
     | > step_time: 2.71530  (3.97061)
     | > loader_time: 0.01770  (0.03583)


[1m   --> STEP: 62/87 -- GLOBAL_STEP: 274550[0m
     | > decoder_loss: 1.67565  (1.92915)
     | > postnet_loss: 1.41823  (1.62002)
     | > stopnet_loss: 2.00049  (1.70614)
     | > decoder_coarse_loss: 1.60486  (1.85270)
     | > decoder_ddc_loss: 0.00096  (0.00128)
     | > ga_loss: 0.00220  (0.00247)
     | > decoder_diff_spec_loss: 0.45402  (0.49929)
     | > postnet_diff_spec_loss: 0.78164  (0.81192)
     | > decoder_ssim_loss: 0.25899  (0.30745)
     | > postnet_ssim_loss: 0.26829  (0.32004)
     | > loss: 3.62715  (3.55394)
     | > align_error: 0.99161  (0.98904)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.88508  (4.36287)
     | > current_lr: 0.00009 
     | > step_time: 3.88940  (3.93758)
     | > loader_time: 0.01940  (0.03792)


[1m   --> STEP: 87/87 -- GLOBAL_STEP: 274575[0m
     | > decoder_loss: 1.65994  (1.92874)
     | > postnet_loss: 1.50085  (1.61760)
     | > stopnet_loss: 1.06305  (1.70967)
     | > decoder_coarse_loss: 1.65148  (1.85420)
     | > decoder_ddc_loss: 0.00130  (0.00123)
     | > ga_loss: 0.00183  (0.00239)
     | > decoder_diff_spec_loss: 0.47715  (0.50136)
     | > postnet_diff_spec_loss: 0.81921  (0.81256)
     | > decoder_ssim_loss: 0.40232  (0.30652)
     | > postnet_ssim_loss: 0.41017  (0.31902)
     | > loss: 2.80280  (3.55690)
     | > align_error: 0.99062  (0.98945)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.03942  (4.19123)
     | > current_lr: 0.00009 
     | > step_time: 2.20330  (3.82172)
     | > loader_time: 0.00700  (0.03531)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.98864 [0m(-0.60035)
     | > avg_decoder_loss:[92m 1.94734 [0m(-0.04293)
     | > avg_postnet_loss:[92m 2.05526 [0m(-0.23081)
     | > avg_stopnet_loss:[91m 1.50168 [0m(+0.00194)
     | > avg_decoder_coarse_loss:[92m 1.90504 [0m(-0.04733)
     | > avg_decoder_ddc_loss:[92m 0.00098 [0m(-0.00000)
     | > avg_ga_loss:[92m 0.00228 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[92m 0.42631 [0m(-0.00248)
     | > avg_postnet_diff_spec_loss:[92m 0.79749 [0m(-0.00174)
     | > avg_decoder_ssim_loss:[92m 0.31608 [0m(-0.00089)
     | > avg_postnet_ssim_loss:[92m 0.33478 [0m(-0.00334)
     | > avg_loss:[92m 3.45889 [0m(-0.08045)
     | > avg_align_error:[91m 0.99071 [0m(+0.00006)


[4m[1m > EPOCH: 52/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 16:13:43) [0m

[1m   --> STEP: 24/87 -- GLOBAL_STEP: 274600[0m
     | > decoder_loss: 2.17625  (1.92803)
     | > postnet_loss: 1.72476  (1.62094)
     | > stopnet_loss: 1.95950  (1.72931)
     | > decoder_coarse_loss: 2.08906  (1.84864)
     | > decoder_ddc_loss: 0.00116  (0.00121)
     | > ga_loss: 0.00254  (0.00242)
     | > decoder_diff_spec_loss: 0.56533  (0.49629)
     | > postnet_diff_spec_loss: 0.85354  (0.80953)
     | > decoder_ssim_loss: 0.26505  (0.30650)
     | > postnet_ssim_loss: 0.27279  (0.31861)
     | > loss: 3.95919  (3.57387)
     | > align_error: 0.98948  (0.98919)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.63106  (4.24128)
     | > current_lr: 0.00009 
     | > step_time: 4.53130  (4.01196)
     | > loader_time: 0.05700  (0.04311)


[1m   --> STEP: 49/87 -- GLOBAL_STEP: 274625[0m
     | > decoder_loss: 1.86884  (1.93241)
     | > postnet_loss: 1.57842  (1.62108)
     | > stopnet_loss: 1.70062  (1.70994)
     | > decoder_coarse_loss: 1.78142  (1.85490)
     | > decoder_ddc_loss: 0.00119  (0.00128)
     | > ga_loss: 0.00237  (0.00250)
     | > decoder_diff_spec_loss: 0.47908  (0.49921)
     | > postnet_diff_spec_loss: 0.79246  (0.81277)
     | > decoder_ssim_loss: 0.29403  (0.30892)
     | > postnet_ssim_loss: 0.30733  (0.32170)
     | > loss: 3.48817  (3.56049)
     | > align_error: 0.98979  (0.98894)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.64984  (4.32658)
     | > current_lr: 0.00009 
     | > step_time: 4.45220  (3.81332)
     | > loader_time: 0.08040  (0.04066)


[1m   --> STEP: 74/87 -- GLOBAL_STEP: 274650[0m
     | > decoder_loss: 1.98896  (1.92131)
     | > postnet_loss: 1.66451  (1.60871)
     | > stopnet_loss: 1.55515  (1.70654)
     | > decoder_coarse_loss: 1.91699  (1.84464)
     | > decoder_ddc_loss: 0.00096  (0.00123)
     | > ga_loss: 0.00200  (0.00243)
     | > decoder_diff_spec_loss: 0.50289  (0.49906)
     | > postnet_diff_spec_loss: 0.79670  (0.81135)
     | > decoder_ssim_loss: 0.31172  (0.30584)
     | > postnet_ssim_loss: 0.32537  (0.31857)
     | > loss: 3.44216  (3.54635)
     | > align_error: 0.99221  (0.98929)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.16103  (4.11771)
     | > current_lr: 0.00009 
     | > step_time: 5.48220  (3.87945)
     | > loader_time: 0.06170  (0.03970)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.61400 [0m(-0.37464)
     | > avg_decoder_loss:[92m 1.91973 [0m(-0.02762)
     | > avg_postnet_loss:[92m 2.02693 [0m(-0.02833)
     | > avg_stopnet_loss:[92m 1.49975 [0m(-0.00192)
     | > avg_decoder_coarse_loss:[92m 1.85998 [0m(-0.04506)
     | > avg_decoder_ddc_loss:[92m 0.00094 [0m(-0.00004)
     | > avg_ga_loss:[92m 0.00227 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.42673 [0m(+0.00041)
     | > avg_postnet_diff_spec_loss:[92m 0.79748 [0m(-0.00001)
     | > avg_decoder_ssim_loss:[92m 0.31531 [0m(-0.00076)
     | > avg_postnet_ssim_loss:[91m 0.33503 [0m(+0.00025)
     | > avg_loss:[92m 3.43165 [0m(-0.02723)
     | > avg_align_error:[92m 0.99070 [0m(-0.00001)


[4m[1m > EPOCH: 53/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 16:20:56) [0m

[1m   --> STEP: 11/87 -- GLOBAL_STEP: 274675[0m
     | > decoder_loss: 1.90290  (1.88156)
     | > postnet_loss: 1.58674  (1.56808)
     | > stopnet_loss: 1.20932  (1.59578)
     | > decoder_coarse_loss: 1.81245  (1.80842)
     | > decoder_ddc_loss: 0.00122  (0.00118)
     | > ga_loss: 0.00223  (0.00237)
     | > decoder_diff_spec_loss: 0.47172  (0.49044)
     | > postnet_diff_spec_loss: 0.78416  (0.80552)
     | > decoder_ssim_loss: 0.38397  (0.32180)
     | > postnet_ssim_loss: 0.40207  (0.33457)
     | > loss: 3.05680  (3.41053)
     | > align_error: 0.98994  (0.98931)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.57173  (4.26046)
     | > current_lr: 0.00009 
     | > step_time: 3.89850  (4.33535)
     | > loader_time: 0.02800  (0.05145)


[1m   --> STEP: 36/87 -- GLOBAL_STEP: 274700[0m
     | > decoder_loss: 1.88990  (1.92954)
     | > postnet_loss: 1.61116  (1.61242)
     | > stopnet_loss: 1.30897  (1.70194)
     | > decoder_coarse_loss: 1.77514  (1.84728)
     | > decoder_ddc_loss: 0.00286  (0.00124)
     | > ga_loss: 0.00470  (0.00250)
     | > decoder_diff_spec_loss: 0.48626  (0.50036)
     | > postnet_diff_spec_loss: 0.82037  (0.81227)
     | > decoder_ssim_loss: 0.42055  (0.30774)
     | > postnet_ssim_loss: 0.43992  (0.31994)
     | > loss: 3.19402  (3.54716)
     | > align_error: 0.97837  (0.98902)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 10.44446  (4.28431)
     | > current_lr: 0.00009 
     | > step_time: 1.70010  (3.90912)
     | > loader_time: 0.05060  (0.04165)


[1m   --> STEP: 61/87 -- GLOBAL_STEP: 274725[0m
     | > decoder_loss: 1.78878  (1.91832)
     | > postnet_loss: 1.51567  (1.60029)
     | > stopnet_loss: 1.72989  (1.69459)
     | > decoder_coarse_loss: 1.69795  (1.83719)
     | > decoder_ddc_loss: 0.00135  (0.00124)
     | > ga_loss: 0.00280  (0.00247)
     | > decoder_diff_spec_loss: 0.44825  (0.49886)
     | > postnet_diff_spec_loss: 0.78140  (0.81159)
     | > decoder_ssim_loss: 0.29836  (0.30719)
     | > postnet_ssim_loss: 0.31189  (0.32009)
     | > loss: 3.45479  (3.53064)
     | > align_error: 0.98801  (0.98912)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.11360  (4.05199)
     | > current_lr: 0.00009 
     | > step_time: 4.21180  (3.78531)
     | > loader_time: 0.07400  (0.03690)


[1m   --> STEP: 86/87 -- GLOBAL_STEP: 274750[0m
     | > decoder_loss: 1.94303  (1.91867)
     | > postnet_loss: 1.52130  (1.59630)
     | > stopnet_loss: 0.76770  (1.70349)
     | > decoder_coarse_loss: 1.86163  (1.83843)
     | > decoder_ddc_loss: 0.00174  (0.00119)
     | > ga_loss: 0.00260  (0.00239)
     | > decoder_diff_spec_loss: 0.50096  (0.50065)
     | > postnet_diff_spec_loss: 0.80124  (0.81155)
     | > decoder_ssim_loss: 0.53868  (0.30445)
     | > postnet_ssim_loss: 0.55867  (0.31711)
     | > loss: 2.71251  (3.53752)
     | > align_error: 0.98702  (0.98953)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.94349  (3.91469)
     | > current_lr: 0.00009 
     | > step_time: 1.90260  (3.68945)
     | > loader_time: 0.01560  (0.03442)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.85258 [0m(+0.23858)
     | > avg_decoder_loss:[91m 1.96710 [0m(+0.04737)
     | > avg_postnet_loss:[91m 2.26657 [0m(+0.23964)
     | > avg_stopnet_loss:[92m 1.49545 [0m(-0.00431)
     | > avg_decoder_coarse_loss:[91m 1.91059 [0m(+0.05060)
     | > avg_decoder_ddc_loss:[91m 0.00094 [0m(+0.00001)
     | > avg_ga_loss:[91m 0.00228 [0m(+0.00000)
     | > avg_decoder_diff_spec_loss:[92m 0.42602 [0m(-0.00071)
     | > avg_postnet_diff_spec_loss:[91m 0.79790 [0m(+0.00042)
     | > avg_decoder_ssim_loss:[92m 0.31484 [0m(-0.00047)
     | > avg_postnet_ssim_loss:[91m 0.33688 [0m(+0.00185)
     | > avg_loss:[91m 3.51203 [0m(+0.08038)
     | > avg_align_error:[91m 0.99074 [0m(+0.00004)


[4m[1m > EPOCH: 54/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 16:28:04) [0m

[1m   --> STEP: 23/87 -- GLOBAL_STEP: 274775[0m
     | > decoder_loss: 1.93115  (1.90189)
     | > postnet_loss: 1.62304  (1.58731)
     | > stopnet_loss: 1.11388  (1.71450)
     | > decoder_coarse_loss: 1.87662  (1.82417)
     | > decoder_ddc_loss: 0.00132  (0.00123)
     | > ga_loss: 0.00244  (0.00241)
     | > decoder_diff_spec_loss: 0.48346  (0.49008)
     | > postnet_diff_spec_loss: 0.80372  (0.80617)
     | > decoder_ssim_loss: 0.43686  (0.30725)
     | > postnet_ssim_loss: 0.45802  (0.31952)
     | > loss: 3.02963  (3.53597)
     | > align_error: 0.98861  (0.98923)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.94276  (4.06653)
     | > current_lr: 0.00009 
     | > step_time: 2.70120  (3.92592)
     | > loader_time: 0.12080  (0.03659)


[1m   --> STEP: 48/87 -- GLOBAL_STEP: 274800[0m
     | > decoder_loss: 1.98800  (1.91758)
     | > postnet_loss: 1.67574  (1.60032)
     | > stopnet_loss: 2.39482  (1.69732)
     | > decoder_coarse_loss: 1.89155  (1.83899)
     | > decoder_ddc_loss: 0.00119  (0.00127)
     | > ga_loss: 0.00265  (0.00249)
     | > decoder_diff_spec_loss: 0.53735  (0.49812)
     | > postnet_diff_spec_loss: 0.84740  (0.81182)
     | > decoder_ssim_loss: 0.22663  (0.30796)
     | > postnet_ssim_loss: 0.23125  (0.32096)
     | > loss: 4.25785  (3.53401)
     | > align_error: 0.98908  (0.98897)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 7.07569  (3.96609)
     | > current_lr: 0.00009 
     | > step_time: 3.73170  (3.76693)
     | > loader_time: 0.01760  (0.03385)


[1m   --> STEP: 73/87 -- GLOBAL_STEP: 274825[0m
     | > decoder_loss: 1.95497  (1.90452)
     | > postnet_loss: 1.57987  (1.58708)
     | > stopnet_loss: 1.22111  (1.69607)
     | > decoder_coarse_loss: 1.88458  (1.82772)
     | > decoder_ddc_loss: 0.00141  (0.00123)
     | > ga_loss: 0.00264  (0.00242)
     | > decoder_diff_spec_loss: 0.49724  (0.49739)
     | > postnet_diff_spec_loss: 0.81050  (0.81031)
     | > decoder_ssim_loss: 0.38227  (0.30451)
     | > postnet_ssim_loss: 0.39141  (0.31758)
     | > loss: 3.10987  (3.52076)
     | > align_error: 0.98696  (0.98928)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.47563  (3.79771)
     | > current_lr: 0.00009 
     | > step_time: 3.59020  (3.76536)
     | > loader_time: 0.01570  (0.03492)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.31177 [0m(-0.54080)
     | > avg_decoder_loss:[91m 1.97592 [0m(+0.00883)
     | > avg_postnet_loss:[92m 2.15236 [0m(-0.11422)
     | > avg_stopnet_loss:[91m 1.49750 [0m(+0.00206)
     | > avg_decoder_coarse_loss:[91m 1.93133 [0m(+0.02075)
     | > avg_decoder_ddc_loss:[91m 0.00096 [0m(+0.00001)
     | > avg_ga_loss:[92m 0.00227 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[91m 0.42811 [0m(+0.00209)
     | > avg_postnet_diff_spec_loss:[92m 0.79731 [0m(-0.00058)
     | > avg_decoder_ssim_loss:[92m 0.31423 [0m(-0.00061)
     | > avg_postnet_ssim_loss:[92m 0.33556 [0m(-0.00132)
     | > avg_loss:[92m 3.49279 [0m(-0.01925)
     | > avg_align_error:[91m 0.99081 [0m(+0.00007)


[4m[1m > EPOCH: 55/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 16:35:12) [0m

[1m   --> STEP: 10/87 -- GLOBAL_STEP: 274850[0m
     | > decoder_loss: 1.81373  (1.87512)
     | > postnet_loss: 1.51663  (1.55728)
     | > stopnet_loss: 1.98158  (1.63460)
     | > decoder_coarse_loss: 1.72470  (1.79819)
     | > decoder_ddc_loss: 0.00113  (0.00120)
     | > ga_loss: 0.00239  (0.00238)
     | > decoder_diff_spec_loss: 0.47797  (0.48896)
     | > postnet_diff_spec_loss: 0.78443  (0.80674)
     | > decoder_ssim_loss: 0.24914  (0.31503)
     | > postnet_ssim_loss: 0.25826  (0.32701)
     | > loss: 3.70001  (3.43889)
     | > align_error: 0.98899  (0.98929)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.28038  (4.74091)
     | > current_lr: 0.00009 
     | > step_time: 4.64630  (4.24618)
     | > loader_time: 0.02060  (0.03693)


[1m   --> STEP: 35/87 -- GLOBAL_STEP: 274875[0m
     | > decoder_loss: 2.09862  (1.92286)
     | > postnet_loss: 1.79426  (1.59925)
     | > stopnet_loss: 1.55402  (1.70555)
     | > decoder_coarse_loss: 1.97066  (1.84111)
     | > decoder_ddc_loss: 0.00141  (0.00124)
     | > ga_loss: 0.00236  (0.00243)
     | > decoder_diff_spec_loss: 0.56426  (0.49991)
     | > postnet_diff_spec_loss: 0.87174  (0.81111)
     | > decoder_ssim_loss: 0.31979  (0.30368)
     | > postnet_ssim_loss: 0.33327  (0.31605)
     | > loss: 3.55433  (3.54151)
     | > align_error: 0.98811  (0.98927)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.23742  (4.12869)
     | > current_lr: 0.00009 
     | > step_time: 4.67490  (3.93057)
     | > loader_time: 0.01560  (0.03292)


[1m   --> STEP: 60/87 -- GLOBAL_STEP: 274900[0m
     | > decoder_loss: 1.96979  (1.91273)
     | > postnet_loss: 1.63204  (1.58984)
     | > stopnet_loss: 1.57351  (1.68751)
     | > decoder_coarse_loss: 1.92977  (1.83611)
     | > decoder_ddc_loss: 0.00097  (0.00130)
     | > ga_loss: 0.00192  (0.00245)
     | > decoder_diff_spec_loss: 0.52827  (0.49913)
     | > postnet_diff_spec_loss: 0.82307  (0.81115)
     | > decoder_ssim_loss: 0.29432  (0.30633)
     | > postnet_ssim_loss: 0.31195  (0.31964)
     | > loss: 3.45567  (3.51884)
     | > align_error: 0.99104  (0.98905)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 6.42228  (4.04168)
     | > current_lr: 0.00009 
     | > step_time: 3.87330  (3.78164)
     | > loader_time: 0.05630  (0.03146)


[1m   --> STEP: 85/87 -- GLOBAL_STEP: 274925[0m
     | > decoder_loss: 2.01197  (1.90941)
     | > postnet_loss: 1.64265  (1.58563)
     | > stopnet_loss: 1.82656  (1.70900)
     | > decoder_coarse_loss: 1.91123  (1.83283)
     | > decoder_ddc_loss: 0.00101  (0.00123)
     | > ga_loss: 0.00196  (0.00237)
     | > decoder_diff_spec_loss: 0.51879  (0.49998)
     | > postnet_diff_spec_loss: 0.80066  (0.81072)
     | > decoder_ssim_loss: 0.26147  (0.30066)
     | > postnet_ssim_loss: 0.27561  (0.31372)
     | > loss: 3.69222  (3.53440)
     | > align_error: 0.99058  (0.98948)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.43170  (3.84225)
     | > current_lr: 0.00009 
     | > step_time: 3.03980  (3.75010)
     | > loader_time: 0.02200  (0.03307)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.66254 [0m(+0.35077)
     | > avg_decoder_loss:[92m 1.94126 [0m(-0.03466)
     | > avg_postnet_loss:[92m 2.12177 [0m(-0.03058)
     | > avg_stopnet_loss:[92m 1.49404 [0m(-0.00346)
     | > avg_decoder_coarse_loss:[92m 1.85396 [0m(-0.07738)
     | > avg_decoder_ddc_loss:[91m 0.00096 [0m(+0.00000)
     | > avg_ga_loss:[92m 0.00226 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.42764 [0m(-0.00047)
     | > avg_postnet_diff_spec_loss:[92m 0.79700 [0m(-0.00032)
     | > avg_decoder_ssim_loss:[92m 0.31347 [0m(-0.00076)
     | > avg_postnet_ssim_loss:[92m 0.33544 [0m(-0.00012)
     | > avg_loss:[92m 3.45321 [0m(-0.03958)
     | > avg_align_error:[92m 0.99072 [0m(-0.00008)


[4m[1m > EPOCH: 56/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 16:42:23) [0m

[1m   --> STEP: 22/87 -- GLOBAL_STEP: 274950[0m
     | > decoder_loss: 1.60869  (1.88476)
     | > postnet_loss: 1.34910  (1.57330)
     | > stopnet_loss: 1.79396  (1.72324)
     | > decoder_coarse_loss: 1.54322  (1.81160)
     | > decoder_ddc_loss: 0.00083  (0.00122)
     | > ga_loss: 0.00201  (0.00240)
     | > decoder_diff_spec_loss: 0.44068  (0.49092)
     | > postnet_diff_spec_loss: 0.75357  (0.80550)
     | > decoder_ssim_loss: 0.27055  (0.30030)
     | > postnet_ssim_loss: 0.28521  (0.31273)
     | > loss: 3.36697  (3.53032)
     | > align_error: 0.99214  (0.98924)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.97305  (4.24970)
     | > current_lr: 0.00009 
     | > step_time: 5.63000  (4.13057)
     | > loader_time: 0.03680  (0.04105)


[1m   --> STEP: 47/87 -- GLOBAL_STEP: 274975[0m
     | > decoder_loss: 1.81207  (1.90489)
     | > postnet_loss: 1.47724  (1.58523)
     | > stopnet_loss: 1.67164  (1.66427)
     | > decoder_coarse_loss: 1.74164  (1.82756)
     | > decoder_ddc_loss: 0.00163  (0.00130)
     | > ga_loss: 0.00258  (0.00247)
     | > decoder_diff_spec_loss: 0.46966  (0.49729)
     | > postnet_diff_spec_loss: 0.78271  (0.81047)
     | > decoder_ssim_loss: 0.29913  (0.30893)
     | > postnet_ssim_loss: 0.31483  (0.32246)
     | > loss: 3.40928  (3.49116)
     | > align_error: 0.98645  (0.98890)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.29468  (4.13727)
     | > current_lr: 0.00009 
     | > step_time: 3.09230  (3.82814)
     | > loader_time: 0.08790  (0.03662)


[1m   --> STEP: 72/87 -- GLOBAL_STEP: 275000[0m
     | > decoder_loss: 1.80409  (1.89224)
     | > postnet_loss: 1.42645  (1.57175)
     | > stopnet_loss: 1.76921  (1.68948)
     | > decoder_coarse_loss: 1.73464  (1.81460)
     | > decoder_ddc_loss: 0.00095  (0.00125)
     | > ga_loss: 0.00191  (0.00241)
     | > decoder_diff_spec_loss: 0.48443  (0.49726)
     | > postnet_diff_spec_loss: 0.77597  (0.80971)
     | > decoder_ssim_loss: 0.27536  (0.30265)
     | > postnet_ssim_loss: 0.29324  (0.31619)
     | > loss: 3.47753  (3.50292)
     | > align_error: 0.99147  (0.98924)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.36648  (3.94066)
     | > current_lr: 0.00009 
     | > step_time: 4.24060  (3.84722)
     | > loader_time: 0.02150  (0.03700)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.43134 [0m(-0.23120)
     | > avg_decoder_loss:[92m 1.87430 [0m(-0.06696)
     | > avg_postnet_loss:[92m 2.01247 [0m(-0.10931)
     | > avg_stopnet_loss:[92m 1.49255 [0m(-0.00149)
     | > avg_decoder_coarse_loss:[92m 1.82991 [0m(-0.02405)
     | > avg_decoder_ddc_loss:[92m 0.00096 [0m(-0.00000)
     | > avg_ga_loss:[92m 0.00225 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.42499 [0m(-0.00265)
     | > avg_postnet_diff_spec_loss:[92m 0.79559 [0m(-0.00141)
     | > avg_decoder_ssim_loss:[92m 0.31280 [0m(-0.00067)
     | > avg_postnet_ssim_loss:[92m 0.33425 [0m(-0.00119)
     | > avg_loss:[92m 3.40012 [0m(-0.05309)
     | > avg_align_error:[91m 0.99074 [0m(+0.00001)


[4m[1m > EPOCH: 57/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 16:49:35) [0m

[1m   --> STEP: 9/87 -- GLOBAL_STEP: 275025[0m
     | > decoder_loss: 1.85731  (1.86150)
     | > postnet_loss: 1.42934  (1.54908)
     | > stopnet_loss: 2.01495  (1.60283)
     | > decoder_coarse_loss: 1.78107  (1.80425)
     | > decoder_ddc_loss: 0.00100  (0.00121)
     | > ga_loss: 0.00213  (0.00236)
     | > decoder_diff_spec_loss: 0.50198  (0.49171)
     | > postnet_diff_spec_loss: 0.79077  (0.80958)
     | > decoder_ssim_loss: 0.24557  (0.32136)
     | > postnet_ssim_loss: 0.25295  (0.33448)
     | > loss: 3.74057  (3.40793)
     | > align_error: 0.99127  (0.98924)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.72727  (4.53534)
     | > current_lr: 0.00009 
     | > step_time: 4.05330  (4.12743)
     | > loader_time: 0.01920  (0.04203)


[1m   --> STEP: 34/87 -- GLOBAL_STEP: 275050[0m
     | > decoder_loss: 1.75929  (1.89471)
     | > postnet_loss: 1.41645  (1.57452)
     | > stopnet_loss: 1.96986  (1.71121)
     | > decoder_coarse_loss: 1.73032  (1.83196)
     | > decoder_ddc_loss: 0.00165  (0.00122)
     | > ga_loss: 0.00345  (0.00240)
     | > decoder_diff_spec_loss: 0.48421  (0.49765)
     | > postnet_diff_spec_loss: 0.79782  (0.80905)
     | > decoder_ssim_loss: 0.26726  (0.30222)
     | > postnet_ssim_loss: 0.27918  (0.31505)
     | > loss: 3.67115  (3.52983)
     | > align_error: 0.98571  (0.98926)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 6.38573  (3.93107)
     | > current_lr: 0.00009 
     | > step_time: 2.45020  (3.80936)
     | > loader_time: 0.02360  (0.03265)


[1m   --> STEP: 59/87 -- GLOBAL_STEP: 275075[0m
     | > decoder_loss: 1.84638  (1.89133)
     | > postnet_loss: 1.60597  (1.57517)
     | > stopnet_loss: 2.26868  (1.67703)
     | > decoder_coarse_loss: 1.79461  (1.82436)
     | > decoder_ddc_loss: 0.00139  (0.00128)
     | > ga_loss: 0.00296  (0.00244)
     | > decoder_diff_spec_loss: 0.49837  (0.49834)
     | > postnet_diff_spec_loss: 0.82018  (0.81075)
     | > decoder_ssim_loss: 0.24290  (0.30565)
     | > postnet_ssim_loss: 0.25454  (0.31948)
     | > loss: 4.04955  (3.49580)
     | > align_error: 0.98775  (0.98899)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 8.50750  (3.86650)
     | > current_lr: 0.00009 
     | > step_time: 3.11880  (3.68748)
     | > loader_time: 0.04170  (0.03311)


[1m   --> STEP: 84/87 -- GLOBAL_STEP: 275100[0m
     | > decoder_loss: 1.86404  (1.89055)
     | > postnet_loss: 1.48835  (1.56976)
     | > stopnet_loss: 1.20854  (1.70420)
     | > decoder_coarse_loss: 1.77570  (1.82004)
     | > decoder_ddc_loss: 0.00121  (0.00121)
     | > ga_loss: 0.00229  (0.00236)
     | > decoder_diff_spec_loss: 0.49748  (0.49960)
     | > postnet_diff_spec_loss: 0.79008  (0.81054)
     | > decoder_ssim_loss: 0.36806  (0.30036)
     | > postnet_ssim_loss: 0.38251  (0.31382)
     | > loss: 3.01187  (3.51746)
     | > align_error: 0.98937  (0.98945)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.19525  (3.77963)
     | > current_lr: 0.00009 
     | > step_time: 2.50160  (3.69011)
     | > loader_time: 0.01640  (0.03366)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.43561 [0m(+0.00427)
     | > avg_decoder_loss:[91m 1.88851 [0m(+0.01421)
     | > avg_postnet_loss:[92m 1.99997 [0m(-0.01249)
     | > avg_stopnet_loss:[92m 1.48784 [0m(-0.00471)
     | > avg_decoder_coarse_loss:[91m 1.85550 [0m(+0.02559)
     | > avg_decoder_ddc_loss:[92m 0.00093 [0m(-0.00002)
     | > avg_ga_loss:[92m 0.00224 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[91m 0.42673 [0m(+0.00174)
     | > avg_postnet_diff_spec_loss:[91m 0.79654 [0m(+0.00095)
     | > avg_decoder_ssim_loss:[92m 0.31227 [0m(-0.00054)
     | > avg_postnet_ssim_loss:[91m 0.33490 [0m(+0.00066)
     | > avg_loss:[91m 3.40288 [0m(+0.00276)
     | > avg_align_error:[91m 0.99078 [0m(+0.00004)


[4m[1m > EPOCH: 58/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 16:56:41) [0m

[1m   --> STEP: 21/87 -- GLOBAL_STEP: 275125[0m
     | > decoder_loss: 1.99677  (1.88679)
     | > postnet_loss: 1.65440  (1.58062)
     | > stopnet_loss: 1.16761  (1.71801)
     | > decoder_coarse_loss: 1.86512  (1.81897)
     | > decoder_ddc_loss: 0.00175  (0.00122)
     | > ga_loss: 0.00283  (0.00240)
     | > decoder_diff_spec_loss: 0.49933  (0.49268)
     | > postnet_diff_spec_loss: 0.81934  (0.80745)
     | > decoder_ssim_loss: 0.39366  (0.30092)
     | > postnet_ssim_loss: 0.41031  (0.31347)
     | > loss: 3.09191  (3.53052)
     | > align_error: 0.98439  (0.98904)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 6.47345  (4.54327)
     | > current_lr: 0.00009 
     | > step_time: 2.78300  (4.05189)
     | > loader_time: 0.09980  (0.03661)


[1m   --> STEP: 46/87 -- GLOBAL_STEP: 275150[0m
     | > decoder_loss: 1.78238  (1.89501)
     | > postnet_loss: 1.50785  (1.57718)
     | > stopnet_loss: 1.48523  (1.66233)
     | > decoder_coarse_loss: 1.73593  (1.83083)
     | > decoder_ddc_loss: 0.00095  (0.00126)
     | > ga_loss: 0.00182  (0.00245)
     | > decoder_diff_spec_loss: 0.48762  (0.49707)
     | > postnet_diff_spec_loss: 0.80101  (0.81027)
     | > decoder_ssim_loss: 0.30318  (0.30828)
     | > postnet_ssim_loss: 0.32283  (0.32222)
     | > loss: 3.22975  (3.48512)
     | > align_error: 0.99190  (0.98894)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.29680  (4.55050)
     | > current_lr: 0.00009 
     | > step_time: 4.99880  (3.86038)
     | > loader_time: 0.02060  (0.03217)


[1m   --> STEP: 71/87 -- GLOBAL_STEP: 275175[0m
     | > decoder_loss: 1.71543  (1.87913)
     | > postnet_loss: 1.48372  (1.56180)
     | > stopnet_loss: 1.90744  (1.68567)
     | > decoder_coarse_loss: 1.65137  (1.81219)
     | > decoder_ddc_loss: 0.00107  (0.00123)
     | > ga_loss: 0.00237  (0.00240)
     | > decoder_diff_spec_loss: 0.46273  (0.49680)
     | > postnet_diff_spec_loss: 0.80043  (0.80962)
     | > decoder_ssim_loss: 0.25918  (0.30220)
     | > postnet_ssim_loss: 0.27651  (0.31599)
     | > loss: 3.58189  (3.49239)
     | > align_error: 0.99026  (0.98921)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 6.45906  (4.24788)
     | > current_lr: 0.00009 
     | > step_time: 3.74530  (3.86772)
     | > loader_time: 0.02140  (0.03211)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.58654 [0m(+0.15093)
     | > avg_decoder_loss:[91m 1.90410 [0m(+0.01559)
     | > avg_postnet_loss:[91m 2.02399 [0m(+0.02401)
     | > avg_stopnet_loss:[91m 1.49271 [0m(+0.00487)
     | > avg_decoder_coarse_loss:[91m 1.86360 [0m(+0.00810)
     | > avg_decoder_ddc_loss:[91m 0.00098 [0m(+0.00005)
     | > avg_ga_loss:[92m 0.00223 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.42598 [0m(-0.00075)
     | > avg_postnet_diff_spec_loss:[92m 0.79611 [0m(-0.00043)
     | > avg_decoder_ssim_loss:[92m 0.31192 [0m(-0.00035)
     | > avg_postnet_ssim_loss:[92m 0.33479 [0m(-0.00011)
     | > avg_loss:[91m 3.41921 [0m(+0.01633)
     | > avg_align_error:[92m 0.99069 [0m(-0.00008)


[4m[1m > EPOCH: 59/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 17:03:54) [0m

[1m   --> STEP: 8/87 -- GLOBAL_STEP: 275200[0m
     | > decoder_loss: 1.65382  (1.84880)
     | > postnet_loss: 1.48198  (1.54181)
     | > stopnet_loss: 2.43219  (1.55126)
     | > decoder_coarse_loss: 1.62980  (1.80277)
     | > decoder_ddc_loss: 0.00123  (0.00125)
     | > ga_loss: 0.00281  (0.00237)
     | > decoder_diff_spec_loss: 0.44342  (0.48852)
     | > postnet_diff_spec_loss: 0.79555  (0.81109)
     | > decoder_ssim_loss: 0.21559  (0.32953)
     | > postnet_ssim_loss: 0.22312  (0.34426)
     | > loss: 4.05735  (3.35512)
     | > align_error: 0.98883  (0.98891)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 8.44753  (5.30295)
     | > current_lr: 0.00009 
     | > step_time: 3.61940  (4.13456)
     | > loader_time: 0.01550  (0.03089)


[1m   --> STEP: 33/87 -- GLOBAL_STEP: 275225[0m
     | > decoder_loss: 1.90382  (1.89241)
     | > postnet_loss: 1.64387  (1.56172)
     | > stopnet_loss: 1.48174  (1.68925)
     | > decoder_coarse_loss: 1.79483  (1.82529)
     | > decoder_ddc_loss: 0.00114  (0.00121)
     | > ga_loss: 0.00218  (0.00236)
     | > decoder_diff_spec_loss: 0.50323  (0.49672)
     | > postnet_diff_spec_loss: 0.82754  (0.80831)
     | > decoder_ssim_loss: 0.32638  (0.30242)
     | > postnet_ssim_loss: 0.34047  (0.31563)
     | > loss: 3.32795  (3.50199)
     | > align_error: 0.98974  (0.98929)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.04052  (4.17884)
     | > current_lr: 0.00009 
     | > step_time: 3.64510  (3.90830)
     | > loader_time: 0.02060  (0.03180)


[1m   --> STEP: 58/87 -- GLOBAL_STEP: 275250[0m
     | > decoder_loss: 1.90247  (1.88406)
     | > postnet_loss: 1.50977  (1.55612)
     | > stopnet_loss: 1.56553  (1.67036)
     | > decoder_coarse_loss: 1.83895  (1.81613)
     | > decoder_ddc_loss: 0.00101  (0.00128)
     | > ga_loss: 0.00191  (0.00241)
     | > decoder_diff_spec_loss: 0.53212  (0.49727)
     | > postnet_diff_spec_loss: 0.82229  (0.80952)
     | > decoder_ssim_loss: 0.30210  (0.30589)
     | > postnet_ssim_loss: 0.31756  (0.31989)
     | > loss: 3.38163  (3.47996)
     | > align_error: 0.99155  (0.98896)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.77763  (4.24624)
     | > current_lr: 0.00009 
     | > step_time: 4.74810  (3.73191)
     | > loader_time: 0.02360  (0.03552)


[1m   --> STEP: 83/87 -- GLOBAL_STEP: 275275[0m
     | > decoder_loss: 1.85967  (1.88053)
     | > postnet_loss: 1.47955  (1.55147)
     | > stopnet_loss: 2.20759  (1.70727)
     | > decoder_coarse_loss: 1.79448  (1.81159)
     | > decoder_ddc_loss: 0.00099  (0.00121)
     | > ga_loss: 0.00202  (0.00234)
     | > decoder_diff_spec_loss: 0.48507  (0.49838)
     | > postnet_diff_spec_loss: 0.78934  (0.80990)
     | > decoder_ssim_loss: 0.23035  (0.29859)
     | > postnet_ssim_loss: 0.24246  (0.31231)
     | > loss: 3.93819  (3.50998)
     | > align_error: 0.99069  (0.98938)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.40840  (4.05171)
     | > current_lr: 0.00009 
     | > step_time: 2.87560  (3.72094)
     | > loader_time: 0.01670  (0.03539)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.58774 [0m(+0.00120)
     | > avg_decoder_loss:[91m 1.91092 [0m(+0.00683)
     | > avg_postnet_loss:[92m 1.99479 [0m(-0.02919)
     | > avg_stopnet_loss:[92m 1.48510 [0m(-0.00761)
     | > avg_decoder_coarse_loss:[92m 1.79821 [0m(-0.06539)
     | > avg_decoder_ddc_loss:[92m 0.00095 [0m(-0.00003)
     | > avg_ga_loss:[91m 0.00223 [0m(+0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.42756 [0m(+0.00158)
     | > avg_postnet_diff_spec_loss:[92m 0.79546 [0m(-0.00065)
     | > avg_decoder_ssim_loss:[92m 0.31166 [0m(-0.00026)
     | > avg_postnet_ssim_loss:[92m 0.33407 [0m(-0.00072)
     | > avg_loss:[92m 3.38966 [0m(-0.02955)
     | > avg_align_error:[91m 0.99077 [0m(+0.00008)


[4m[1m > EPOCH: 60/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 17:11:01) [0m

[1m   --> STEP: 20/87 -- GLOBAL_STEP: 275300[0m
     | > decoder_loss: 1.72215  (1.86145)
     | > postnet_loss: 1.47435  (1.54141)
     | > stopnet_loss: 2.14158  (1.74957)
     | > decoder_coarse_loss: 1.63667  (1.79897)
     | > decoder_ddc_loss: 0.00137  (0.00123)
     | > ga_loss: 0.00268  (0.00234)
     | > decoder_diff_spec_loss: 0.46157  (0.49279)
     | > postnet_diff_spec_loss: 0.78286  (0.80644)
     | > decoder_ssim_loss: 0.23322  (0.29521)
     | > postnet_ssim_loss: 0.24569  (0.30849)
     | > loss: 3.79445  (3.53776)
     | > align_error: 0.98788  (0.98913)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.08035  (4.21977)
     | > current_lr: 0.00009 
     | > step_time: 3.03510  (4.06953)
     | > loader_time: 0.01700  (0.03114)


[1m   --> STEP: 45/87 -- GLOBAL_STEP: 275325[0m
     | > decoder_loss: 1.92232  (1.87698)
     | > postnet_loss: 1.53228  (1.55096)
     | > stopnet_loss: 1.93532  (1.66932)
     | > decoder_coarse_loss: 1.80296  (1.81196)
     | > decoder_ddc_loss: 0.00117  (0.00128)
     | > ga_loss: 0.00242  (0.00244)
     | > decoder_diff_spec_loss: 0.49890  (0.49680)
     | > postnet_diff_spec_loss: 0.80765  (0.81004)
     | > decoder_ssim_loss: 0.24986  (0.30726)
     | > postnet_ssim_loss: 0.25946  (0.32182)
     | > loss: 3.71604  (3.47580)
     | > align_error: 0.98921  (0.98877)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.19905  (4.29204)
     | > current_lr: 0.00009 
     | > step_time: 3.81270  (3.78816)
     | > loader_time: 0.02090  (0.03292)


[1m   --> STEP: 70/87 -- GLOBAL_STEP: 275350[0m
     | > decoder_loss: 2.04489  (1.86742)
     | > postnet_loss: 1.54396  (1.54119)
     | > stopnet_loss: 1.71577  (1.68332)
     | > decoder_coarse_loss: 1.91862  (1.79816)
     | > decoder_ddc_loss: 0.00096  (0.00124)
     | > ga_loss: 0.00196  (0.00238)
     | > decoder_diff_spec_loss: 0.54912  (0.49664)
     | > postnet_diff_spec_loss: 0.80887  (0.80901)
     | > decoder_ssim_loss: 0.28534  (0.30187)
     | > postnet_ssim_loss: 0.30067  (0.31610)
     | > loss: 3.58867  (3.47812)
     | > align_error: 0.99158  (0.98912)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.69981  (4.04167)
     | > current_lr: 0.00009 
     | > step_time: 4.52290  (3.81134)
     | > loader_time: 0.04340  (0.03389)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.52369 [0m(-0.06405)
     | > avg_decoder_loss:[92m 1.90641 [0m(-0.00452)
     | > avg_postnet_loss:[91m 2.10429 [0m(+0.10950)
     | > avg_stopnet_loss:[91m 1.48813 [0m(+0.00303)
     | > avg_decoder_coarse_loss:[91m 1.81418 [0m(+0.01597)
     | > avg_decoder_ddc_loss:[91m 0.00095 [0m(+0.00000)
     | > avg_ga_loss:[92m 0.00223 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.42906 [0m(+0.00150)
     | > avg_postnet_diff_spec_loss:[91m 0.79591 [0m(+0.00045)
     | > avg_decoder_ssim_loss:[92m 0.31126 [0m(-0.00040)
     | > avg_postnet_ssim_loss:[91m 0.33503 [0m(+0.00096)
     | > avg_loss:[91m 3.42355 [0m(+0.03389)
     | > avg_align_error:[91m 0.99081 [0m(+0.00004)


[4m[1m > EPOCH: 61/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 17:18:12) [0m

[1m   --> STEP: 7/87 -- GLOBAL_STEP: 275375[0m
     | > decoder_loss: 1.94498  (1.87239)
     | > postnet_loss: 1.53415  (1.54170)
     | > stopnet_loss: 1.57995  (1.41387)
     | > decoder_coarse_loss: 1.83580  (1.80478)
     | > decoder_ddc_loss: 0.00103  (0.00121)
     | > ga_loss: 0.00225  (0.00230)
     | > decoder_diff_spec_loss: 0.47978  (0.49621)
     | > postnet_diff_spec_loss: 0.79052  (0.81220)
     | > decoder_ssim_loss: 0.31318  (0.34547)
     | > postnet_ssim_loss: 0.32277  (0.36095)
     | > loss: 3.39676  (3.23412)
     | > align_error: 0.99048  (0.98900)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.42927  (4.44701)
     | > current_lr: 0.00009 
     | > step_time: 2.90560  (4.44822)
     | > loader_time: 0.01430  (0.03296)


[1m   --> STEP: 32/87 -- GLOBAL_STEP: 275400[0m
     | > decoder_loss: 1.61944  (1.87837)
     | > postnet_loss: 1.46608  (1.54618)
     | > stopnet_loss: 1.85258  (1.69907)
     | > decoder_coarse_loss: 1.56955  (1.80962)
     | > decoder_ddc_loss: 0.00131  (0.00119)
     | > ga_loss: 0.00273  (0.00237)
     | > decoder_diff_spec_loss: 0.44044  (0.49792)
     | > postnet_diff_spec_loss: 0.79449  (0.80717)
     | > decoder_ssim_loss: 0.26832  (0.30096)
     | > postnet_ssim_loss: 0.28449  (0.31434)
     | > loss: 3.47724  (3.49985)
     | > align_error: 0.98835  (0.98932)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.80725  (4.28607)
     | > current_lr: 0.00009 
     | > step_time: 3.60500  (3.96700)
     | > loader_time: 0.01630  (0.03141)


[1m   --> STEP: 57/87 -- GLOBAL_STEP: 275425[0m
     | > decoder_loss: 2.04036  (1.87020)
     | > postnet_loss: 1.60386  (1.54069)
     | > stopnet_loss: 1.57263  (1.67369)
     | > decoder_coarse_loss: 2.02806  (1.80250)
     | > decoder_ddc_loss: 0.00138  (0.00126)
     | > ga_loss: 0.00267  (0.00242)
     | > decoder_diff_spec_loss: 0.57676  (0.49664)
     | > postnet_diff_spec_loss: 0.85663  (0.80866)
     | > decoder_ssim_loss: 0.32782  (0.30514)
     | > postnet_ssim_loss: 0.33987  (0.31943)
     | > loss: 3.52964  (3.47194)
     | > align_error: 0.98886  (0.98892)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.41232  (4.38170)
     | > current_lr: 0.00009 
     | > step_time: 2.50910  (3.73822)
     | > loader_time: 0.01910  (0.02910)


[1m   --> STEP: 82/87 -- GLOBAL_STEP: 275450[0m
     | > decoder_loss: 1.82110  (1.86912)
     | > postnet_loss: 1.52217  (1.53498)
     | > stopnet_loss: 2.35183  (1.70088)
     | > decoder_coarse_loss: 1.71779  (1.79987)
     | > decoder_ddc_loss: 0.00099  (0.00119)
     | > ga_loss: 0.00198  (0.00235)
     | > decoder_diff_spec_loss: 0.52190  (0.49869)
     | > postnet_diff_spec_loss: 0.82551  (0.80950)
     | > decoder_ssim_loss: 0.22356  (0.29872)
     | > postnet_ssim_loss: 0.23026  (0.31266)
     | > loss: 4.07752  (3.49382)
     | > align_error: 0.99058  (0.98939)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.01110  (4.20229)
     | > current_lr: 0.00009 
     | > step_time: 2.88180  (3.77301)
     | > loader_time: 0.01800  (0.02930)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.42264 [0m(-0.10105)
     | > avg_decoder_loss:[92m 1.86242 [0m(-0.04399)
     | > avg_postnet_loss:[92m 1.95448 [0m(-0.14982)
     | > avg_stopnet_loss:[92m 1.48243 [0m(-0.00571)
     | > avg_decoder_coarse_loss:[92m 1.74708 [0m(-0.06710)
     | > avg_decoder_ddc_loss:[92m 0.00093 [0m(-0.00002)
     | > avg_ga_loss:[92m 0.00223 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[92m 0.42839 [0m(-0.00067)
     | > avg_postnet_diff_spec_loss:[92m 0.79545 [0m(-0.00046)
     | > avg_decoder_ssim_loss:[92m 0.31065 [0m(-0.00061)
     | > avg_postnet_ssim_loss:[92m 0.33390 [0m(-0.00113)
     | > avg_loss:[92m 3.35189 [0m(-0.07166)
     | > avg_align_error:[91m 0.99081 [0m(+0.00000)


[4m[1m > EPOCH: 62/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 17:25:19) [0m

[1m   --> STEP: 19/87 -- GLOBAL_STEP: 275475[0m
     | > decoder_loss: 1.82578  (1.86625)
     | > postnet_loss: 1.54207  (1.55048)
     | > stopnet_loss: 1.19794  (1.71180)
     | > decoder_coarse_loss: 1.79149  (1.80973)
     | > decoder_ddc_loss: 0.00119  (0.00117)
     | > ga_loss: 0.00218  (0.00234)
     | > decoder_diff_spec_loss: 0.49300  (0.49265)
     | > postnet_diff_spec_loss: 0.80633  (0.80724)
     | > decoder_ssim_loss: 0.37537  (0.29814)
     | > postnet_ssim_loss: 0.39217  (0.31154)
     | > loss: 3.01570  (3.50778)
     | > align_error: 0.98987  (0.98933)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 6.02034  (4.41101)
     | > current_lr: 0.00009 
     | > step_time: 3.40780  (4.10414)
     | > loader_time: 0.06650  (0.03130)


[1m   --> STEP: 44/87 -- GLOBAL_STEP: 275500[0m
     | > decoder_loss: 1.76542  (1.87229)
     | > postnet_loss: 1.47534  (1.54597)
     | > stopnet_loss: 1.22369  (1.66093)
     | > decoder_coarse_loss: 1.68195  (1.80859)
     | > decoder_ddc_loss: 0.00094  (0.00124)
     | > ga_loss: 0.00177  (0.00244)
     | > decoder_diff_spec_loss: 0.46021  (0.49586)
     | > postnet_diff_spec_loss: 0.78784  (0.80969)
     | > decoder_ssim_loss: 0.36876  (0.30812)
     | > postnet_ssim_loss: 0.38664  (0.32282)
     | > loss: 2.96430  (3.46430)
     | > align_error: 0.99161  (0.98888)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.87461  (4.29630)
     | > current_lr: 0.00009 
     | > step_time: 4.67650  (3.76615)
     | > loader_time: 0.02860  (0.03467)


[1m   --> STEP: 69/87 -- GLOBAL_STEP: 275525[0m
     | > decoder_loss: 1.79040  (1.85667)
     | > postnet_loss: 1.39945  (1.53000)
     | > stopnet_loss: 2.29109  (1.68334)
     | > decoder_coarse_loss: 1.70443  (1.79462)
     | > decoder_ddc_loss: 0.00088  (0.00121)
     | > ga_loss: 0.00199  (0.00238)
     | > decoder_diff_spec_loss: 0.48014  (0.49503)
     | > postnet_diff_spec_loss: 0.78742  (0.80853)
     | > decoder_ssim_loss: 0.22404  (0.30150)
     | > postnet_ssim_loss: 0.23700  (0.31583)
     | > loss: 3.95699  (3.47111)
     | > align_error: 0.99117  (0.98916)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.63591  (4.12706)
     | > current_lr: 0.00009 
     | > step_time: 3.56830  (3.76834)
     | > loader_time: 0.06790  (0.03569)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.41845 [0m(-0.00419)
     | > avg_decoder_loss:[91m 1.88009 [0m(+0.01767)
     | > avg_postnet_loss:[91m 1.98205 [0m(+0.02757)
     | > avg_stopnet_loss:[92m 1.48100 [0m(-0.00142)
     | > avg_decoder_coarse_loss:[91m 1.75977 [0m(+0.01269)
     | > avg_decoder_ddc_loss:[92m 0.00093 [0m(-0.00001)
     | > avg_ga_loss:[92m 0.00222 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[92m 0.42749 [0m(-0.00089)
     | > avg_postnet_diff_spec_loss:[92m 0.79506 [0m(-0.00039)
     | > avg_decoder_ssim_loss:[92m 0.31032 [0m(-0.00033)
     | > avg_postnet_ssim_loss:[91m 0.33457 [0m(+0.00067)
     | > avg_loss:[91m 3.36469 [0m(+0.01280)
     | > avg_align_error:[92m 0.99078 [0m(-0.00004)


[4m[1m > EPOCH: 63/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 17:32:25) [0m

[1m   --> STEP: 6/87 -- GLOBAL_STEP: 275550[0m
     | > decoder_loss: 1.70182  (1.85506)
     | > postnet_loss: 1.41175  (1.52119)
     | > stopnet_loss: 0.96872  (1.35859)
     | > decoder_coarse_loss: 1.64565  (1.78625)
     | > decoder_ddc_loss: 0.00102  (0.00122)
     | > ga_loss: 0.00160  (0.00230)
     | > decoder_diff_spec_loss: 0.47743  (0.49613)
     | > postnet_diff_spec_loss: 0.79618  (0.81515)
     | > decoder_ssim_loss: 0.41415  (0.35041)
     | > postnet_ssim_loss: 0.43069  (0.36645)
     | > loss: 2.69641  (3.16805)
     | > align_error: 0.99139  (0.98869)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.92821  (4.55825)
     | > current_lr: 0.00008 
     | > step_time: 4.08300  (4.57339)
     | > loader_time: 0.01860  (0.04062)


[1m   --> STEP: 31/87 -- GLOBAL_STEP: 275575[0m
     | > decoder_loss: 2.05200  (1.86953)
     | > postnet_loss: 1.64950  (1.52243)
     | > stopnet_loss: 1.42044  (1.68081)
     | > decoder_coarse_loss: 2.00909  (1.80588)
     | > decoder_ddc_loss: 0.00148  (0.00118)
     | > ga_loss: 0.00259  (0.00234)
     | > decoder_diff_spec_loss: 0.56126  (0.49734)
     | > postnet_diff_spec_loss: 0.85850  (0.80715)
     | > decoder_ssim_loss: 0.34171  (0.30124)
     | > postnet_ssim_loss: 0.35388  (0.31467)
     | > loss: 3.39025  (3.47237)
     | > align_error: 0.98788  (0.98927)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.44277  (4.03973)
     | > current_lr: 0.00008 
     | > step_time: 2.85310  (3.88755)
     | > loader_time: 0.04260  (0.03946)


[1m   --> STEP: 56/87 -- GLOBAL_STEP: 275600[0m
     | > decoder_loss: 1.80164  (1.85194)
     | > postnet_loss: 1.39242  (1.51822)
     | > stopnet_loss: 2.04191  (1.66102)
     | > decoder_coarse_loss: 1.72707  (1.78830)
     | > decoder_ddc_loss: 0.00074  (0.00126)
     | > ga_loss: 0.00180  (0.00240)
     | > decoder_diff_spec_loss: 0.49899  (0.49425)
     | > postnet_diff_spec_loss: 0.79942  (0.80731)
     | > decoder_ssim_loss: 0.24667  (0.30409)
     | > postnet_ssim_loss: 0.25655  (0.31850)
     | > loss: 3.73180  (3.44400)
     | > align_error: 0.99307  (0.98882)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.04672  (3.86963)
     | > current_lr: 0.00008 
     | > step_time: 4.65940  (3.77651)
     | > loader_time: 0.02070  (0.03501)


[1m   --> STEP: 81/87 -- GLOBAL_STEP: 275625[0m
     | > decoder_loss: 2.03784  (1.85569)
     | > postnet_loss: 1.53783  (1.51674)
     | > stopnet_loss: 2.24841  (1.67818)
     | > decoder_coarse_loss: 1.97117  (1.79191)
     | > decoder_ddc_loss: 0.00098  (0.00120)
     | > ga_loss: 0.00223  (0.00234)
     | > decoder_diff_spec_loss: 0.57563  (0.49717)
     | > postnet_diff_spec_loss: 0.84951  (0.80873)
     | > decoder_ssim_loss: 0.22437  (0.29895)
     | > postnet_ssim_loss: 0.23077  (0.31309)
     | > loss: 4.11658  (3.46073)
     | > align_error: 0.99014  (0.98928)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.98979  (3.83208)
     | > current_lr: 0.00008 
     | > step_time: 3.28550  (3.77762)
     | > loader_time: 0.01750  (0.03354)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.65772 [0m(+0.23927)
     | > avg_decoder_loss:[91m 1.92470 [0m(+0.04461)
     | > avg_postnet_loss:[91m 2.08200 [0m(+0.09995)
     | > avg_stopnet_loss:[92m 1.48099 [0m(-0.00001)
     | > avg_decoder_coarse_loss:[91m 1.85000 [0m(+0.09023)
     | > avg_decoder_ddc_loss:[91m 0.00094 [0m(+0.00001)
     | > avg_ga_loss:[92m 0.00222 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.42958 [0m(+0.00209)
     | > avg_postnet_diff_spec_loss:[92m 0.79415 [0m(-0.00091)
     | > avg_decoder_ssim_loss:[92m 0.31022 [0m(-0.00010)
     | > avg_postnet_ssim_loss:[92m 0.33453 [0m(-0.00004)
     | > avg_loss:[91m 3.42363 [0m(+0.05893)
     | > avg_align_error:[91m 0.99084 [0m(+0.00006)


[4m[1m > EPOCH: 64/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 17:39:37) [0m

[1m   --> STEP: 18/87 -- GLOBAL_STEP: 275650[0m
     | > decoder_loss: 1.95952  (1.85604)
     | > postnet_loss: 1.60390  (1.51639)
     | > stopnet_loss: 1.41041  (1.74304)
     | > decoder_coarse_loss: 1.87169  (1.79588)
     | > decoder_ddc_loss: 0.00123  (0.00118)
     | > ga_loss: 0.00207  (0.00233)
     | > decoder_diff_spec_loss: 0.54762  (0.49270)
     | > postnet_diff_spec_loss: 0.85042  (0.80659)
     | > decoder_ssim_loss: 0.32393  (0.29317)
     | > postnet_ssim_loss: 0.33442  (0.30630)
     | > loss: 3.29392  (3.52175)
     | > align_error: 0.98956  (0.98927)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 9.04036  (3.90873)
     | > current_lr: 0.00008 
     | > step_time: 3.74940  (3.94713)
     | > loader_time: 0.02150  (0.04475)


[1m   --> STEP: 43/87 -- GLOBAL_STEP: 275675[0m
     | > decoder_loss: 1.82963  (1.86111)
     | > postnet_loss: 1.42677  (1.51929)
     | > stopnet_loss: 1.66201  (1.66455)
     | > decoder_coarse_loss: 1.78487  (1.79614)
     | > decoder_ddc_loss: 0.00153  (0.00127)
     | > ga_loss: 0.00269  (0.00245)
     | > decoder_diff_spec_loss: 0.48071  (0.49697)
     | > postnet_diff_spec_loss: 0.79706  (0.80915)
     | > decoder_ssim_loss: 0.30958  (0.30590)
     | > postnet_ssim_loss: 0.32182  (0.32043)
     | > loss: 3.41347  (3.45434)
     | > align_error: 0.98718  (0.98872)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.35177  (4.24455)
     | > current_lr: 0.00008 
     | > step_time: 3.30750  (3.68549)
     | > loader_time: 0.01370  (0.03875)


[1m   --> STEP: 68/87 -- GLOBAL_STEP: 275700[0m
     | > decoder_loss: 2.06114  (1.84880)
     | > postnet_loss: 1.61925  (1.50869)
     | > stopnet_loss: 1.48213  (1.66351)
     | > decoder_coarse_loss: 1.96440  (1.78252)
     | > decoder_ddc_loss: 0.00122  (0.00125)
     | > ga_loss: 0.00189  (0.00237)
     | > decoder_diff_spec_loss: 0.56359  (0.49534)
     | > postnet_diff_spec_loss: 0.85695  (0.80806)
     | > decoder_ssim_loss: 0.30246  (0.30197)
     | > postnet_ssim_loss: 0.31201  (0.31622)
     | > loss: 3.41184  (3.44105)
     | > align_error: 0.98933  (0.98899)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.02308  (4.01576)
     | > current_lr: 0.00008 
     | > step_time: 3.95210  (3.73133)
     | > loader_time: 0.01730  (0.03485)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.35655 [0m(-0.30117)
     | > avg_decoder_loss:[92m 1.86828 [0m(-0.05642)
     | > avg_postnet_loss:[92m 2.00957 [0m(-0.07243)
     | > avg_stopnet_loss:[92m 1.48092 [0m(-0.00007)
     | > avg_decoder_coarse_loss:[92m 1.78621 [0m(-0.06379)
     | > avg_decoder_ddc_loss:[91m 0.00097 [0m(+0.00003)
     | > avg_ga_loss:[92m 0.00221 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[91m 0.43026 [0m(+0.00068)
     | > avg_postnet_diff_spec_loss:[91m 0.79466 [0m(+0.00051)
     | > avg_decoder_ssim_loss:[92m 0.31003 [0m(-0.00020)
     | > avg_postnet_ssim_loss:[92m 0.33417 [0m(-0.00035)
     | > avg_loss:[92m 3.37553 [0m(-0.04809)
     | > avg_align_error:[92m 0.99077 [0m(-0.00007)


[4m[1m > EPOCH: 65/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 17:46:41) [0m

[1m   --> STEP: 5/87 -- GLOBAL_STEP: 275725[0m
     | > decoder_loss: 1.83857  (1.86993)
     | > postnet_loss: 1.51088  (1.51762)
     | > stopnet_loss: 1.07117  (1.39343)
     | > decoder_coarse_loss: 1.81288  (1.80861)
     | > decoder_ddc_loss: 0.00149  (0.00133)
     | > ga_loss: 0.00234  (0.00241)
     | > decoder_diff_spec_loss: 0.46883  (0.49800)
     | > postnet_diff_spec_loss: 0.79606  (0.81764)
     | > decoder_ssim_loss: 0.39991  (0.33654)
     | > postnet_ssim_loss: 0.42263  (0.35242)
     | > loss: 2.89567  (3.20598)
     | > align_error: 0.98736  (0.98799)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.55293  (3.94240)
     | > current_lr: 0.00008 
     | > step_time: 3.36850  (4.67579)
     | > loader_time: 0.02870  (0.06537)


[1m   --> STEP: 30/87 -- GLOBAL_STEP: 275750[0m
     | > decoder_loss: 1.99298  (1.85531)
     | > postnet_loss: 1.51265  (1.50584)
     | > stopnet_loss: 1.28834  (1.69129)
     | > decoder_coarse_loss: 1.92231  (1.79590)
     | > decoder_ddc_loss: 0.00098  (0.00122)
     | > ga_loss: 0.00175  (0.00230)
     | > decoder_diff_spec_loss: 0.52461  (0.49508)
     | > postnet_diff_spec_loss: 0.80011  (0.80441)
     | > decoder_ssim_loss: 0.34404  (0.29927)
     | > postnet_ssim_loss: 0.36424  (0.31265)
     | > loss: 3.16257  (3.47022)
     | > align_error: 0.99071  (0.98920)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.51750  (4.04782)
     | > current_lr: 0.00008 
     | > step_time: 4.05190  (4.05001)
     | > loader_time: 0.05480  (0.04704)


[1m   --> STEP: 55/87 -- GLOBAL_STEP: 275775[0m
     | > decoder_loss: 1.83364  (1.84666)
     | > postnet_loss: 1.49356  (1.50555)
     | > stopnet_loss: 1.26823  (1.65309)
     | > decoder_coarse_loss: 1.75896  (1.78155)
     | > decoder_ddc_loss: 0.00168  (0.00129)
     | > ga_loss: 0.00277  (0.00239)
     | > decoder_diff_spec_loss: 0.50248  (0.49349)
     | > postnet_diff_spec_loss: 0.80887  (0.80641)
     | > decoder_ssim_loss: 0.36401  (0.30437)
     | > postnet_ssim_loss: 0.37939  (0.31884)
     | > loss: 3.06773  (3.42959)
     | > align_error: 0.98571  (0.98872)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.17417  (4.11480)
     | > current_lr: 0.00008 
     | > step_time: 2.87120  (3.78027)
     | > loader_time: 0.01820  (0.04170)


[1m   --> STEP: 80/87 -- GLOBAL_STEP: 275800[0m
     | > decoder_loss: 1.82694  (1.84665)
     | > postnet_loss: 1.51155  (1.50098)
     | > stopnet_loss: 1.22244  (1.66887)
     | > decoder_coarse_loss: 1.76140  (1.78032)
     | > decoder_ddc_loss: 0.00108  (0.00122)
     | > ga_loss: 0.00197  (0.00232)
     | > decoder_diff_spec_loss: 0.47885  (0.49583)
     | > postnet_diff_spec_loss: 0.80593  (0.80735)
     | > decoder_ssim_loss: 0.36079  (0.29921)
     | > postnet_ssim_loss: 0.37684  (0.31339)
     | > loss: 3.01312  (3.44168)
     | > align_error: 0.99097  (0.98925)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.85980  (3.97408)
     | > current_lr: 0.00008 
     | > step_time: 2.82950  (3.83112)
     | > loader_time: 0.01950  (0.03674)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.49856 [0m(+0.14201)
     | > avg_decoder_loss:[91m 1.87726 [0m(+0.00898)
     | > avg_postnet_loss:[91m 2.07376 [0m(+0.06419)
     | > avg_stopnet_loss:[92m 1.47913 [0m(-0.00179)
     | > avg_decoder_coarse_loss:[92m 1.75125 [0m(-0.03496)
     | > avg_decoder_ddc_loss:[92m 0.00096 [0m(-0.00001)
     | > avg_ga_loss:[92m 0.00221 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[91m 0.43095 [0m(+0.00069)
     | > avg_postnet_diff_spec_loss:[91m 0.79564 [0m(+0.00098)
     | > avg_decoder_ssim_loss:[92m 0.30992 [0m(-0.00011)
     | > avg_postnet_ssim_loss:[91m 0.33526 [0m(+0.00109)
     | > avg_loss:[91m 3.38393 [0m(+0.00840)
     | > avg_align_error:[91m 0.99090 [0m(+0.00013)


[4m[1m > EPOCH: 66/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 17:53:55) [0m

[1m   --> STEP: 17/87 -- GLOBAL_STEP: 275825[0m
     | > decoder_loss: 1.92722  (1.84614)
     | > postnet_loss: 1.54185  (1.50260)
     | > stopnet_loss: 1.82610  (1.74929)
     | > decoder_coarse_loss: 1.83339  (1.78349)
     | > decoder_ddc_loss: 0.00119  (0.00117)
     | > ga_loss: 0.00258  (0.00233)
     | > decoder_diff_spec_loss: 0.48713  (0.48948)
     | > postnet_diff_spec_loss: 0.80080  (0.80327)
     | > decoder_ssim_loss: 0.26303  (0.29080)
     | > postnet_ssim_loss: 0.27667  (0.30425)
     | > loss: 3.62181  (3.51625)
     | > align_error: 0.98856  (0.98916)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.85481  (3.91958)
     | > current_lr: 0.00008 
     | > step_time: 3.04760  (3.95359)
     | > loader_time: 0.10980  (0.03926)


[1m   --> STEP: 42/87 -- GLOBAL_STEP: 275850[0m
     | > decoder_loss: 1.90589  (1.85669)
     | > postnet_loss: 1.60606  (1.51157)
     | > stopnet_loss: 1.31998  (1.66372)
     | > decoder_coarse_loss: 1.82184  (1.79142)
     | > decoder_ddc_loss: 0.00177  (0.00130)
     | > ga_loss: 0.00268  (0.00242)
     | > decoder_diff_spec_loss: 0.48898  (0.49658)
     | > postnet_diff_spec_loss: 0.82271  (0.80886)
     | > decoder_ssim_loss: 0.36655  (0.30529)
     | > postnet_ssim_loss: 0.38773  (0.31997)
     | > loss: 3.18376  (3.44876)
     | > align_error: 0.98661  (0.98863)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.94367  (4.25165)
     | > current_lr: 0.00008 
     | > step_time: 2.86450  (3.64970)
     | > loader_time: 0.06240  (0.03906)


[1m   --> STEP: 67/87 -- GLOBAL_STEP: 275875[0m
     | > decoder_loss: 1.69206  (1.83742)
     | > postnet_loss: 1.40928  (1.49736)
     | > stopnet_loss: 1.45681  (1.66091)
     | > decoder_coarse_loss: 1.64689  (1.76978)
     | > decoder_ddc_loss: 0.00096  (0.00128)
     | > ga_loss: 0.00214  (0.00236)
     | > decoder_diff_spec_loss: 0.45847  (0.49401)
     | > postnet_diff_spec_loss: 0.78197  (0.80682)
     | > decoder_ssim_loss: 0.32446  (0.30143)
     | > postnet_ssim_loss: 0.33625  (0.31593)
     | > loss: 3.13007  (3.42874)
     | > align_error: 0.99179  (0.98889)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.16361  (3.98665)
     | > current_lr: 0.00008 
     | > step_time: 3.90290  (3.70725)
     | > loader_time: 0.05900  (0.03530)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.51680 [0m(+0.01824)
     | > avg_decoder_loss:[92m 1.85104 [0m(-0.02622)
     | > avg_postnet_loss:[92m 1.93730 [0m(-0.13646)
     | > avg_stopnet_loss:[92m 1.47743 [0m(-0.00170)
     | > avg_decoder_coarse_loss:[91m 1.75607 [0m(+0.00482)
     | > avg_decoder_ddc_loss:[91m 0.00098 [0m(+0.00002)
     | > avg_ga_loss:[92m 0.00220 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.43071 [0m(-0.00025)
     | > avg_postnet_diff_spec_loss:[92m 0.79354 [0m(-0.00210)
     | > avg_decoder_ssim_loss:[92m 0.30914 [0m(-0.00078)
     | > avg_postnet_ssim_loss:[92m 0.33244 [0m(-0.00282)
     | > avg_loss:[92m 3.34122 [0m(-0.04271)
     | > avg_align_error:[92m 0.99071 [0m(-0.00019)


[4m[1m > EPOCH: 67/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 18:01:03) [0m

[1m   --> STEP: 4/87 -- GLOBAL_STEP: 275900[0m
     | > decoder_loss: 1.91512  (1.87156)
     | > postnet_loss: 1.51988  (1.51864)
     | > stopnet_loss: 1.70330  (1.52494)
     | > decoder_coarse_loss: 1.84035  (1.80858)
     | > decoder_ddc_loss: 0.00159  (0.00129)
     | > ga_loss: 0.00304  (0.00241)
     | > decoder_diff_spec_loss: 0.51409  (0.50643)
     | > postnet_diff_spec_loss: 0.82258  (0.82284)
     | > decoder_ssim_loss: 0.32196  (0.31943)
     | > postnet_ssim_loss: 0.32703  (0.33419)
     | > loss: 3.53414  (3.33274)
     | > align_error: 0.98483  (0.98808)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 12.70274  (5.34913)
     | > current_lr: 0.00008 
     | > step_time: 2.68790  (4.71749)
     | > loader_time: 0.03790  (0.04704)


[1m   --> STEP: 29/87 -- GLOBAL_STEP: 275925[0m
     | > decoder_loss: 1.77167  (1.84872)
     | > postnet_loss: 1.42413  (1.49610)
     | > stopnet_loss: 1.55830  (1.70229)
     | > decoder_coarse_loss: 1.72118  (1.78534)
     | > decoder_ddc_loss: 0.00146  (0.00121)
     | > ga_loss: 0.00303  (0.00232)
     | > decoder_diff_spec_loss: 0.47892  (0.49318)
     | > postnet_diff_spec_loss: 0.78840  (0.80429)
     | > decoder_ssim_loss: 0.32214  (0.29713)
     | > postnet_ssim_loss: 0.33584  (0.31065)
     | > loss: 3.28438  (3.47305)
     | > align_error: 0.98743  (0.98908)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 8.16830  (4.10721)
     | > current_lr: 0.00008 
     | > step_time: 2.69480  (3.94284)
     | > loader_time: 0.01790  (0.03076)


[1m   --> STEP: 54/87 -- GLOBAL_STEP: 275950[0m
     | > decoder_loss: 1.81174  (1.84181)
     | > postnet_loss: 1.45890  (1.49752)
     | > stopnet_loss: 1.40895  (1.65375)
     | > decoder_coarse_loss: 1.76855  (1.77760)
     | > decoder_ddc_loss: 0.00108  (0.00128)
     | > ga_loss: 0.00200  (0.00237)
     | > decoder_diff_spec_loss: 0.47317  (0.49228)
     | > postnet_diff_spec_loss: 0.78150  (0.80630)
     | > decoder_ssim_loss: 0.32467  (0.30276)
     | > postnet_ssim_loss: 0.34542  (0.31752)
     | > loss: 3.16021  (3.42488)
     | > align_error: 0.99065  (0.98866)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.87796  (4.07514)
     | > current_lr: 0.00008 
     | > step_time: 3.52860  (3.73749)
     | > loader_time: 0.02440  (0.02998)


[1m   --> STEP: 79/87 -- GLOBAL_STEP: 275975[0m
     | > decoder_loss: 2.01476  (1.83920)
     | > postnet_loss: 1.62134  (1.49149)
     | > stopnet_loss: 1.93820  (1.66837)
     | > decoder_coarse_loss: 1.95226  (1.77683)
     | > decoder_ddc_loss: 0.00090  (0.00121)
     | > ga_loss: 0.00215  (0.00231)
     | > decoder_diff_spec_loss: 0.50740  (0.49494)
     | > postnet_diff_spec_loss: 0.81581  (0.80707)
     | > decoder_ssim_loss: 0.24920  (0.29793)
     | > postnet_ssim_loss: 0.25982  (0.31240)
     | > loss: 3.80430  (3.43520)
     | > align_error: 0.99154  (0.98913)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.15054  (3.98765)
     | > current_lr: 0.00008 
     | > step_time: 3.60100  (3.75242)
     | > loader_time: 0.02870  (0.03105)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 4.08460 [0m(+0.56779)
     | > avg_decoder_loss:[92m 1.82974 [0m(-0.02130)
     | > avg_postnet_loss:[91m 1.99676 [0m(+0.05946)
     | > avg_stopnet_loss:[92m 1.47525 [0m(-0.00218)
     | > avg_decoder_coarse_loss:[91m 1.79327 [0m(+0.03720)
     | > avg_decoder_ddc_loss:[92m 0.00096 [0m(-0.00002)
     | > avg_ga_loss:[92m 0.00220 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.43119 [0m(+0.00049)
     | > avg_postnet_diff_spec_loss:[91m 0.79465 [0m(+0.00111)
     | > avg_decoder_ssim_loss:[92m 0.30888 [0m(-0.00026)
     | > avg_postnet_ssim_loss:[91m 0.33369 [0m(+0.00125)
     | > avg_loss:[91m 3.35851 [0m(+0.01729)
     | > avg_align_error:[91m 0.99074 [0m(+0.00003)


[4m[1m > EPOCH: 68/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 18:08:07) [0m

[1m   --> STEP: 16/87 -- GLOBAL_STEP: 276000[0m
     | > decoder_loss: 1.67885  (1.83339)
     | > postnet_loss: 1.52945  (1.48323)
     | > stopnet_loss: 2.08503  (1.73678)
     | > decoder_coarse_loss: 1.62506  (1.77230)
     | > decoder_ddc_loss: 0.00144  (0.00118)
     | > ga_loss: 0.00260  (0.00230)
     | > decoder_diff_spec_loss: 0.47083  (0.48959)
     | > postnet_diff_spec_loss: 0.81731  (0.80266)
     | > decoder_ssim_loss: 0.23811  (0.29220)
     | > postnet_ssim_loss: 0.25141  (0.30515)
     | > loss: 3.75115  (3.49319)
     | > align_error: 0.98601  (0.98909)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.31719  (4.10104)
     | > current_lr: 0.00008 
     | > step_time: 3.62090  (4.17287)
     | > loader_time: 0.01970  (0.05020)


[1m   --> STEP: 41/87 -- GLOBAL_STEP: 276025[0m
     | > decoder_loss: 1.73561  (1.84918)
     | > postnet_loss: 1.48029  (1.49313)
     | > stopnet_loss: 2.46411  (1.65848)
     | > decoder_coarse_loss: 1.66250  (1.78455)
     | > decoder_ddc_loss: 0.00075  (0.00127)
     | > ga_loss: 0.00188  (0.00240)
     | > decoder_diff_spec_loss: 0.46223  (0.49647)
     | > postnet_diff_spec_loss: 0.80605  (0.80781)
     | > decoder_ssim_loss: 0.20124  (0.30333)
     | > postnet_ssim_loss: 0.21588  (0.31764)
     | > loss: 4.11467  (3.43383)
     | > align_error: 0.99245  (0.98863)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.94933  (4.18888)
     | > current_lr: 0.00008 
     | > step_time: 4.45790  (3.76014)
     | > loader_time: 0.02030  (0.03941)


[1m   --> STEP: 66/87 -- GLOBAL_STEP: 276050[0m
     | > decoder_loss: 1.97349  (1.83027)
     | > postnet_loss: 1.58159  (1.48039)
     | > stopnet_loss: 1.30458  (1.65190)
     | > decoder_coarse_loss: 1.89186  (1.76830)
     | > decoder_ddc_loss: 0.00134  (0.00126)
     | > ga_loss: 0.00222  (0.00235)
     | > decoder_diff_spec_loss: 0.51052  (0.49366)
     | > postnet_diff_spec_loss: 0.83313  (0.80646)
     | > decoder_ssim_loss: 0.34080  (0.30048)
     | > postnet_ssim_loss: 0.35947  (0.31491)
     | > loss: 3.18871  (3.41260)
     | > align_error: 0.98852  (0.98880)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.07029  (3.92919)
     | > current_lr: 0.00008 
     | > step_time: 3.27390  (3.79341)
     | > loader_time: 0.01690  (0.03581)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.24352 [0m(-0.84108)
     | > avg_decoder_loss:[92m 1.80775 [0m(-0.02199)
     | > avg_postnet_loss:[92m 1.92370 [0m(-0.07305)
     | > avg_stopnet_loss:[91m 1.47883 [0m(+0.00358)
     | > avg_decoder_coarse_loss:[92m 1.72567 [0m(-0.06759)
     | > avg_decoder_ddc_loss:[91m 0.00097 [0m(+0.00001)
     | > avg_ga_loss:[92m 0.00219 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[92m 0.42904 [0m(-0.00215)
     | > avg_postnet_diff_spec_loss:[92m 0.79414 [0m(-0.00052)
     | > avg_decoder_ssim_loss:[92m 0.30849 [0m(-0.00039)
     | > avg_postnet_ssim_loss:[92m 0.33271 [0m(-0.00098)
     | > avg_loss:[92m 3.32041 [0m(-0.03810)
     | > avg_align_error:[92m 0.99065 [0m(-0.00009)


[4m[1m > EPOCH: 69/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 18:15:17) [0m

[1m   --> STEP: 3/87 -- GLOBAL_STEP: 276075[0m
     | > decoder_loss: 1.83718  (1.86639)
     | > postnet_loss: 1.49420  (1.49697)
     | > stopnet_loss: 1.77612  (1.46085)
     | > decoder_coarse_loss: 1.76771  (1.77775)
     | > decoder_ddc_loss: 0.00112  (0.00117)
     | > ga_loss: 0.00245  (0.00221)
     | > decoder_diff_spec_loss: 0.48640  (0.50304)
     | > postnet_diff_spec_loss: 0.81633  (0.82104)
     | > decoder_ssim_loss: 0.27687  (0.31870)
     | > postnet_ssim_loss: 0.28815  (0.33584)
     | > loss: 3.53036  (3.25214)
     | > align_error: 0.98923  (0.98908)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.87139  (2.67638)
     | > current_lr: 0.00008 
     | > step_time: 4.00590  (5.71650)
     | > loader_time: 0.05820  (0.05202)


[1m   --> STEP: 28/87 -- GLOBAL_STEP: 276100[0m
     | > decoder_loss: 1.68604  (1.84133)
     | > postnet_loss: 1.38546  (1.48291)
     | > stopnet_loss: 2.19725  (1.68726)
     | > decoder_coarse_loss: 1.64861  (1.77466)
     | > decoder_ddc_loss: 0.00099  (0.00117)
     | > ga_loss: 0.00224  (0.00229)
     | > decoder_diff_spec_loss: 0.49200  (0.49333)
     | > postnet_diff_spec_loss: 0.80217  (0.80384)
     | > decoder_ssim_loss: 0.22758  (0.29566)
     | > postnet_ssim_loss: 0.23747  (0.30907)
     | > loss: 3.82854  (3.44918)
     | > align_error: 0.99043  (0.98919)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.52467  (3.64091)
     | > current_lr: 0.00008 
     | > step_time: 5.23980  (4.02269)
     | > loader_time: 0.02030  (0.03633)


[1m   --> STEP: 53/87 -- GLOBAL_STEP: 276125[0m
     | > decoder_loss: 1.68634  (1.83261)
     | > postnet_loss: 1.41240  (1.48117)
     | > stopnet_loss: 1.61282  (1.64629)
     | > decoder_coarse_loss: 1.66498  (1.76717)
     | > decoder_ddc_loss: 0.00082  (0.00126)
     | > ga_loss: 0.00167  (0.00237)
     | > decoder_diff_spec_loss: 0.46580  (0.49271)
     | > postnet_diff_spec_loss: 0.78781  (0.80561)
     | > decoder_ssim_loss: 0.28204  (0.30175)
     | > postnet_ssim_loss: 0.29692  (0.31617)
     | > loss: 3.27046  (3.40775)
     | > align_error: 0.99181  (0.98864)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.47557  (3.67932)
     | > current_lr: 0.00008 
     | > step_time: 5.49200  (3.74003)
     | > loader_time: 0.02590  (0.03658)


[1m   --> STEP: 78/87 -- GLOBAL_STEP: 276150[0m
     | > decoder_loss: 1.99731  (1.82986)
     | > postnet_loss: 1.61750  (1.47508)
     | > stopnet_loss: 1.55369  (1.65307)
     | > decoder_coarse_loss: 1.93323  (1.76510)
     | > decoder_ddc_loss: 0.00094  (0.00119)
     | > ga_loss: 0.00195  (0.00230)
     | > decoder_diff_spec_loss: 0.53142  (0.49451)
     | > postnet_diff_spec_loss: 0.83575  (0.80601)
     | > decoder_ssim_loss: 0.29478  (0.29794)
     | > postnet_ssim_loss: 0.30841  (0.31239)
     | > loss: 3.44328  (3.41010)
     | > align_error: 0.99173  (0.98911)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.21499  (3.67656)
     | > current_lr: 0.00008 
     | > step_time: 4.11580  (3.77764)
     | > loader_time: 0.02660  (0.03565)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.35543 [0m(+0.11191)
     | > avg_decoder_loss:[92m 1.80210 [0m(-0.00565)
     | > avg_postnet_loss:[92m 1.90759 [0m(-0.01611)
     | > avg_stopnet_loss:[92m 1.47568 [0m(-0.00315)
     | > avg_decoder_coarse_loss:[91m 1.81277 [0m(+0.08710)
     | > avg_decoder_ddc_loss:[91m 0.00098 [0m(+0.00001)
     | > avg_ga_loss:[92m 0.00219 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.43104 [0m(+0.00199)
     | > avg_postnet_diff_spec_loss:[92m 0.79376 [0m(-0.00037)
     | > avg_decoder_ssim_loss:[92m 0.30823 [0m(-0.00026)
     | > avg_postnet_ssim_loss:[92m 0.33227 [0m(-0.00044)
     | > avg_loss:[91m 3.33380 [0m(+0.01340)
     | > avg_align_error:[91m 0.99069 [0m(+0.00004)


[4m[1m > EPOCH: 70/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 18:22:21) [0m

[1m   --> STEP: 15/87 -- GLOBAL_STEP: 276175[0m
     | > decoder_loss: 1.79919  (1.83896)
     | > postnet_loss: 1.51968  (1.48385)
     | > stopnet_loss: 2.39917  (1.72173)
     | > decoder_coarse_loss: 1.76558  (1.78238)
     | > decoder_ddc_loss: 0.00086  (0.00116)
     | > ga_loss: 0.00216  (0.00226)
     | > decoder_diff_spec_loss: 0.48585  (0.48931)
     | > postnet_diff_spec_loss: 0.80663  (0.80111)
     | > decoder_ssim_loss: 0.21841  (0.29527)
     | > postnet_ssim_loss: 0.22569  (0.30863)
     | > loss: 4.11545  (3.48322)
     | > align_error: 0.99159  (0.98925)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.10366  (3.99569)
     | > current_lr: 0.00008 
     | > step_time: 4.37040  (4.05623)
     | > loader_time: 0.02280  (0.03231)


[1m   --> STEP: 40/87 -- GLOBAL_STEP: 276200[0m
     | > decoder_loss: 1.72816  (1.84824)
     | > postnet_loss: 1.37770  (1.49138)
     | > stopnet_loss: 1.16362  (1.64313)
     | > decoder_coarse_loss: 1.69209  (1.78530)
     | > decoder_ddc_loss: 0.00181  (0.00129)
     | > ga_loss: 0.00275  (0.00240)
     | > decoder_diff_spec_loss: 0.47576  (0.49659)
     | > postnet_diff_spec_loss: 0.79696  (0.80715)
     | > decoder_ssim_loss: 0.40347  (0.30518)
     | > postnet_ssim_loss: 0.43277  (0.31995)
     | > loss: 2.90456  (3.41893)
     | > align_error: 0.98626  (0.98845)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.24467  (4.24590)
     | > current_lr: 0.00008 
     | > step_time: 3.30550  (3.70262)
     | > loader_time: 0.06230  (0.03281)


[1m   --> STEP: 65/87 -- GLOBAL_STEP: 276225[0m
     | > decoder_loss: 1.64620  (1.82374)
     | > postnet_loss: 1.44189  (1.47734)
     | > stopnet_loss: 1.59518  (1.65916)
     | > decoder_coarse_loss: 1.60194  (1.76108)
     | > decoder_ddc_loss: 0.00089  (0.00127)
     | > ga_loss: 0.00191  (0.00234)
     | > decoder_diff_spec_loss: 0.45742  (0.49267)
     | > postnet_diff_spec_loss: 0.80740  (0.80556)
     | > decoder_ssim_loss: 0.28310  (0.29931)
     | > postnet_ssim_loss: 0.30087  (0.31402)
     | > loss: 3.23968  (3.41460)
     | > align_error: 0.99208  (0.98871)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.22623  (4.09940)
     | > current_lr: 0.00008 
     | > step_time: 4.33810  (3.75258)
     | > loader_time: 0.03520  (0.03419)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.34283 [0m(-0.01260)
     | > avg_decoder_loss:[91m 1.86621 [0m(+0.06411)
     | > avg_postnet_loss:[91m 2.24636 [0m(+0.33877)
     | > avg_stopnet_loss:[92m 1.47266 [0m(-0.00302)
     | > avg_decoder_coarse_loss:[92m 1.78773 [0m(-0.02504)
     | > avg_decoder_ddc_loss:[91m 0.00099 [0m(+0.00001)
     | > avg_ga_loss:[92m 0.00219 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.43438 [0m(+0.00334)
     | > avg_postnet_diff_spec_loss:[91m 0.79532 [0m(+0.00156)
     | > avg_decoder_ssim_loss:[91m 0.30837 [0m(+0.00014)
     | > avg_postnet_ssim_loss:[91m 0.33557 [0m(+0.00330)
     | > avg_loss:[91m 3.42732 [0m(+0.09351)
     | > avg_align_error:[92m 0.99068 [0m(-0.00001)


[4m[1m > EPOCH: 71/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 18:29:25) [0m

[1m   --> STEP: 2/87 -- GLOBAL_STEP: 276250[0m
     | > decoder_loss: 1.98555  (1.87216)
     | > postnet_loss: 1.59390  (1.50287)
     | > stopnet_loss: 1.34404  (1.29520)
     | > decoder_coarse_loss: 1.87225  (1.79726)
     | > decoder_ddc_loss: 0.00127  (0.00119)
     | > ga_loss: 0.00221  (0.00208)
     | > decoder_diff_spec_loss: 0.52712  (0.50987)
     | > postnet_diff_spec_loss: 0.84052  (0.82361)
     | > decoder_ssim_loss: 0.33074  (0.33847)
     | > postnet_ssim_loss: 0.35436  (0.35993)
     | > loss: 3.23155  (3.10692)
     | > align_error: 0.98811  (0.98903)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.30820  (2.61155)
     | > current_lr: 0.00008 
     | > step_time: 5.98740  (6.83914)
     | > loader_time: 0.03780  (0.03310)


[1m   --> STEP: 27/87 -- GLOBAL_STEP: 276275[0m
     | > decoder_loss: 1.81941  (1.83674)
     | > postnet_loss: 1.49565  (1.47288)
     | > stopnet_loss: 1.44866  (1.67565)
     | > decoder_coarse_loss: 1.78921  (1.77256)
     | > decoder_ddc_loss: 0.00112  (0.00119)
     | > ga_loss: 0.00216  (0.00228)
     | > decoder_diff_spec_loss: 0.48654  (0.49401)
     | > postnet_diff_spec_loss: 0.80393  (0.80339)
     | > decoder_ssim_loss: 0.32042  (0.29750)
     | > postnet_ssim_loss: 0.33388  (0.31133)
     | > loss: 3.22200  (3.43444)
     | > align_error: 0.99060  (0.98909)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.61127  (3.99462)
     | > current_lr: 0.00008 
     | > step_time: 3.27950  (3.94644)
     | > loader_time: 0.02170  (0.03544)


[1m   --> STEP: 52/87 -- GLOBAL_STEP: 276300[0m
     | > decoder_loss: 1.93542  (1.82616)
     | > postnet_loss: 1.41862  (1.47255)
     | > stopnet_loss: 1.57374  (1.64523)
     | > decoder_coarse_loss: 1.88216  (1.76327)
     | > decoder_ddc_loss: 0.00131  (0.00130)
     | > ga_loss: 0.00217  (0.00237)
     | > decoder_diff_spec_loss: 0.51891  (0.49293)
     | > postnet_diff_spec_loss: 0.79982  (0.80596)
     | > decoder_ssim_loss: 0.30353  (0.30148)
     | > postnet_ssim_loss: 0.31282  (0.31638)
     | > loss: 3.37773  (3.40208)
     | > align_error: 0.98777  (0.98850)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.80687  (4.13394)
     | > current_lr: 0.00008 
     | > step_time: 3.70250  (3.69640)
     | > loader_time: 0.02230  (0.03367)


[1m   --> STEP: 77/87 -- GLOBAL_STEP: 276325[0m
     | > decoder_loss: 1.78805  (1.81692)
     | > postnet_loss: 1.55003  (1.46245)
     | > stopnet_loss: 1.47502  (1.65236)
     | > decoder_coarse_loss: 1.75432  (1.75505)
     | > decoder_ddc_loss: 0.00087  (0.00124)
     | > ga_loss: 0.00212  (0.00229)
     | > decoder_diff_spec_loss: 0.49363  (0.49388)
     | > postnet_diff_spec_loss: 0.83069  (0.80565)
     | > decoder_ssim_loss: 0.30732  (0.29738)
     | > postnet_ssim_loss: 0.32115  (0.31221)
     | > loss: 3.24712  (3.40002)
     | > align_error: 0.99185  (0.98897)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.91299  (3.94529)
     | > current_lr: 0.00008 
     | > step_time: 3.02750  (3.75663)
     | > loader_time: 0.01890  (0.03221)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.43377 [0m(+0.09093)
     | > avg_decoder_loss:[92m 1.79548 [0m(-0.07073)
     | > avg_postnet_loss:[92m 1.83020 [0m(-0.41616)
     | > avg_stopnet_loss:[92m 1.47239 [0m(-0.00027)
     | > avg_decoder_coarse_loss:[92m 1.68799 [0m(-0.09974)
     | > avg_decoder_ddc_loss:[91m 0.00101 [0m(+0.00001)
     | > avg_ga_loss:[92m 0.00217 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.42926 [0m(-0.00512)
     | > avg_postnet_diff_spec_loss:[92m 0.79279 [0m(-0.00253)
     | > avg_decoder_ssim_loss:[92m 0.30747 [0m(-0.00090)
     | > avg_postnet_ssim_loss:[92m 0.33080 [0m(-0.00476)
     | > avg_loss:[92m 3.27699 [0m(-0.15032)
     | > avg_align_error:[92m 0.99054 [0m(-0.00014)


[4m[1m > EPOCH: 72/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 18:36:30) [0m

[1m   --> STEP: 14/87 -- GLOBAL_STEP: 276350[0m
     | > decoder_loss: 1.97730  (1.82749)
     | > postnet_loss: 1.52087  (1.46764)
     | > stopnet_loss: 1.77945  (1.64893)
     | > decoder_coarse_loss: 1.89857  (1.77245)
     | > decoder_ddc_loss: 0.00107  (0.00122)
     | > ga_loss: 0.00211  (0.00225)
     | > decoder_diff_spec_loss: 0.51434  (0.48787)
     | > postnet_diff_spec_loss: 0.79429  (0.80047)
     | > decoder_ssim_loss: 0.27705  (0.30008)
     | > postnet_ssim_loss: 0.29378  (0.31410)
     | > loss: 3.60934  (3.40303)
     | > align_error: 0.99040  (0.98903)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.60905  (3.71496)
     | > current_lr: 0.00008 
     | > step_time: 2.91970  (4.11435)
     | > loader_time: 0.05720  (0.06013)


[1m   --> STEP: 39/87 -- GLOBAL_STEP: 276375[0m
     | > decoder_loss: 1.77125  (1.83454)
     | > postnet_loss: 1.44398  (1.48175)
     | > stopnet_loss: 1.33881  (1.63671)
     | > decoder_coarse_loss: 1.70359  (1.77788)
     | > decoder_ddc_loss: 0.00173  (0.00129)
     | > ga_loss: 0.00289  (0.00238)
     | > decoder_diff_spec_loss: 0.49694  (0.49592)
     | > postnet_diff_spec_loss: 0.81008  (0.80700)
     | > decoder_ssim_loss: 0.35167  (0.30196)
     | > postnet_ssim_loss: 0.36908  (0.31661)
     | > loss: 3.09034  (3.40283)
     | > align_error: 0.98522  (0.98846)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.55030  (4.03977)
     | > current_lr: 0.00008 
     | > step_time: 2.76100  (3.78054)
     | > loader_time: 0.04670  (0.03822)


[1m   --> STEP: 64/87 -- GLOBAL_STEP: 276400[0m
     | > decoder_loss: 1.82810  (1.81237)
     | > postnet_loss: 1.48050  (1.46683)
     | > stopnet_loss: 1.31559  (1.64919)
     | > decoder_coarse_loss: 1.75003  (1.75439)
     | > decoder_ddc_loss: 0.00140  (0.00126)
     | > ga_loss: 0.00229  (0.00234)
     | > decoder_diff_spec_loss: 0.50951  (0.49244)
     | > postnet_diff_spec_loss: 0.80874  (0.80506)
     | > decoder_ssim_loss: 0.32817  (0.29891)
     | > postnet_ssim_loss: 0.34081  (0.31385)
     | > loss: 3.08885  (3.39715)
     | > align_error: 0.98766  (0.98869)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.02174  (3.98162)
     | > current_lr: 0.00008 
     | > step_time: 4.38470  (3.84296)
     | > loader_time: 0.05990  (0.03829)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.42808 [0m(-0.00569)
     | > avg_decoder_loss:[92m 1.78235 [0m(-0.01313)
     | > avg_postnet_loss:[91m 1.90026 [0m(+0.07006)
     | > avg_stopnet_loss:[92m 1.47170 [0m(-0.00069)
     | > avg_decoder_coarse_loss:[91m 1.70389 [0m(+0.01591)
     | > avg_decoder_ddc_loss:[92m 0.00098 [0m(-0.00003)
     | > avg_ga_loss:[91m 0.00218 [0m(+0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.42987 [0m(+0.00061)
     | > avg_postnet_diff_spec_loss:[91m 0.79312 [0m(+0.00033)
     | > avg_decoder_ssim_loss:[91m 0.30762 [0m(+0.00015)
     | > avg_postnet_ssim_loss:[91m 0.33170 [0m(+0.00089)
     | > avg_loss:[91m 3.29503 [0m(+0.01803)
     | > avg_align_error:[91m 0.99062 [0m(+0.00008)


[4m[1m > EPOCH: 73/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 18:43:43) [0m

[1m   --> STEP: 1/87 -- GLOBAL_STEP: 276425[0m
     | > decoder_loss: 1.78144  (1.78144)
     | > postnet_loss: 1.37413  (1.37413)
     | > stopnet_loss: 1.28713  (1.28713)
     | > decoder_coarse_loss: 1.73959  (1.73959)
     | > decoder_ddc_loss: 0.00115  (0.00115)
     | > ga_loss: 0.00193  (0.00193)
     | > decoder_diff_spec_loss: 0.49659  (0.49659)
     | > postnet_diff_spec_loss: 0.80274  (0.80274)
     | > decoder_ssim_loss: 0.34683  (0.34683)
     | > postnet_ssim_loss: 0.36266  (0.36266)
     | > loss: 3.02306  (3.02306)
     | > align_error: 0.99007  (0.99007)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.69268  (1.69268)
     | > current_lr: 0.00008 
     | > step_time: 7.33010  (7.33013)
     | > loader_time: 0.07510  (0.07506)


[1m   --> STEP: 26/87 -- GLOBAL_STEP: 276450[0m
     | > decoder_loss: 1.95587  (1.83048)
     | > postnet_loss: 1.46735  (1.46752)
     | > stopnet_loss: 1.72893  (1.67304)
     | > decoder_coarse_loss: 1.90354  (1.76529)
     | > decoder_ddc_loss: 0.00085  (0.00118)
     | > ga_loss: 0.00184  (0.00227)
     | > decoder_diff_spec_loss: 0.52448  (0.49194)
     | > postnet_diff_spec_loss: 0.80030  (0.80264)
     | > decoder_ssim_loss: 0.26693  (0.29604)
     | > postnet_ssim_loss: 0.28468  (0.30992)
     | > loss: 3.53912  (3.42562)
     | > align_error: 0.99184  (0.98906)
     | > amp_scaler: 65536.00000  (51672.61538)
     | > grad_norm: 3.19739  (3.89131)
     | > current_lr: 0.00008 
     | > step_time: 4.64940  (4.00494)
     | > loader_time: 0.02210  (0.03670)


[1m   --> STEP: 51/87 -- GLOBAL_STEP: 276475[0m
     | > decoder_loss: 1.72632  (1.81677)
     | > postnet_loss: 1.50598  (1.46638)
     | > stopnet_loss: 1.60964  (1.64832)
     | > decoder_coarse_loss: 1.64488  (1.75382)
     | > decoder_ddc_loss: 0.00152  (0.00128)
     | > ga_loss: 0.00245  (0.00236)
     | > decoder_diff_spec_loss: 0.45800  (0.49124)
     | > postnet_diff_spec_loss: 0.79791  (0.80509)
     | > decoder_ssim_loss: 0.29071  (0.30082)
     | > postnet_ssim_loss: 0.30953  (0.31589)
     | > loss: 3.30558  (3.39795)
     | > align_error: 0.98702  (0.98852)
     | > amp_scaler: 32768.00000  (48188.23529)
     | > grad_norm: 2.26682  (3.73116)
     | > current_lr: 0.00008 
     | > step_time: 4.05500  (3.72678)
     | > loader_time: 0.01780  (0.03674)


[1m   --> STEP: 76/87 -- GLOBAL_STEP: 276500[0m
     | > decoder_loss: 1.81879  (1.81250)
     | > postnet_loss: 1.38806  (1.45538)
     | > stopnet_loss: 1.93658  (1.65510)
     | > decoder_coarse_loss: 1.75967  (1.75025)
     | > decoder_ddc_loss: 0.00089  (0.00123)
     | > ga_loss: 0.00196  (0.00229)
     | > decoder_diff_spec_loss: 0.51148  (0.49300)
     | > postnet_diff_spec_loss: 0.79900  (0.80440)
     | > decoder_ssim_loss: 0.25054  (0.29672)
     | > postnet_ssim_loss: 0.26040  (0.31154)
     | > loss: 3.64361  (3.39779)
     | > align_error: 0.99096  (0.98894)
     | > amp_scaler: 32768.00000  (43115.78947)
     | > grad_norm: 2.33115  (3.56718)
     | > current_lr: 0.00008 
     | > step_time: 4.02550  (3.76038)
     | > loader_time: 0.01990  (0.03346)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.81525 [0m(+0.38718)
     | > avg_decoder_loss:[92m 1.77675 [0m(-0.00559)
     | > avg_postnet_loss:[92m 1.82035 [0m(-0.07992)
     | > avg_stopnet_loss:[92m 1.47046 [0m(-0.00125)
     | > avg_decoder_coarse_loss:[92m 1.69730 [0m(-0.00659)
     | > avg_decoder_ddc_loss:[91m 0.00100 [0m(+0.00002)
     | > avg_ga_loss:[91m 0.00218 [0m(+0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.43000 [0m(+0.00013)
     | > avg_postnet_diff_spec_loss:[92m 0.79283 [0m(-0.00029)
     | > avg_decoder_ssim_loss:[92m 0.30710 [0m(-0.00052)
     | > avg_postnet_ssim_loss:[92m 0.33093 [0m(-0.00076)
     | > avg_loss:[92m 3.27040 [0m(-0.02462)
     | > avg_align_error:[91m 0.99064 [0m(+0.00003)


[4m[1m > EPOCH: 74/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 18:50:49) [0m

[1m   --> STEP: 13/87 -- GLOBAL_STEP: 276525[0m
     | > decoder_loss: 1.95725  (1.80943)
     | > postnet_loss: 1.62271  (1.44897)
     | > stopnet_loss: 2.04624  (1.65424)
     | > decoder_coarse_loss: 1.88133  (1.74138)
     | > decoder_ddc_loss: 0.00150  (0.00119)
     | > ga_loss: 0.00243  (0.00226)
     | > decoder_diff_spec_loss: 0.48265  (0.48690)
     | > postnet_diff_spec_loss: 0.79956  (0.80042)
     | > decoder_ssim_loss: 0.23556  (0.30157)
     | > postnet_ssim_loss: 0.25013  (0.31518)
     | > loss: 3.86607  (3.39182)
     | > align_error: 0.98622  (0.98895)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.68298  (4.01791)
     | > current_lr: 0.00008 
     | > step_time: 3.51380  (4.32039)
     | > loader_time: 0.02030  (0.03107)


[1m   --> STEP: 38/87 -- GLOBAL_STEP: 276550[0m
     | > decoder_loss: 2.00347  (1.83745)
     | > postnet_loss: 1.53032  (1.47383)
     | > stopnet_loss: 1.69327  (1.65105)
     | > decoder_coarse_loss: 1.96328  (1.76848)
     | > decoder_ddc_loss: 0.00143  (0.00124)
     | > ga_loss: 0.00219  (0.00236)
     | > decoder_diff_spec_loss: 0.54609  (0.49629)
     | > postnet_diff_spec_loss: 0.83337  (0.80660)
     | > decoder_ssim_loss: 0.27665  (0.30038)
     | > postnet_ssim_loss: 0.29233  (0.31486)
     | > loss: 3.56596  (3.41264)
     | > align_error: 0.98720  (0.98859)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 7.01030  (4.01394)
     | > current_lr: 0.00008 
     | > step_time: 4.08160  (3.78512)
     | > loader_time: 0.06270  (0.03342)


[1m   --> STEP: 63/87 -- GLOBAL_STEP: 276575[0m
     | > decoder_loss: 1.69586  (1.81150)
     | > postnet_loss: 1.40397  (1.45541)
     | > stopnet_loss: 2.15777  (1.65572)
     | > decoder_coarse_loss: 1.63615  (1.74904)
     | > decoder_ddc_loss: 0.00117  (0.00126)
     | > ga_loss: 0.00257  (0.00233)
     | > decoder_diff_spec_loss: 0.48029  (0.49204)
     | > postnet_diff_spec_loss: 0.79727  (0.80454)
     | > decoder_ssim_loss: 0.23307  (0.29807)
     | > postnet_ssim_loss: 0.24433  (0.31299)
     | > loss: 3.79363  (3.39857)
     | > align_error: 0.98898  (0.98866)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.88270  (3.88267)
     | > current_lr: 0.00008 
     | > step_time: 5.09220  (3.74575)
     | > loader_time: 0.01720  (0.03432)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.44427 [0m(-0.37099)
     | > avg_decoder_loss:[91m 1.80257 [0m(+0.02582)
     | > avg_postnet_loss:[91m 1.94328 [0m(+0.12293)
     | > avg_stopnet_loss:[92m 1.46883 [0m(-0.00162)
     | > avg_decoder_coarse_loss:[91m 1.72688 [0m(+0.02958)
     | > avg_decoder_ddc_loss:[91m 0.00102 [0m(+0.00002)
     | > avg_ga_loss:[92m 0.00217 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.42935 [0m(-0.00065)
     | > avg_postnet_diff_spec_loss:[91m 0.79317 [0m(+0.00034)
     | > avg_decoder_ssim_loss:[92m 0.30704 [0m(-0.00005)
     | > avg_postnet_ssim_loss:[91m 0.33256 [0m(+0.00163)
     | > avg_loss:[91m 3.31364 [0m(+0.04323)
     | > avg_align_error:[92m 0.99053 [0m(-0.00011)


[4m[1m > EPOCH: 75/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 18:57:55) [0m

[1m   --> STEP: 0/87 -- GLOBAL_STEP: 276600[0m
     | > decoder_loss: 1.82725  (1.82725)
     | > postnet_loss: 1.47455  (1.47455)
     | > stopnet_loss: 0.97113  (0.97113)
     | > decoder_coarse_loss: 1.74128  (1.74128)
     | > decoder_ddc_loss: 0.00166  (0.00166)
     | > ga_loss: 0.00236  (0.00236)
     | > decoder_diff_spec_loss: 0.47706  (0.47706)
     | > postnet_diff_spec_loss: 0.80202  (0.80202)
     | > decoder_ssim_loss: 0.42146  (0.42146)
     | > postnet_ssim_loss: 0.44783  (0.44783)
     | > loss: 2.78121  (2.78121)
     | > align_error: 0.98730  (0.98730)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.90529  (1.90529)
     | > current_lr: 0.00008 
     | > step_time: 5.24490  (5.24488)
     | > loader_time: 6.83530  (6.83529)


[1m   --> STEP: 25/87 -- GLOBAL_STEP: 276625[0m
     | > decoder_loss: 2.01573  (1.81724)
     | > postnet_loss: 1.50547  (1.45102)
     | > stopnet_loss: 1.86054  (1.67570)
     | > decoder_coarse_loss: 1.97636  (1.76043)
     | > decoder_ddc_loss: 0.00105  (0.00122)
     | > ga_loss: 0.00213  (0.00227)
     | > decoder_diff_spec_loss: 0.54390  (0.49090)
     | > postnet_diff_spec_loss: 0.81864  (0.80283)
     | > decoder_ssim_loss: 0.25818  (0.29698)
     | > postnet_ssim_loss: 0.26358  (0.31060)
     | > loss: 3.71692  (3.41985)
     | > align_error: 0.98994  (0.98890)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.98042  (3.93514)
     | > current_lr: 0.00008 
     | > step_time: 4.58930  (4.05215)
     | > loader_time: 0.02610  (0.03510)


[1m   --> STEP: 50/87 -- GLOBAL_STEP: 276650[0m
     | > decoder_loss: 1.70395  (1.81096)
     | > postnet_loss: 1.43719  (1.45394)
     | > stopnet_loss: 1.68985  (1.64736)
     | > decoder_coarse_loss: 1.66290  (1.75459)
     | > decoder_ddc_loss: 0.00152  (0.00129)
     | > ga_loss: 0.00246  (0.00235)
     | > decoder_diff_spec_loss: 0.45760  (0.49203)
     | > postnet_diff_spec_loss: 0.79677  (0.80537)
     | > decoder_ssim_loss: 0.30126  (0.30077)
     | > postnet_ssim_loss: 0.31525  (0.31580)
     | > loss: 3.37128  (3.39279)
     | > align_error: 0.98774  (0.98856)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.74749  (4.18506)
     | > current_lr: 0.00008 
     | > step_time: 3.23380  (3.77568)
     | > loader_time: 0.01830  (0.03325)


[1m   --> STEP: 75/87 -- GLOBAL_STEP: 276675[0m
     | > decoder_loss: 1.86537  (1.80562)
     | > postnet_loss: 1.48693  (1.44488)
     | > stopnet_loss: 2.00443  (1.65155)
     | > decoder_coarse_loss: 1.78211  (1.74686)
     | > decoder_ddc_loss: 0.00103  (0.00123)
     | > ga_loss: 0.00208  (0.00228)
     | > decoder_diff_spec_loss: 0.51298  (0.49230)
     | > postnet_diff_spec_loss: 0.81466  (0.80443)
     | > decoder_ssim_loss: 0.23988  (0.29704)
     | > postnet_ssim_loss: 0.25215  (0.31191)
     | > loss: 3.75362  (3.38902)
     | > align_error: 0.99007  (0.98893)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.96359  (3.95196)
     | > current_lr: 0.00008 
     | > step_time: 4.03200  (3.80273)
     | > loader_time: 0.02620  (0.03433)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.42434 [0m(-0.01993)
     | > avg_decoder_loss:[91m 1.80433 [0m(+0.00176)
     | > avg_postnet_loss:[91m 1.97066 [0m(+0.02738)
     | > avg_stopnet_loss:[91m 1.46923 [0m(+0.00040)
     | > avg_decoder_coarse_loss:[92m 1.70968 [0m(-0.01720)
     | > avg_decoder_ddc_loss:[92m 0.00101 [0m(-0.00002)
     | > avg_ga_loss:[91m 0.00217 [0m(+0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.43015 [0m(+0.00081)
     | > avg_postnet_diff_spec_loss:[91m 0.79391 [0m(+0.00074)
     | > avg_decoder_ssim_loss:[91m 0.30721 [0m(+0.00016)
     | > avg_postnet_ssim_loss:[91m 0.33266 [0m(+0.00010)
     | > avg_loss:[91m 3.31748 [0m(+0.00384)
     | > avg_align_error:[91m 0.99060 [0m(+0.00008)


[4m[1m > EPOCH: 76/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 19:05:03) [0m

[1m   --> STEP: 12/87 -- GLOBAL_STEP: 276700[0m
     | > decoder_loss: 1.91466  (1.78907)
     | > postnet_loss: 1.41898  (1.42525)
     | > stopnet_loss: 2.27756  (1.61362)
     | > decoder_coarse_loss: 1.85319  (1.73486)
     | > decoder_ddc_loss: 0.00094  (0.00121)
     | > ga_loss: 0.00220  (0.00223)
     | > decoder_diff_spec_loss: 0.53214  (0.48599)
     | > postnet_diff_spec_loss: 0.80955  (0.79977)
     | > decoder_ssim_loss: 0.22390  (0.30676)
     | > postnet_ssim_loss: 0.22927  (0.32002)
     | > loss: 4.03420  (3.34050)
     | > align_error: 0.99106  (0.98899)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.19263  (4.29719)
     | > current_lr: 0.00008 
     | > step_time: 4.84100  (4.27060)
     | > loader_time: 0.02400  (0.03493)


[1m   --> STEP: 37/87 -- GLOBAL_STEP: 276725[0m
     | > decoder_loss: 1.92032  (1.82275)
     | > postnet_loss: 1.59343  (1.46749)
     | > stopnet_loss: 1.49310  (1.64485)
     | > decoder_coarse_loss: 1.81890  (1.76586)
     | > decoder_ddc_loss: 0.00116  (0.00125)
     | > ga_loss: 0.00237  (0.00236)
     | > decoder_diff_spec_loss: 0.49537  (0.49407)
     | > postnet_diff_spec_loss: 0.80678  (0.80570)
     | > decoder_ssim_loss: 0.31665  (0.30068)
     | > postnet_ssim_loss: 0.33478  (0.31516)
     | > loss: 3.32679  (3.39988)
     | > align_error: 0.99001  (0.98859)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.32600  (4.64926)
     | > current_lr: 0.00008 
     | > step_time: 3.07080  (3.77413)
     | > loader_time: 0.01310  (0.03297)


[1m   --> STEP: 62/87 -- GLOBAL_STEP: 276750[0m
     | > decoder_loss: 1.56259  (1.80624)
     | > postnet_loss: 1.29072  (1.45177)
     | > stopnet_loss: 1.96027  (1.64411)
     | > decoder_coarse_loss: 1.50090  (1.74586)
     | > decoder_ddc_loss: 0.00092  (0.00124)
     | > ga_loss: 0.00205  (0.00232)
     | > decoder_diff_spec_loss: 0.45453  (0.49156)
     | > postnet_diff_spec_loss: 0.77612  (0.80433)
     | > decoder_ssim_loss: 0.25174  (0.29885)
     | > postnet_ssim_loss: 0.26464  (0.31380)
     | > loss: 3.49604  (3.38410)
     | > align_error: 0.99112  (0.98871)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.91021  (4.37622)
     | > current_lr: 0.00008 
     | > step_time: 4.68900  (3.77040)
     | > loader_time: 0.02140  (0.03337)


[1m   --> STEP: 87/87 -- GLOBAL_STEP: 276775[0m
     | > decoder_loss: 1.56693  (1.80846)
     | > postnet_loss: 1.38472  (1.44827)
     | > stopnet_loss: 1.12428  (1.64313)
     | > decoder_coarse_loss: 1.52961  (1.74703)
     | > decoder_ddc_loss: 0.00133  (0.00120)
     | > ga_loss: 0.00176  (0.00224)
     | > decoder_diff_spec_loss: 0.46736  (0.49348)
     | > postnet_diff_spec_loss: 0.81686  (0.80487)
     | > decoder_ssim_loss: 0.39379  (0.29802)
     | > postnet_ssim_loss: 0.40579  (0.31273)
     | > loss: 2.77470  (3.38285)
     | > align_error: 0.98926  (0.98907)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.99387  (4.06367)
     | > current_lr: 0.00008 
     | > step_time: 2.23600  (3.65236)
     | > loader_time: 0.00820  (0.03121)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.37703 [0m(-0.04731)
     | > avg_decoder_loss:[92m 1.77715 [0m(-0.02718)
     | > avg_postnet_loss:[92m 1.87458 [0m(-0.09608)
     | > avg_stopnet_loss:[91m 1.46955 [0m(+0.00032)
     | > avg_decoder_coarse_loss:[91m 1.73782 [0m(+0.02813)
     | > avg_decoder_ddc_loss:[92m 0.00098 [0m(-0.00002)
     | > avg_ga_loss:[91m 0.00217 [0m(+0.00000)
     | > avg_decoder_diff_spec_loss:[92m 0.42901 [0m(-0.00115)
     | > avg_postnet_diff_spec_loss:[92m 0.79339 [0m(-0.00052)
     | > avg_decoder_ssim_loss:[92m 0.30656 [0m(-0.00065)
     | > avg_postnet_ssim_loss:[92m 0.33224 [0m(-0.00042)
     | > avg_loss:[92m 3.29335 [0m(-0.02412)
     | > avg_align_error:[91m 0.99063 [0m(+0.00002)


[4m[1m > EPOCH: 77/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 19:12:07) [0m

[1m   --> STEP: 24/87 -- GLOBAL_STEP: 276800[0m
     | > decoder_loss: 2.04583  (1.79844)
     | > postnet_loss: 1.56023  (1.44582)
     | > stopnet_loss: 1.85462  (1.65710)
     | > decoder_coarse_loss: 1.98113  (1.74493)
     | > decoder_ddc_loss: 0.00102  (0.00121)
     | > ga_loss: 0.00237  (0.00227)
     | > decoder_diff_spec_loss: 0.54668  (0.48613)
     | > postnet_diff_spec_loss: 0.84190  (0.80150)
     | > decoder_ssim_loss: 0.25716  (0.29783)
     | > postnet_ssim_loss: 0.26712  (0.31206)
     | > loss: 3.74175  (3.39041)
     | > align_error: 0.98926  (0.98878)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.83596  (3.81298)
     | > current_lr: 0.00008 
     | > step_time: 4.47220  (4.02214)
     | > loader_time: 0.02310  (0.03987)


[1m   --> STEP: 49/87 -- GLOBAL_STEP: 276825[0m
     | > decoder_loss: 1.74973  (1.80796)
     | > postnet_loss: 1.38313  (1.45256)
     | > stopnet_loss: 1.57136  (1.63334)
     | > decoder_coarse_loss: 1.66574  (1.75064)
     | > decoder_ddc_loss: 0.00121  (0.00125)
     | > ga_loss: 0.00223  (0.00234)
     | > decoder_diff_spec_loss: 0.46718  (0.49152)
     | > postnet_diff_spec_loss: 0.78361  (0.80512)
     | > decoder_ssim_loss: 0.28650  (0.30015)
     | > postnet_ssim_loss: 0.30263  (0.31544)
     | > loss: 3.24244  (3.37618)
     | > align_error: 0.98929  (0.98854)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.26282  (3.94663)
     | > current_lr: 0.00008 
     | > step_time: 3.85790  (3.78147)
     | > loader_time: 0.01740  (0.03505)


[1m   --> STEP: 74/87 -- GLOBAL_STEP: 276850[0m
     | > decoder_loss: 1.86068  (1.79877)
     | > postnet_loss: 1.45271  (1.44062)
     | > stopnet_loss: 1.50999  (1.63079)
     | > decoder_coarse_loss: 1.80591  (1.73978)
     | > decoder_ddc_loss: 0.00101  (0.00123)
     | > ga_loss: 0.00192  (0.00227)
     | > decoder_diff_spec_loss: 0.49923  (0.49112)
     | > postnet_diff_spec_loss: 0.78675  (0.80386)
     | > decoder_ssim_loss: 0.30218  (0.29726)
     | > postnet_ssim_loss: 0.31811  (0.31247)
     | > loss: 3.27623  (3.36341)
     | > align_error: 0.99188  (0.98882)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.95401  (3.73040)
     | > current_lr: 0.00008 
     | > step_time: 4.71590  (3.79883)
     | > loader_time: 0.05260  (0.03446)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.52598 [0m(+0.14895)
     | > avg_decoder_loss:[91m 1.82565 [0m(+0.04850)
     | > avg_postnet_loss:[91m 1.94468 [0m(+0.07010)
     | > avg_stopnet_loss:[92m 1.46813 [0m(-0.00142)
     | > avg_decoder_coarse_loss:[92m 1.70463 [0m(-0.03318)
     | > avg_decoder_ddc_loss:[91m 0.00100 [0m(+0.00002)
     | > avg_ga_loss:[92m 0.00217 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[92m 0.42666 [0m(-0.00235)
     | > avg_postnet_diff_spec_loss:[92m 0.79320 [0m(-0.00019)
     | > avg_decoder_ssim_loss:[92m 0.30638 [0m(-0.00017)
     | > avg_postnet_ssim_loss:[91m 0.33225 [0m(+0.00001)
     | > avg_loss:[91m 3.31260 [0m(+0.01924)
     | > avg_align_error:[92m 0.99049 [0m(-0.00013)


[4m[1m > EPOCH: 78/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 19:19:17) [0m

[1m   --> STEP: 11/87 -- GLOBAL_STEP: 276875[0m
     | > decoder_loss: 1.80126  (1.77158)
     | > postnet_loss: 1.44456  (1.42253)
     | > stopnet_loss: 1.13000  (1.55708)
     | > decoder_coarse_loss: 1.72408  (1.70855)
     | > decoder_ddc_loss: 0.00133  (0.00123)
     | > ga_loss: 0.00207  (0.00222)
     | > decoder_diff_spec_loss: 0.46465  (0.48194)
     | > postnet_diff_spec_loss: 0.77560  (0.79876)
     | > decoder_ssim_loss: 0.37353  (0.31337)
     | > postnet_ssim_loss: 0.39371  (0.32798)
     | > loss: 2.88501  (3.27466)
     | > align_error: 0.98912  (0.98875)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.91447  (3.85476)
     | > current_lr: 0.00008 
     | > step_time: 3.97370  (4.25329)
     | > loader_time: 0.04100  (0.03170)


[1m   --> STEP: 36/87 -- GLOBAL_STEP: 276900[0m
     | > decoder_loss: 1.76459  (1.81267)
     | > postnet_loss: 1.44804  (1.44818)
     | > stopnet_loss: 1.36039  (1.64888)
     | > decoder_coarse_loss: 1.68721  (1.74810)
     | > decoder_ddc_loss: 0.00272  (0.00124)
     | > ga_loss: 0.00431  (0.00234)
     | > decoder_diff_spec_loss: 0.48186  (0.49357)
     | > postnet_diff_spec_loss: 0.81460  (0.80492)
     | > decoder_ssim_loss: 0.41133  (0.29955)
     | > postnet_ssim_loss: 0.43569  (0.31419)
     | > loss: 3.14345  (3.39120)
     | > align_error: 0.97732  (0.98851)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 13.46030  (4.22248)
     | > current_lr: 0.00008 
     | > step_time: 1.41490  (3.85221)
     | > loader_time: 0.01210  (0.02789)


[1m   --> STEP: 61/87 -- GLOBAL_STEP: 276925[0m
     | > decoder_loss: 1.61962  (1.80137)
     | > postnet_loss: 1.33730  (1.44310)
     | > stopnet_loss: 1.68338  (1.63641)
     | > decoder_coarse_loss: 1.59225  (1.74020)
     | > decoder_ddc_loss: 0.00147  (0.00125)
     | > ga_loss: 0.00257  (0.00231)
     | > decoder_diff_spec_loss: 0.44281  (0.49181)
     | > postnet_diff_spec_loss: 0.77318  (0.80428)
     | > decoder_ssim_loss: 0.28949  (0.29893)
     | > postnet_ssim_loss: 0.30468  (0.31413)
     | > loss: 3.28642  (3.37173)
     | > align_error: 0.98691  (0.98858)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.91924  (4.12012)
     | > current_lr: 0.00008 
     | > step_time: 3.31530  (3.78806)
     | > loader_time: 0.05840  (0.02820)


[1m   --> STEP: 86/87 -- GLOBAL_STEP: 276950[0m
     | > decoder_loss: 1.83449  (1.80336)
     | > postnet_loss: 1.41981  (1.43826)
     | > stopnet_loss: 0.73065  (1.64767)
     | > decoder_coarse_loss: 1.75972  (1.74077)
     | > decoder_ddc_loss: 0.00187  (0.00120)
     | > ga_loss: 0.00240  (0.00224)
     | > decoder_diff_spec_loss: 0.49486  (0.49304)
     | > postnet_diff_spec_loss: 0.79497  (0.80416)
     | > decoder_ssim_loss: 0.52365  (0.29621)
     | > postnet_ssim_loss: 0.54852  (0.31121)
     | > loss: 2.58711  (3.38090)
     | > align_error: 0.98596  (0.98901)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.88019  (3.81351)
     | > current_lr: 0.00008 
     | > step_time: 1.76330  (3.69781)
     | > loader_time: 0.01430  (0.02734)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.37390 [0m(-0.15208)
     | > avg_decoder_loss:[92m 1.81177 [0m(-0.01388)
     | > avg_postnet_loss:[91m 2.01473 [0m(+0.07005)
     | > avg_stopnet_loss:[91m 1.46820 [0m(+0.00007)
     | > avg_decoder_coarse_loss:[91m 1.73110 [0m(+0.02647)
     | > avg_decoder_ddc_loss:[92m 0.00098 [0m(-0.00002)
     | > avg_ga_loss:[91m 0.00217 [0m(+0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.43170 [0m(+0.00504)
     | > avg_postnet_diff_spec_loss:[91m 0.79359 [0m(+0.00039)
     | > avg_decoder_ssim_loss:[91m 0.30660 [0m(+0.00021)
     | > avg_postnet_ssim_loss:[91m 0.33294 [0m(+0.00068)
     | > avg_loss:[91m 3.33491 [0m(+0.02232)
     | > avg_align_error:[91m 0.99068 [0m(+0.00018)


[4m[1m > EPOCH: 79/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 19:26:26) [0m

[1m   --> STEP: 23/87 -- GLOBAL_STEP: 276975[0m
     | > decoder_loss: 1.84107  (1.79190)
     | > postnet_loss: 1.48704  (1.43710)
     | > stopnet_loss: 0.98325  (1.65258)
     | > decoder_coarse_loss: 1.77717  (1.72705)
     | > decoder_ddc_loss: 0.00147  (0.00124)
     | > ga_loss: 0.00227  (0.00225)
     | > decoder_diff_spec_loss: 0.47291  (0.48489)
     | > postnet_diff_spec_loss: 0.79769  (0.79938)
     | > decoder_ssim_loss: 0.42716  (0.29950)
     | > postnet_ssim_loss: 0.45028  (0.31381)
     | > loss: 2.80832  (3.37754)
     | > align_error: 0.98829  (0.98868)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.22283  (3.91938)
     | > current_lr: 0.00008 
     | > step_time: 2.85930  (4.02701)
     | > loader_time: 0.01800  (0.03534)


[1m   --> STEP: 48/87 -- GLOBAL_STEP: 277000[0m
     | > decoder_loss: 1.87054  (1.81027)
     | > postnet_loss: 1.52318  (1.44646)
     | > stopnet_loss: 2.24984  (1.63811)
     | > decoder_coarse_loss: 1.82976  (1.74459)
     | > decoder_ddc_loss: 0.00119  (0.00129)
     | > ga_loss: 0.00240  (0.00233)
     | > decoder_diff_spec_loss: 0.53038  (0.49194)
     | > postnet_diff_spec_loss: 0.83994  (0.80469)
     | > decoder_ssim_loss: 0.22001  (0.30021)
     | > postnet_ssim_loss: 0.22676  (0.31519)
     | > loss: 4.02231  (3.37840)
     | > align_error: 0.98863  (0.98841)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 5.58883  (3.95179)
     | > current_lr: 0.00008 
     | > step_time: 3.58920  (3.80722)
     | > loader_time: 0.02160  (0.03369)


[1m   --> STEP: 73/87 -- GLOBAL_STEP: 277025[0m
     | > decoder_loss: 1.82682  (1.79632)
     | > postnet_loss: 1.43262  (1.43207)
     | > stopnet_loss: 1.23351  (1.63988)
     | > decoder_coarse_loss: 1.78782  (1.73272)
     | > decoder_ddc_loss: 0.00163  (0.00127)
     | > ga_loss: 0.00243  (0.00226)
     | > decoder_diff_spec_loss: 0.49281  (0.49098)
     | > postnet_diff_spec_loss: 0.80181  (0.80319)
     | > decoder_ssim_loss: 0.37378  (0.29686)
     | > postnet_ssim_loss: 0.38343  (0.31178)
     | > loss: 3.02083  (3.36749)
     | > align_error: 0.98590  (0.98867)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.98110  (3.72652)
     | > current_lr: 0.00008 
     | > step_time: 3.02860  (3.85445)
     | > loader_time: 0.01680  (0.03641)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.35669 [0m(-0.01721)
     | > avg_decoder_loss:[92m 1.77578 [0m(-0.03599)
     | > avg_postnet_loss:[92m 1.83336 [0m(-0.18137)
     | > avg_stopnet_loss:[92m 1.46785 [0m(-0.00035)
     | > avg_decoder_coarse_loss:[92m 1.67509 [0m(-0.05601)
     | > avg_decoder_ddc_loss:[91m 0.00099 [0m(+0.00001)
     | > avg_ga_loss:[92m 0.00217 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[92m 0.42867 [0m(-0.00303)
     | > avg_postnet_diff_spec_loss:[92m 0.79301 [0m(-0.00058)
     | > avg_decoder_ssim_loss:[92m 0.30600 [0m(-0.00060)
     | > avg_postnet_ssim_loss:[92m 0.33060 [0m(-0.00234)
     | > avg_loss:[92m 3.26457 [0m(-0.07035)
     | > avg_align_error:[92m 0.99061 [0m(-0.00007)


[4m[1m > EPOCH: 80/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 19:33:37) [0m

[1m   --> STEP: 10/87 -- GLOBAL_STEP: 277050[0m
     | > decoder_loss: 1.71010  (1.76518)
     | > postnet_loss: 1.38873  (1.42360)
     | > stopnet_loss: 1.95087  (1.59362)
     | > decoder_coarse_loss: 1.64730  (1.70372)
     | > decoder_ddc_loss: 0.00115  (0.00121)
     | > ga_loss: 0.00226  (0.00223)
     | > decoder_diff_spec_loss: 0.47909  (0.48422)
     | > postnet_diff_spec_loss: 0.77719  (0.80074)
     | > decoder_ssim_loss: 0.24292  (0.30706)
     | > postnet_ssim_loss: 0.25295  (0.32120)
     | > loss: 3.58700  (3.30649)
     | > align_error: 0.98843  (0.98875)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.85885  (4.03849)
     | > current_lr: 0.00008 
     | > step_time: 5.03870  (4.30126)
     | > loader_time: 0.07040  (0.03469)


[1m   --> STEP: 35/87 -- GLOBAL_STEP: 277075[0m
     | > decoder_loss: 1.96385  (1.81204)
     | > postnet_loss: 1.62895  (1.45204)
     | > stopnet_loss: 1.46585  (1.64527)
     | > decoder_coarse_loss: 1.89967  (1.74179)
     | > decoder_ddc_loss: 0.00127  (0.00120)
     | > ga_loss: 0.00225  (0.00227)
     | > decoder_diff_spec_loss: 0.55163  (0.49311)
     | > postnet_diff_spec_loss: 0.86555  (0.80457)
     | > decoder_ssim_loss: 0.31129  (0.29590)
     | > postnet_ssim_loss: 0.32689  (0.31046)
     | > loss: 3.36437  (3.38442)
     | > align_error: 0.98789  (0.98880)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.17250  (4.07324)
     | > current_lr: 0.00008 
     | > step_time: 3.46710  (3.85730)
     | > loader_time: 0.01750  (0.03890)


[1m   --> STEP: 60/87 -- GLOBAL_STEP: 277100[0m
     | > decoder_loss: 1.84569  (1.80524)
     | > postnet_loss: 1.40729  (1.44579)
     | > stopnet_loss: 1.53555  (1.62721)
     | > decoder_coarse_loss: 1.79051  (1.73776)
     | > decoder_ddc_loss: 0.00108  (0.00127)
     | > ga_loss: 0.00178  (0.00229)
     | > decoder_diff_spec_loss: 0.51822  (0.49246)
     | > postnet_diff_spec_loss: 0.81362  (0.80449)
     | > decoder_ssim_loss: 0.28787  (0.29861)
     | > postnet_ssim_loss: 0.30272  (0.31402)
     | > loss: 3.28619  (3.36355)
     | > align_error: 0.99016  (0.98848)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.89428  (4.21107)
     | > current_lr: 0.00008 
     | > step_time: 5.10170  (3.76167)
     | > loader_time: 0.02230  (0.03643)


[1m   --> STEP: 85/87 -- GLOBAL_STEP: 277125[0m
     | > decoder_loss: 1.88871  (1.80364)
     | > postnet_loss: 1.45900  (1.43964)
     | > stopnet_loss: 1.80825  (1.65103)
     | > decoder_coarse_loss: 1.79736  (1.73642)
     | > decoder_ddc_loss: 0.00099  (0.00122)
     | > ga_loss: 0.00185  (0.00222)
     | > decoder_diff_spec_loss: 0.50651  (0.49328)
     | > postnet_diff_spec_loss: 0.79222  (0.80410)
     | > decoder_ssim_loss: 0.25469  (0.29314)
     | > postnet_ssim_loss: 0.26858  (0.30820)
     | > loss: 3.55954  (3.38203)
     | > align_error: 0.98983  (0.98890)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.25165  (3.86649)
     | > current_lr: 0.00007 
     | > step_time: 2.96380  (3.68171)
     | > loader_time: 0.01730  (0.03678)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.29752 [0m(-0.05918)
     | > avg_decoder_loss:[91m 1.78528 [0m(+0.00950)
     | > avg_postnet_loss:[91m 1.83773 [0m(+0.00437)
     | > avg_stopnet_loss:[92m 1.46675 [0m(-0.00110)
     | > avg_decoder_coarse_loss:[91m 1.70523 [0m(+0.03014)
     | > avg_decoder_ddc_loss:[91m 0.00103 [0m(+0.00004)
     | > avg_ga_loss:[92m 0.00216 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.42857 [0m(-0.00010)
     | > avg_postnet_diff_spec_loss:[91m 0.79313 [0m(+0.00012)
     | > avg_decoder_ssim_loss:[92m 0.30597 [0m(-0.00003)
     | > avg_postnet_ssim_loss:[92m 0.33052 [0m(-0.00007)
     | > avg_loss:[91m 3.27441 [0m(+0.00984)
     | > avg_align_error:[92m 0.99049 [0m(-0.00012)


[4m[1m > EPOCH: 81/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 19:40:40) [0m

[1m   --> STEP: 22/87 -- GLOBAL_STEP: 277150[0m
     | > decoder_loss: 1.51631  (1.77627)
     | > postnet_loss: 1.23124  (1.43125)
     | > stopnet_loss: 1.72878  (1.68516)
     | > decoder_coarse_loss: 1.47637  (1.72578)
     | > decoder_ddc_loss: 0.00082  (0.00126)
     | > ga_loss: 0.00189  (0.00223)
     | > decoder_diff_spec_loss: 0.43592  (0.48474)
     | > postnet_diff_spec_loss: 0.74888  (0.79907)
     | > decoder_ssim_loss: 0.26442  (0.29304)
     | > postnet_ssim_loss: 0.27947  (0.30738)
     | > loss: 3.22657  (3.40100)
     | > align_error: 0.99181  (0.98852)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.98872  (4.33128)
     | > current_lr: 0.00007 
     | > step_time: 4.75720  (3.98985)
     | > loader_time: 0.05170  (0.04084)


[1m   --> STEP: 47/87 -- GLOBAL_STEP: 277175[0m
     | > decoder_loss: 1.68574  (1.80352)
     | > postnet_loss: 1.38481  (1.44556)
     | > stopnet_loss: 1.59655  (1.62066)
     | > decoder_coarse_loss: 1.68868  (1.74492)
     | > decoder_ddc_loss: 0.00157  (0.00130)
     | > ga_loss: 0.00237  (0.00230)
     | > decoder_diff_spec_loss: 0.46033  (0.49130)
     | > postnet_diff_spec_loss: 0.77546  (0.80330)
     | > decoder_ssim_loss: 0.29178  (0.30146)
     | > postnet_ssim_loss: 0.30875  (0.31669)
     | > loss: 3.25771  (3.35918)
     | > align_error: 0.98573  (0.98830)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.04122  (4.38342)
     | > current_lr: 0.00007 
     | > step_time: 2.91640  (3.74058)
     | > loader_time: 0.01950  (0.03942)


[1m   --> STEP: 72/87 -- GLOBAL_STEP: 277200[0m
     | > decoder_loss: 1.70105  (1.79076)
     | > postnet_loss: 1.26449  (1.43753)
     | > stopnet_loss: 1.76555  (1.63971)
     | > decoder_coarse_loss: 1.63843  (1.73214)
     | > decoder_ddc_loss: 0.00096  (0.00126)
     | > ga_loss: 0.00179  (0.00224)
     | > decoder_diff_spec_loss: 0.47399  (0.49053)
     | > postnet_diff_spec_loss: 0.77109  (0.80306)
     | > decoder_ssim_loss: 0.26757  (0.29529)
     | > postnet_ssim_loss: 0.28828  (0.31069)
     | > loss: 3.37597  (3.36624)
     | > align_error: 0.99097  (0.98862)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.00540  (4.09451)
     | > current_lr: 0.00007 
     | > step_time: 3.87030  (3.76296)
     | > loader_time: 0.01920  (0.03720)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.60620 [0m(+0.30868)
     | > avg_decoder_loss:[92m 1.78094 [0m(-0.00434)
     | > avg_postnet_loss:[91m 1.91209 [0m(+0.07436)
     | > avg_stopnet_loss:[92m 1.46434 [0m(-0.00240)
     | > avg_decoder_coarse_loss:[92m 1.68349 [0m(-0.02174)
     | > avg_decoder_ddc_loss:[91m 0.00103 [0m(+0.00000)
     | > avg_ga_loss:[91m 0.00216 [0m(+0.00000)
     | > avg_decoder_diff_spec_loss:[92m 0.42764 [0m(-0.00093)
     | > avg_postnet_diff_spec_loss:[91m 0.79370 [0m(+0.00057)
     | > avg_decoder_ssim_loss:[92m 0.30545 [0m(-0.00052)
     | > avg_postnet_ssim_loss:[91m 0.33221 [0m(+0.00169)
     | > avg_loss:[91m 3.28429 [0m(+0.00989)
     | > avg_align_error:[92m 0.99044 [0m(-0.00004)


[4m[1m > EPOCH: 82/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 19:47:47) [0m

[1m   --> STEP: 9/87 -- GLOBAL_STEP: 277225[0m
     | > decoder_loss: 1.76265  (1.77078)
     | > postnet_loss: 1.31666  (1.41012)
     | > stopnet_loss: 1.96638  (1.56206)
     | > decoder_coarse_loss: 1.69290  (1.70324)
     | > decoder_ddc_loss: 0.00092  (0.00130)
     | > ga_loss: 0.00199  (0.00220)
     | > decoder_diff_spec_loss: 0.49337  (0.48322)
     | > postnet_diff_spec_loss: 0.78377  (0.80246)
     | > decoder_ssim_loss: 0.23926  (0.31371)
     | > postnet_ssim_loss: 0.24757  (0.32899)
     | > loss: 3.61062  (3.27653)
     | > align_error: 0.99063  (0.98853)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.94681  (4.07219)
     | > current_lr: 0.00007 
     | > step_time: 4.75260  (4.45357)
     | > loader_time: 0.02070  (0.03831)


[1m   --> STEP: 34/87 -- GLOBAL_STEP: 277250[0m
     | > decoder_loss: 1.71170  (1.79766)
     | > postnet_loss: 1.32845  (1.43117)
     | > stopnet_loss: 1.90201  (1.66700)
     | > decoder_coarse_loss: 1.64217  (1.74123)
     | > decoder_ddc_loss: 0.00162  (0.00124)
     | > ga_loss: 0.00323  (0.00226)
     | > decoder_diff_spec_loss: 0.48119  (0.49083)
     | > postnet_diff_spec_loss: 0.79013  (0.80207)
     | > decoder_ssim_loss: 0.26227  (0.29507)
     | > postnet_ssim_loss: 0.27305  (0.30979)
     | > loss: 3.54083  (3.39556)
     | > align_error: 0.98497  (0.98864)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 6.31797  (4.10023)
     | > current_lr: 0.00007 
     | > step_time: 3.40050  (3.94350)
     | > loader_time: 0.02710  (0.03720)


[1m   --> STEP: 59/87 -- GLOBAL_STEP: 277275[0m
     | > decoder_loss: 1.76977  (1.79327)
     | > postnet_loss: 1.49187  (1.43448)
     | > stopnet_loss: 2.06237  (1.63460)
     | > decoder_coarse_loss: 1.73070  (1.73866)
     | > decoder_ddc_loss: 0.00143  (0.00129)
     | > ga_loss: 0.00276  (0.00229)
     | > decoder_diff_spec_loss: 0.49022  (0.49053)
     | > postnet_diff_spec_loss: 0.81546  (0.80362)
     | > decoder_ssim_loss: 0.23773  (0.29841)
     | > postnet_ssim_loss: 0.25091  (0.31386)
     | > loss: 3.77319  (3.36458)
     | > align_error: 0.98671  (0.98837)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 6.65356  (4.25007)
     | > current_lr: 0.00007 
     | > step_time: 3.09210  (3.83402)
     | > loader_time: 0.01550  (0.03546)


[1m   --> STEP: 84/87 -- GLOBAL_STEP: 277300[0m
     | > decoder_loss: 1.76269  (1.79192)
     | > postnet_loss: 1.35898  (1.43136)
     | > stopnet_loss: 1.23866  (1.65316)
     | > decoder_coarse_loss: 1.68933  (1.73602)
     | > decoder_ddc_loss: 0.00135  (0.00123)
     | > ga_loss: 0.00215  (0.00222)
     | > decoder_diff_spec_loss: 0.48782  (0.49165)
     | > postnet_diff_spec_loss: 0.78361  (0.80361)
     | > decoder_ssim_loss: 0.35875  (0.29324)
     | > postnet_ssim_loss: 0.37645  (0.30839)
     | > loss: 2.95415  (3.37859)
     | > align_error: 0.98848  (0.98883)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.33939  (3.98399)
     | > current_lr: 0.00007 
     | > step_time: 2.55100  (3.78941)
     | > loader_time: 0.01490  (0.03410)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.54504 [0m(-0.06116)
     | > avg_decoder_loss:[92m 1.77427 [0m(-0.00668)
     | > avg_postnet_loss:[91m 1.92110 [0m(+0.00900)
     | > avg_stopnet_loss:[92m 1.46408 [0m(-0.00027)
     | > avg_decoder_coarse_loss:[92m 1.64227 [0m(-0.04123)
     | > avg_decoder_ddc_loss:[92m 0.00101 [0m(-0.00002)
     | > avg_ga_loss:[92m 0.00216 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.42941 [0m(+0.00177)
     | > avg_postnet_diff_spec_loss:[92m 0.79339 [0m(-0.00031)
     | > avg_decoder_ssim_loss:[92m 0.30542 [0m(-0.00003)
     | > avg_postnet_ssim_loss:[92m 0.33209 [0m(-0.00013)
     | > avg_loss:[92m 3.27462 [0m(-0.00967)
     | > avg_align_error:[91m 0.99056 [0m(+0.00011)


[4m[1m > EPOCH: 83/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 19:55:02) [0m

[1m   --> STEP: 21/87 -- GLOBAL_STEP: 277325[0m
     | > decoder_loss: 1.83374  (1.79065)
     | > postnet_loss: 1.44574  (1.43287)
     | > stopnet_loss: 1.14928  (1.67908)
     | > decoder_coarse_loss: 1.74699  (1.72663)
     | > decoder_ddc_loss: 0.00190  (0.00124)
     | > ga_loss: 0.00267  (0.00226)
     | > decoder_diff_spec_loss: 0.49065  (0.48612)
     | > postnet_diff_spec_loss: 0.81042  (0.80125)
     | > decoder_ssim_loss: 0.38467  (0.29420)
     | > postnet_ssim_loss: 0.40362  (0.30845)
     | > loss: 2.94204  (3.40072)
     | > align_error: 0.98371  (0.98844)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.62629  (4.27459)
     | > current_lr: 0.00007 
     | > step_time: 2.66810  (3.89485)
     | > loader_time: 0.01460  (0.03201)


[1m   --> STEP: 46/87 -- GLOBAL_STEP: 277350[0m
     | > decoder_loss: 1.68953  (1.79799)
     | > postnet_loss: 1.39503  (1.43551)
     | > stopnet_loss: 1.48502  (1.61609)
     | > decoder_coarse_loss: 1.65268  (1.73700)
     | > decoder_ddc_loss: 0.00090  (0.00126)
     | > ga_loss: 0.00174  (0.00231)
     | > decoder_diff_spec_loss: 0.47911  (0.49085)
     | > postnet_diff_spec_loss: 0.79678  (0.80372)
     | > decoder_ssim_loss: 0.29731  (0.30124)
     | > postnet_ssim_loss: 0.31752  (0.31674)
     | > loss: 3.15092  (3.34872)
     | > align_error: 0.99167  (0.98837)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.61283  (4.34738)
     | > current_lr: 0.00007 
     | > step_time: 3.94810  (3.74362)
     | > loader_time: 0.06740  (0.03453)


[1m   --> STEP: 71/87 -- GLOBAL_STEP: 277375[0m
     | > decoder_loss: 1.64176  (1.78627)
     | > postnet_loss: 1.42437  (1.42979)
     | > stopnet_loss: 1.88367  (1.63678)
     | > decoder_coarse_loss: 1.61431  (1.72872)
     | > decoder_ddc_loss: 0.00112  (0.00123)
     | > ga_loss: 0.00217  (0.00225)
     | > decoder_diff_spec_loss: 0.45569  (0.48990)
     | > postnet_diff_spec_loss: 0.79579  (0.80312)
     | > decoder_ssim_loss: 0.25259  (0.29538)
     | > postnet_ssim_loss: 0.27210  (0.31075)
     | > loss: 3.50897  (3.35933)
     | > align_error: 0.98937  (0.98859)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.28929  (4.14818)
     | > current_lr: 0.00007 
     | > step_time: 3.03910  (3.75298)
     | > loader_time: 0.02460  (0.03258)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.26249 [0m(-0.28255)
     | > avg_decoder_loss:[91m 1.83497 [0m(+0.06070)
     | > avg_postnet_loss:[91m 1.99967 [0m(+0.07857)
     | > avg_stopnet_loss:[92m 1.46310 [0m(-0.00098)
     | > avg_decoder_coarse_loss:[91m 1.72027 [0m(+0.07800)
     | > avg_decoder_ddc_loss:[92m 0.00100 [0m(-0.00001)
     | > avg_ga_loss:[91m 0.00216 [0m(+0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.42964 [0m(+0.00024)
     | > avg_postnet_diff_spec_loss:[91m 0.79368 [0m(+0.00029)
     | > avg_decoder_ssim_loss:[91m 0.30549 [0m(+0.00008)
     | > avg_postnet_ssim_loss:[91m 0.33257 [0m(+0.00048)
     | > avg_loss:[91m 3.32824 [0m(+0.05362)
     | > avg_align_error:[92m 0.99053 [0m(-0.00003)


[4m[1m > EPOCH: 84/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 20:02:07) [0m

[1m   --> STEP: 8/87 -- GLOBAL_STEP: 277400[0m
     | > decoder_loss: 1.56741  (1.77352)
     | > postnet_loss: 1.35552  (1.41648)
     | > stopnet_loss: 2.38148  (1.48673)
     | > decoder_coarse_loss: 1.53415  (1.70700)
     | > decoder_ddc_loss: 0.00117  (0.00127)
     | > ga_loss: 0.00264  (0.00222)
     | > decoder_diff_spec_loss: 0.43592  (0.48389)
     | > postnet_diff_spec_loss: 0.78863  (0.80412)
     | > decoder_ssim_loss: 0.21179  (0.32291)
     | > postnet_ssim_loss: 0.21985  (0.33739)
     | > loss: 3.92330  (3.20947)
     | > align_error: 0.98829  (0.98828)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 7.48159  (4.46808)
     | > current_lr: 0.00007 
     | > step_time: 3.59060  (4.29328)
     | > loader_time: 0.01920  (0.03078)


[1m   --> STEP: 33/87 -- GLOBAL_STEP: 277425[0m
     | > decoder_loss: 1.79750  (1.79581)
     | > postnet_loss: 1.51010  (1.42689)
     | > stopnet_loss: 1.40725  (1.63839)
     | > decoder_coarse_loss: 1.69457  (1.73072)
     | > decoder_ddc_loss: 0.00114  (0.00120)
     | > ga_loss: 0.00208  (0.00222)
     | > decoder_diff_spec_loss: 0.48524  (0.49089)
     | > postnet_diff_spec_loss: 0.82463  (0.80205)
     | > decoder_ssim_loss: 0.31856  (0.29581)
     | > postnet_ssim_loss: 0.33736  (0.31030)
     | > loss: 3.15995  (3.36291)
     | > align_error: 0.98948  (0.98874)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.11017  (3.87170)
     | > current_lr: 0.00007 
     | > step_time: 3.94020  (3.89897)
     | > loader_time: 0.02130  (0.02765)


[1m   --> STEP: 58/87 -- GLOBAL_STEP: 277450[0m
     | > decoder_loss: 1.80561  (1.79488)
     | > postnet_loss: 1.40369  (1.43651)
     | > stopnet_loss: 1.51627  (1.61603)
     | > decoder_coarse_loss: 1.77039  (1.72645)
     | > decoder_ddc_loss: 0.00097  (0.00126)
     | > ga_loss: 0.00176  (0.00227)
     | > decoder_diff_spec_loss: 0.52011  (0.49051)
     | > postnet_diff_spec_loss: 0.81522  (0.80348)
     | > decoder_ssim_loss: 0.29607  (0.29921)
     | > postnet_ssim_loss: 0.31195  (0.31477)
     | > loss: 3.25609  (3.34414)
     | > align_error: 0.99101  (0.98839)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.27076  (4.05211)
     | > current_lr: 0.00007 
     | > step_time: 4.69120  (3.72398)
     | > loader_time: 0.01660  (0.02954)


[1m   --> STEP: 83/87 -- GLOBAL_STEP: 277475[0m
     | > decoder_loss: 1.76161  (1.79327)
     | > postnet_loss: 1.35686  (1.43308)
     | > stopnet_loss: 2.13518  (1.65051)
     | > decoder_coarse_loss: 1.71111  (1.72663)
     | > decoder_ddc_loss: 0.00096  (0.00121)
     | > ga_loss: 0.00192  (0.00221)
     | > decoder_diff_spec_loss: 0.47790  (0.49126)
     | > postnet_diff_spec_loss: 0.78357  (0.80380)
     | > decoder_ssim_loss: 0.22504  (0.29217)
     | > postnet_ssim_loss: 0.23928  (0.30741)
     | > loss: 3.78387  (3.37374)
     | > align_error: 0.99012  (0.98879)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.57636  (3.87042)
     | > current_lr: 0.00007 
     | > step_time: 3.03660  (3.72342)
     | > loader_time: 0.01760  (0.03154)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.33053 [0m(+0.06804)
     | > avg_decoder_loss:[92m 1.82468 [0m(-0.01029)
     | > avg_postnet_loss:[92m 1.88878 [0m(-0.11089)
     | > avg_stopnet_loss:[91m 1.46353 [0m(+0.00042)
     | > avg_decoder_coarse_loss:[91m 1.73103 [0m(+0.01076)
     | > avg_decoder_ddc_loss:[91m 0.00103 [0m(+0.00003)
     | > avg_ga_loss:[92m 0.00215 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.42818 [0m(-0.00146)
     | > avg_postnet_diff_spec_loss:[92m 0.79333 [0m(-0.00035)
     | > avg_decoder_ssim_loss:[92m 0.30510 [0m(-0.00040)
     | > avg_postnet_ssim_loss:[92m 0.33139 [0m(-0.00118)
     | > avg_loss:[92m 3.30018 [0m(-0.02806)
     | > avg_align_error:[92m 0.99034 [0m(-0.00019)


[4m[1m > EPOCH: 85/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 20:09:15) [0m

[1m   --> STEP: 20/87 -- GLOBAL_STEP: 277500[0m
     | > decoder_loss: 1.60957  (1.77584)
     | > postnet_loss: 1.29432  (1.42239)
     | > stopnet_loss: 2.11759  (1.69869)
     | > decoder_coarse_loss: 1.59033  (1.71917)
     | > decoder_ddc_loss: 0.00124  (0.00127)
     | > ga_loss: 0.00259  (0.00220)
     | > decoder_diff_spec_loss: 0.45670  (0.48503)
     | > postnet_diff_spec_loss: 0.77684  (0.80105)
     | > decoder_ssim_loss: 0.22778  (0.28923)
     | > postnet_ssim_loss: 0.24177  (0.30367)
     | > loss: 3.68017  (3.40911)
     | > align_error: 0.98743  (0.98838)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.22945  (3.95062)
     | > current_lr: 0.00007 
     | > step_time: 3.51840  (3.99841)
     | > loader_time: 0.04810  (0.03777)


[1m   --> STEP: 45/87 -- GLOBAL_STEP: 277525[0m
     | > decoder_loss: 1.81925  (1.79190)
     | > postnet_loss: 1.44899  (1.42970)
     | > stopnet_loss: 1.94698  (1.61734)
     | > decoder_coarse_loss: 1.74306  (1.72817)
     | > decoder_ddc_loss: 0.00113  (0.00130)
     | > ga_loss: 0.00226  (0.00230)
     | > decoder_diff_spec_loss: 0.49139  (0.49016)
     | > postnet_diff_spec_loss: 0.80279  (0.80370)
     | > decoder_ssim_loss: 0.24328  (0.30081)
     | > postnet_ssim_loss: 0.25411  (0.31641)
     | > loss: 3.65928  (3.34439)
     | > align_error: 0.98851  (0.98812)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.73916  (4.29825)
     | > current_lr: 0.00007 
     | > step_time: 3.52870  (3.79234)
     | > loader_time: 0.02550  (0.03552)


[1m   --> STEP: 70/87 -- GLOBAL_STEP: 277550[0m
     | > decoder_loss: 1.88709  (1.78272)
     | > postnet_loss: 1.34503  (1.42489)
     | > stopnet_loss: 1.64291  (1.62859)
     | > decoder_coarse_loss: 1.87444  (1.72144)
     | > decoder_ddc_loss: 0.00101  (0.00127)
     | > ga_loss: 0.00183  (0.00224)
     | > decoder_diff_spec_loss: 0.53099  (0.48940)
     | > postnet_diff_spec_loss: 0.80133  (0.80288)
     | > decoder_ssim_loss: 0.27712  (0.29542)
     | > postnet_ssim_loss: 0.29442  (0.31100)
     | > loss: 3.40493  (3.34702)
     | > align_error: 0.99101  (0.98845)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.58091  (4.08347)
     | > current_lr: 0.00007 
     | > step_time: 4.46940  (3.81167)
     | > loader_time: 0.02300  (0.04008)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.32632 [0m(-0.00421)
     | > avg_decoder_loss:[92m 1.81235 [0m(-0.01233)
     | > avg_postnet_loss:[91m 1.92875 [0m(+0.03997)
     | > avg_stopnet_loss:[92m 1.46339 [0m(-0.00014)
     | > avg_decoder_coarse_loss:[91m 1.75659 [0m(+0.02557)
     | > avg_decoder_ddc_loss:[91m 0.00104 [0m(+0.00001)
     | > avg_ga_loss:[92m 0.00215 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.42764 [0m(-0.00054)
     | > avg_postnet_diff_spec_loss:[92m 0.79307 [0m(-0.00026)
     | > avg_decoder_ssim_loss:[92m 0.30506 [0m(-0.00004)
     | > avg_postnet_ssim_loss:[91m 0.33204 [0m(+0.00066)
     | > avg_loss:[91m 3.31327 [0m(+0.01309)
     | > avg_align_error:[91m 0.99044 [0m(+0.00010)


[4m[1m > EPOCH: 86/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 20:16:23) [0m

[1m   --> STEP: 7/87 -- GLOBAL_STEP: 277575[0m
     | > decoder_loss: 1.83021  (1.78723)
     | > postnet_loss: 1.43336  (1.42929)
     | > stopnet_loss: 1.55454  (1.36389)
     | > decoder_coarse_loss: 1.75530  (1.72080)
     | > decoder_ddc_loss: 0.00107  (0.00131)
     | > ga_loss: 0.00217  (0.00217)
     | > decoder_diff_spec_loss: 0.47730  (0.48803)
     | > postnet_diff_spec_loss: 0.78271  (0.80643)
     | > decoder_ssim_loss: 0.30847  (0.33789)
     | > postnet_ssim_loss: 0.31676  (0.35470)
     | > loss: 3.29171  (3.10614)
     | > align_error: 0.98969  (0.98816)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.30093  (4.14398)
     | > current_lr: 0.00007 
     | > step_time: 3.05640  (4.38963)
     | > loader_time: 0.01620  (0.02985)


[1m   --> STEP: 32/87 -- GLOBAL_STEP: 277600[0m
     | > decoder_loss: 1.54713  (1.78686)
     | > postnet_loss: 1.39187  (1.42627)
     | > stopnet_loss: 1.81387  (1.64740)
     | > decoder_coarse_loss: 1.49899  (1.73052)
     | > decoder_ddc_loss: 0.00124  (0.00121)
     | > ga_loss: 0.00254  (0.00222)
     | > decoder_diff_spec_loss: 0.43845  (0.48979)
     | > postnet_diff_spec_loss: 0.78851  (0.80115)
     | > decoder_ssim_loss: 0.26315  (0.29450)
     | > postnet_ssim_loss: 0.28095  (0.30942)
     | > loss: 3.37915  (3.36844)
     | > align_error: 0.98779  (0.98860)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.18558  (4.00981)
     | > current_lr: 0.00007 
     | > step_time: 3.11630  (3.99336)
     | > loader_time: 0.06020  (0.04380)


[1m   --> STEP: 57/87 -- GLOBAL_STEP: 277625[0m
     | > decoder_loss: 1.99059  (1.78312)
     | > postnet_loss: 1.56410  (1.43051)
     | > stopnet_loss: 1.53380  (1.61810)
     | > decoder_coarse_loss: 1.93173  (1.72208)
     | > decoder_ddc_loss: 0.00131  (0.00128)
     | > ga_loss: 0.00251  (0.00227)
     | > decoder_diff_spec_loss: 0.56242  (0.48958)
     | > postnet_diff_spec_loss: 0.84517  (0.80266)
     | > decoder_ssim_loss: 0.32069  (0.29873)
     | > postnet_ssim_loss: 0.33300  (0.31445)
     | > loss: 3.43359  (3.34007)
     | > align_error: 0.98829  (0.98823)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 7.40973  (4.10732)
     | > current_lr: 0.00007 
     | > step_time: 2.52590  (3.73837)
     | > loader_time: 0.06030  (0.04266)


[1m   --> STEP: 82/87 -- GLOBAL_STEP: 277650[0m
     | > decoder_loss: 1.72739  (1.78264)
     | > postnet_loss: 1.43412  (1.42784)
     | > stopnet_loss: 2.27571  (1.64679)
     | > decoder_coarse_loss: 1.68992  (1.72412)
     | > decoder_ddc_loss: 0.00100  (0.00122)
     | > ga_loss: 0.00187  (0.00220)
     | > decoder_diff_spec_loss: 0.50503  (0.49078)
     | > postnet_diff_spec_loss: 0.82354  (0.80357)
     | > decoder_ssim_loss: 0.21803  (0.29245)
     | > postnet_ssim_loss: 0.22828  (0.30797)
     | > loss: 3.94186  (3.36546)
     | > align_error: 0.99023  (0.98870)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.91753  (3.94886)
     | > current_lr: 0.00007 
     | > step_time: 3.27480  (3.74420)
     | > loader_time: 0.01950  (0.03846)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.44568 [0m(+0.11935)
     | > avg_decoder_loss:[92m 1.81011 [0m(-0.00224)
     | > avg_postnet_loss:[91m 2.05740 [0m(+0.12865)
     | > avg_stopnet_loss:[92m 1.46224 [0m(-0.00114)
     | > avg_decoder_coarse_loss:[91m 1.81449 [0m(+0.05790)
     | > avg_decoder_ddc_loss:[92m 0.00103 [0m(-0.00001)
     | > avg_ga_loss:[92m 0.00214 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.42667 [0m(-0.00097)
     | > avg_postnet_diff_spec_loss:[91m 0.79395 [0m(+0.00088)
     | > avg_decoder_ssim_loss:[92m 0.30468 [0m(-0.00038)
     | > avg_postnet_ssim_loss:[91m 0.33356 [0m(+0.00151)
     | > avg_loss:[91m 3.35840 [0m(+0.04513)
     | > avg_align_error:[92m 0.99033 [0m(-0.00011)


[4m[1m > EPOCH: 87/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 20:23:29) [0m

[1m   --> STEP: 19/87 -- GLOBAL_STEP: 277675[0m
     | > decoder_loss: 1.76865  (1.78618)
     | > postnet_loss: 1.48580  (1.43012)
     | > stopnet_loss: 1.23066  (1.67485)
     | > decoder_coarse_loss: 1.77652  (1.72670)
     | > decoder_ddc_loss: 0.00122  (0.00119)
     | > ga_loss: 0.00205  (0.00219)
     | > decoder_diff_spec_loss: 0.48480  (0.48734)
     | > postnet_diff_spec_loss: 0.80104  (0.80161)
     | > decoder_ssim_loss: 0.36886  (0.29225)
     | > postnet_ssim_loss: 0.38823  (0.30667)
     | > loss: 3.00967  (3.39382)
     | > align_error: 0.98906  (0.98857)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 8.36523  (4.41232)
     | > current_lr: 0.00007 
     | > step_time: 3.46560  (4.05853)
     | > loader_time: 0.03030  (0.03638)


[1m   --> STEP: 44/87 -- GLOBAL_STEP: 277700[0m
     | > decoder_loss: 1.67571  (1.78806)
     | > postnet_loss: 1.37493  (1.43446)
     | > stopnet_loss: 1.13390  (1.60924)
     | > decoder_coarse_loss: 1.62751  (1.73284)
     | > decoder_ddc_loss: 0.00099  (0.00127)
     | > ga_loss: 0.00169  (0.00230)
     | > decoder_diff_spec_loss: 0.46157  (0.48994)
     | > postnet_diff_spec_loss: 0.78281  (0.80324)
     | > decoder_ssim_loss: 0.36144  (0.30176)
     | > postnet_ssim_loss: 0.38082  (0.31767)
     | > loss: 2.80881  (3.33803)
     | > align_error: 0.99106  (0.98810)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 6.74882  (4.69698)
     | > current_lr: 0.00007 
     | > step_time: 4.49780  (3.76467)
     | > loader_time: 0.01960  (0.03626)


[1m   --> STEP: 69/87 -- GLOBAL_STEP: 277725[0m
     | > decoder_loss: 1.75853  (1.77632)
     | > postnet_loss: 1.31137  (1.42360)
     | > stopnet_loss: 2.17097  (1.62860)
     | > decoder_coarse_loss: 1.65632  (1.71711)
     | > decoder_ddc_loss: 0.00085  (0.00125)
     | > ga_loss: 0.00187  (0.00224)
     | > decoder_diff_spec_loss: 0.47100  (0.48888)
     | > postnet_diff_spec_loss: 0.78349  (0.80236)
     | > decoder_ssim_loss: 0.22015  (0.29532)
     | > postnet_ssim_loss: 0.23456  (0.31095)
     | > loss: 3.78937  (3.34372)
     | > align_error: 0.99065  (0.98837)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.11132  (4.25633)
     | > current_lr: 0.00007 
     | > step_time: 3.81790  (3.82319)
     | > loader_time: 0.02650  (0.03397)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

warning: audio amplitude out of range, auto clipped.
 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.36696 [0m(-0.07871)
     | > avg_decoder_loss:[92m 1.80536 [0m(-0.00475)
     | > avg_postnet_loss:[92m 1.92313 [0m(-0.13427)
     | > avg_stopnet_loss:[92m 1.46172 [0m(-0.00052)
     | > avg_decoder_coarse_loss:[92m 1.75748 [0m(-0.05701)
     | > avg_decoder_ddc_loss:[92m 0.00103 [0m(-0.00000)
     | > avg_ga_loss:[91m 0.00214 [0m(+0.00001)
     | > avg_decoder_diff_spec_loss:[91m 0.42817 [0m(+0.00150)
     | > avg_postnet_diff_spec_loss:[92m 0.79310 [0m(-0.00085)
     | > avg_decoder_ssim_loss:[92m 0.30448 [0m(-0.00020)
     | > avg_postnet_ssim_loss:[92m 0.33179 [0m(-0.00177)
     | > avg_loss:[92m 3.30858 [0m(-0.04982)
     | > avg_align_error:[91m 0.99036 [0m(+0.00003)


[4m[1m > EPOCH: 88/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 20:30:39) [0m

[1m   --> STEP: 6/87 -- GLOBAL_STEP: 277750[0m
     | > decoder_loss: 1.65664  (1.78544)
     | > postnet_loss: 1.40661  (1.42824)
     | > stopnet_loss: 0.95180  (1.33381)
     | > decoder_coarse_loss: 1.57080  (1.72665)
     | > decoder_ddc_loss: 0.00115  (0.00135)
     | > ga_loss: 0.00154  (0.00215)
     | > decoder_diff_spec_loss: 0.46714  (0.48804)
     | > postnet_diff_spec_loss: 0.78984  (0.80854)
     | > decoder_ssim_loss: 0.40514  (0.34292)
     | > postnet_ssim_loss: 0.42312  (0.35992)
     | > loss: 2.63961  (3.07984)
     | > align_error: 0.99075  (0.98788)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.24175  (4.47752)
     | > current_lr: 0.00007 
     | > step_time: 4.70010  (4.62001)
     | > loader_time: 0.02240  (0.02738)


[1m   --> STEP: 31/87 -- GLOBAL_STEP: 277775[0m
     | > decoder_loss: 1.97338  (1.79424)
     | > postnet_loss: 1.56892  (1.42329)
     | > stopnet_loss: 1.31468  (1.64780)
     | > decoder_coarse_loss: 1.90149  (1.73808)
     | > decoder_ddc_loss: 0.00142  (0.00121)
     | > ga_loss: 0.00243  (0.00219)
     | > decoder_diff_spec_loss: 0.55802  (0.49135)
     | > postnet_diff_spec_loss: 0.85246  (0.80113)
     | > decoder_ssim_loss: 0.33458  (0.29507)
     | > postnet_ssim_loss: 0.34936  (0.30994)
     | > loss: 3.21173  (3.37235)
     | > align_error: 0.98711  (0.98853)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.96836  (4.77055)
     | > current_lr: 0.00007 
     | > step_time: 3.28190  (4.10061)
     | > loader_time: 0.01640  (0.03156)


[1m   --> STEP: 56/87 -- GLOBAL_STEP: 277800[0m
     | > decoder_loss: 1.71913  (1.77615)
     | > postnet_loss: 1.30466  (1.42259)
     | > stopnet_loss: 1.99749  (1.62298)
     | > decoder_coarse_loss: 1.66255  (1.71588)
     | > decoder_ddc_loss: 0.00077  (0.00129)
     | > ga_loss: 0.00172  (0.00225)
     | > decoder_diff_spec_loss: 0.48984  (0.48803)
     | > postnet_diff_spec_loss: 0.79646  (0.80162)
     | > decoder_ssim_loss: 0.24203  (0.29787)
     | > postnet_ssim_loss: 0.25437  (0.31377)
     | > loss: 3.62354  (3.33852)
     | > align_error: 0.99271  (0.98811)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.18875  (4.64283)
     | > current_lr: 0.00007 
     | > step_time: 4.15660  (3.84800)
     | > loader_time: 0.01910  (0.03311)


[1m   --> STEP: 81/87 -- GLOBAL_STEP: 277825[0m
     | > decoder_loss: 1.97385  (1.78240)
     | > postnet_loss: 1.43112  (1.42272)
     | > stopnet_loss: 2.15670  (1.63690)
     | > decoder_coarse_loss: 1.88297  (1.71989)
     | > decoder_ddc_loss: 0.00097  (0.00123)
     | > ga_loss: 0.00214  (0.00219)
     | > decoder_diff_spec_loss: 0.55817  (0.49051)
     | > postnet_diff_spec_loss: 0.84245  (0.80321)
     | > decoder_ssim_loss: 0.22078  (0.29298)
     | > postnet_ssim_loss: 0.22699  (0.30864)
     | > loss: 3.95171  (3.35325)
     | > align_error: 0.98960  (0.98856)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.95522  (4.36991)
     | > current_lr: 0.00007 
     | > step_time: 3.37760  (3.83712)
     | > loader_time: 0.01720  (0.03463)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.35432 [0m(-0.01264)
     | > avg_decoder_loss:[92m 1.75376 [0m(-0.05160)
     | > avg_postnet_loss:[91m 1.96746 [0m(+0.04434)
     | > avg_stopnet_loss:[92m 1.46141 [0m(-0.00032)
     | > avg_decoder_coarse_loss:[92m 1.70589 [0m(-0.05159)
     | > avg_decoder_ddc_loss:[91m 0.00104 [0m(+0.00001)
     | > avg_ga_loss:[92m 0.00213 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.42803 [0m(-0.00014)
     | > avg_postnet_diff_spec_loss:[91m 0.79332 [0m(+0.00023)
     | > avg_decoder_ssim_loss:[92m 0.30424 [0m(-0.00024)
     | > avg_postnet_ssim_loss:[91m 0.33289 [0m(+0.00110)
     | > avg_loss:[92m 3.29372 [0m(-0.01486)
     | > avg_align_error:[92m 0.99027 [0m(-0.00009)


[4m[1m > EPOCH: 89/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 20:37:54) [0m

[1m   --> STEP: 18/87 -- GLOBAL_STEP: 277850[0m
     | > decoder_loss: 1.91453  (1.78614)
     | > postnet_loss: 1.50835  (1.42471)
     | > stopnet_loss: 1.37945  (1.69043)
     | > decoder_coarse_loss: 1.85682  (1.71983)
     | > decoder_ddc_loss: 0.00131  (0.00124)
     | > ga_loss: 0.00200  (0.00216)
     | > decoder_diff_spec_loss: 0.53609  (0.48726)
     | > postnet_diff_spec_loss: 0.84415  (0.80155)
     | > decoder_ssim_loss: 0.31738  (0.28790)
     | > postnet_ssim_loss: 0.32995  (0.30242)
     | > loss: 3.21657  (3.40402)
     | > align_error: 0.98877  (0.98832)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 10.61915  (4.19523)
     | > current_lr: 0.00007 
     | > step_time: 3.04250  (4.04708)
     | > loader_time: 0.07670  (0.04279)


[1m   --> STEP: 43/87 -- GLOBAL_STEP: 277875[0m
     | > decoder_loss: 1.76109  (1.78484)
     | > postnet_loss: 1.39134  (1.42578)
     | > stopnet_loss: 1.54762  (1.61398)
     | > decoder_coarse_loss: 1.71455  (1.72633)
     | > decoder_ddc_loss: 0.00161  (0.00133)
     | > ga_loss: 0.00255  (0.00229)
     | > decoder_diff_spec_loss: 0.47728  (0.49068)
     | > postnet_diff_spec_loss: 0.79502  (0.80373)
     | > decoder_ssim_loss: 0.30428  (0.30007)
     | > postnet_ssim_loss: 0.31866  (0.31623)
     | > loss: 3.25132  (3.33765)
     | > align_error: 0.98657  (0.98784)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.27918  (4.40934)
     | > current_lr: 0.00007 
     | > step_time: 3.15000  (3.72562)
     | > loader_time: 0.01960  (0.03365)


[1m   --> STEP: 68/87 -- GLOBAL_STEP: 277900[0m
     | > decoder_loss: 1.97790  (1.76722)
     | > postnet_loss: 1.51460  (1.41411)
     | > stopnet_loss: 1.54723  (1.61609)
     | > decoder_coarse_loss: 1.90035  (1.70998)
     | > decoder_ddc_loss: 0.00114  (0.00128)
     | > ga_loss: 0.00182  (0.00222)
     | > decoder_diff_spec_loss: 0.55968  (0.48887)
     | > postnet_diff_spec_loss: 0.85027  (0.80259)
     | > decoder_ssim_loss: 0.29580  (0.29605)
     | > postnet_ssim_loss: 0.30780  (0.31201)
     | > loss: 3.40822  (3.32523)
     | > align_error: 0.98875  (0.98822)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.24389  (4.14576)
     | > current_lr: 0.00007 
     | > step_time: 4.98570  (3.78548)
     | > loader_time: 0.05130  (0.03402)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.13227 [0m(-0.22206)
     | > avg_decoder_loss:[92m 1.73292 [0m(-0.02084)
     | > avg_postnet_loss:[92m 1.77616 [0m(-0.19130)
     | > avg_stopnet_loss:[92m 1.45927 [0m(-0.00214)
     | > avg_decoder_coarse_loss:[92m 1.66564 [0m(-0.04025)
     | > avg_decoder_ddc_loss:[92m 0.00103 [0m(-0.00000)
     | > avg_ga_loss:[91m 0.00213 [0m(+0.00000)
     | > avg_decoder_diff_spec_loss:[92m 0.42721 [0m(-0.00082)
     | > avg_postnet_diff_spec_loss:[92m 0.79243 [0m(-0.00089)
     | > avg_decoder_ssim_loss:[92m 0.30389 [0m(-0.00036)
     | > avg_postnet_ssim_loss:[92m 0.33038 [0m(-0.00251)
     | > avg_loss:[92m 3.22735 [0m(-0.06637)
     | > avg_align_error:[92m 0.99025 [0m(-0.00002)


[4m[1m > EPOCH: 90/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 20:45:02) [0m

[1m   --> STEP: 5/87 -- GLOBAL_STEP: 277925[0m
     | > decoder_loss: 1.77738  (1.80457)
     | > postnet_loss: 1.46354  (1.43877)
     | > stopnet_loss: 1.05166  (1.40583)
     | > decoder_coarse_loss: 1.67027  (1.72556)
     | > decoder_ddc_loss: 0.00164  (0.00142)
     | > ga_loss: 0.00212  (0.00225)
     | > decoder_diff_spec_loss: 0.46580  (0.49399)
     | > postnet_diff_spec_loss: 0.79410  (0.81322)
     | > decoder_ssim_loss: 0.39186  (0.32995)
     | > postnet_ssim_loss: 0.41768  (0.34754)
     | > loss: 2.80783  (3.15583)
     | > align_error: 0.98633  (0.98710)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.65130  (4.25961)
     | > current_lr: 0.00007 
     | > step_time: 3.63060  (4.53636)
     | > loader_time: 0.01740  (0.05301)


[1m   --> STEP: 30/87 -- GLOBAL_STEP: 277950[0m
     | > decoder_loss: 1.91105  (1.78075)
     | > postnet_loss: 1.41249  (1.41104)
     | > stopnet_loss: 1.24108  (1.64447)
     | > decoder_coarse_loss: 1.85159  (1.71967)
     | > decoder_ddc_loss: 0.00102  (0.00123)
     | > ga_loss: 0.00166  (0.00217)
     | > decoder_diff_spec_loss: 0.52228  (0.48875)
     | > postnet_diff_spec_loss: 0.79686  (0.79923)
     | > decoder_ssim_loss: 0.33606  (0.29339)
     | > postnet_ssim_loss: 0.36035  (0.30854)
     | > loss: 3.04731  (3.35594)
     | > align_error: 0.99007  (0.98846)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.74595  (4.06176)
     | > current_lr: 0.00007 
     | > step_time: 4.37780  (4.08086)
     | > loader_time: 0.01850  (0.03863)


[1m   --> STEP: 55/87 -- GLOBAL_STEP: 277975[0m
     | > decoder_loss: 1.75727  (1.77205)
     | > postnet_loss: 1.42493  (1.41755)
     | > stopnet_loss: 1.27459  (1.60356)
     | > decoder_coarse_loss: 1.68026  (1.70543)
     | > decoder_ddc_loss: 0.00171  (0.00131)
     | > ga_loss: 0.00258  (0.00224)
     | > decoder_diff_spec_loss: 0.48797  (0.48756)
     | > postnet_diff_spec_loss: 0.80632  (0.80127)
     | > decoder_ssim_loss: 0.35817  (0.29860)
     | > postnet_ssim_loss: 0.37578  (0.31474)
     | > loss: 3.01060  (3.31439)
     | > align_error: 0.98488  (0.98792)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.74749  (4.15970)
     | > current_lr: 0.00007 
     | > step_time: 2.91540  (3.78935)
     | > loader_time: 0.04600  (0.03497)


[1m   --> STEP: 80/87 -- GLOBAL_STEP: 278000[0m
     | > decoder_loss: 1.80582  (1.77144)
     | > postnet_loss: 1.49393  (1.41240)
     | > stopnet_loss: 1.20639  (1.62278)
     | > decoder_coarse_loss: 1.70899  (1.70610)
     | > decoder_ddc_loss: 0.00109  (0.00124)
     | > ga_loss: 0.00187  (0.00218)
     | > decoder_diff_spec_loss: 0.47135  (0.48919)
     | > postnet_diff_spec_loss: 0.79989  (0.80201)
     | > decoder_ssim_loss: 0.35527  (0.29354)
     | > postnet_ssim_loss: 0.37220  (0.30938)
     | > loss: 2.96790  (3.32999)
     | > align_error: 0.99024  (0.98847)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.62098  (3.98375)
     | > current_lr: 0.00007 
     | > step_time: 2.79590  (3.79862)
     | > loader_time: 0.01610  (0.03486)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.50999 [0m(+0.37772)
     | > avg_decoder_loss:[92m 1.72965 [0m(-0.00327)
     | > avg_postnet_loss:[91m 1.84042 [0m(+0.06426)
     | > avg_stopnet_loss:[92m 1.45901 [0m(-0.00025)
     | > avg_decoder_coarse_loss:[91m 1.70488 [0m(+0.03924)
     | > avg_decoder_ddc_loss:[92m 0.00100 [0m(-0.00004)
     | > avg_ga_loss:[91m 0.00214 [0m(+0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.42663 [0m(-0.00058)
     | > avg_postnet_diff_spec_loss:[91m 0.79304 [0m(+0.00061)
     | > avg_decoder_ssim_loss:[92m 0.30369 [0m(-0.00020)
     | > avg_postnet_ssim_loss:[91m 0.33166 [0m(+0.00128)
     | > avg_loss:[91m 3.25245 [0m(+0.02510)
     | > avg_align_error:[91m 0.99033 [0m(+0.00008)


[4m[1m > EPOCH: 91/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 20:52:14) [0m

[1m   --> STEP: 17/87 -- GLOBAL_STEP: 278025[0m
     | > decoder_loss: 1.87904  (1.76999)
     | > postnet_loss: 1.42238  (1.40729)
     | > stopnet_loss: 1.80957  (1.71090)
     | > decoder_coarse_loss: 1.75094  (1.71338)
     | > decoder_ddc_loss: 0.00117  (0.00125)
     | > ga_loss: 0.00239  (0.00217)
     | > decoder_diff_spec_loss: 0.48912  (0.48294)
     | > postnet_diff_spec_loss: 0.79782  (0.79800)
     | > decoder_ssim_loss: 0.26012  (0.28586)
     | > postnet_ssim_loss: 0.27259  (0.29991)
     | > loss: 3.53979  (3.41143)
     | > align_error: 0.98765  (0.98828)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.18788  (4.01369)
     | > current_lr: 0.00007 
     | > step_time: 3.42750  (4.17843)
     | > loader_time: 0.02600  (0.03197)


[1m   --> STEP: 42/87 -- GLOBAL_STEP: 278050[0m
     | > decoder_loss: 1.80208  (1.77957)
     | > postnet_loss: 1.48286  (1.41692)
     | > stopnet_loss: 1.25341  (1.61347)
     | > decoder_coarse_loss: 1.70592  (1.72230)
     | > decoder_ddc_loss: 0.00169  (0.00131)
     | > ga_loss: 0.00253  (0.00228)
     | > decoder_diff_spec_loss: 0.48320  (0.48981)
     | > postnet_diff_spec_loss: 0.81670  (0.80329)
     | > decoder_ssim_loss: 0.35927  (0.29961)
     | > postnet_ssim_loss: 0.38297  (0.31532)
     | > loss: 3.02472  (3.33189)
     | > align_error: 0.98593  (0.98791)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 7.13444  (4.81763)
     | > current_lr: 0.00007 
     | > step_time: 2.34980  (3.81036)
     | > loader_time: 0.02360  (0.03104)


[1m   --> STEP: 67/87 -- GLOBAL_STEP: 278075[0m
     | > decoder_loss: 1.63232  (1.76027)
     | > postnet_loss: 1.33624  (1.40533)
     | > stopnet_loss: 1.42809  (1.61420)
     | > decoder_coarse_loss: 1.56520  (1.70112)
     | > decoder_ddc_loss: 0.00097  (0.00127)
     | > ga_loss: 0.00202  (0.00222)
     | > decoder_diff_spec_loss: 0.45581  (0.48710)
     | > postnet_diff_spec_loss: 0.77877  (0.80140)
     | > decoder_ssim_loss: 0.31693  (0.29566)
     | > postnet_ssim_loss: 0.33178  (0.31147)
     | > loss: 3.04270  (3.31621)
     | > align_error: 0.99117  (0.98820)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.85489  (4.36323)
     | > current_lr: 0.00007 
     | > step_time: 3.45440  (3.76572)
     | > loader_time: 0.01850  (0.03389)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.61120 [0m(+0.10122)
     | > avg_decoder_loss:[92m 1.68078 [0m(-0.04887)
     | > avg_postnet_loss:[92m 1.63016 [0m(-0.21026)
     | > avg_stopnet_loss:[92m 1.45835 [0m(-0.00067)
     | > avg_decoder_coarse_loss:[92m 1.62907 [0m(-0.07581)
     | > avg_decoder_ddc_loss:[91m 0.00102 [0m(+0.00002)
     | > avg_ga_loss:[92m 0.00213 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.42622 [0m(-0.00041)
     | > avg_postnet_diff_spec_loss:[92m 0.79115 [0m(-0.00190)
     | > avg_decoder_ssim_loss:[92m 0.30329 [0m(-0.00040)
     | > avg_postnet_ssim_loss:[92m 0.32791 [0m(-0.00375)
     | > avg_loss:[92m 3.16638 [0m(-0.08607)
     | > avg_align_error:[92m 0.99018 [0m(-0.00015)


[4m[1m > EPOCH: 92/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 20:59:23) [0m

[1m   --> STEP: 4/87 -- GLOBAL_STEP: 278100[0m
     | > decoder_loss: 1.88026  (1.80876)
     | > postnet_loss: 1.46690  (1.44332)
     | > stopnet_loss: 1.57435  (1.48104)
     | > decoder_coarse_loss: 1.82461  (1.76852)
     | > decoder_ddc_loss: 0.00167  (0.00133)
     | > ga_loss: 0.00286  (0.00226)
     | > decoder_diff_spec_loss: 0.51201  (0.50302)
     | > postnet_diff_spec_loss: 0.81828  (0.81726)
     | > decoder_ssim_loss: 0.31746  (0.31444)
     | > postnet_ssim_loss: 0.32279  (0.32877)
     | > loss: 3.37463  (3.23869)
     | > align_error: 0.98410  (0.98723)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 9.45619  (4.36747)
     | > current_lr: 0.00007 
     | > step_time: 2.40630  (4.57574)
     | > loader_time: 0.01820  (0.06378)


[1m   --> STEP: 29/87 -- GLOBAL_STEP: 278125[0m
     | > decoder_loss: 1.70065  (1.77814)
     | > postnet_loss: 1.33033  (1.41235)
     | > stopnet_loss: 1.57612  (1.67241)
     | > decoder_coarse_loss: 1.67351  (1.72216)
     | > decoder_ddc_loss: 0.00144  (0.00121)
     | > ga_loss: 0.00284  (0.00218)
     | > decoder_diff_spec_loss: 0.47697  (0.48693)
     | > postnet_diff_spec_loss: 0.78047  (0.79901)
     | > decoder_ssim_loss: 0.31620  (0.29167)
     | > postnet_ssim_loss: 0.32847  (0.30657)
     | > loss: 3.24234  (3.38282)
     | > align_error: 0.98665  (0.98843)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 9.99483  (4.46536)
     | > current_lr: 0.00007 
     | > step_time: 2.09480  (3.96941)
     | > loader_time: 0.04280  (0.03637)


[1m   --> STEP: 54/87 -- GLOBAL_STEP: 278150[0m
     | > decoder_loss: 1.75517  (1.76756)
     | > postnet_loss: 1.33219  (1.41175)
     | > stopnet_loss: 1.35328  (1.61971)
     | > decoder_coarse_loss: 1.66586  (1.70926)
     | > decoder_ddc_loss: 0.00112  (0.00129)
     | > ga_loss: 0.00192  (0.00223)
     | > decoder_diff_spec_loss: 0.46940  (0.48621)
     | > postnet_diff_spec_loss: 0.77620  (0.80109)
     | > decoder_ssim_loss: 0.32020  (0.29716)
     | > postnet_ssim_loss: 0.34149  (0.31349)
     | > loss: 3.02827  (3.32779)
     | > align_error: 0.98984  (0.98795)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.70085  (4.34462)
     | > current_lr: 0.00007 
     | > step_time: 4.03200  (3.76124)
     | > loader_time: 0.02090  (0.03412)


[1m   --> STEP: 79/87 -- GLOBAL_STEP: 278175[0m
     | > decoder_loss: 1.93923  (1.76624)
     | > postnet_loss: 1.55427  (1.40535)
     | > stopnet_loss: 1.87736  (1.63400)
     | > decoder_coarse_loss: 1.89884  (1.70918)
     | > decoder_ddc_loss: 0.00096  (0.00124)
     | > ga_loss: 0.00198  (0.00217)
     | > decoder_diff_spec_loss: 0.50210  (0.48859)
     | > postnet_diff_spec_loss: 0.81196  (0.80199)
     | > decoder_ssim_loss: 0.24576  (0.29245)
     | > postnet_ssim_loss: 0.25741  (0.30846)
     | > loss: 3.68990  (3.33821)
     | > align_error: 0.99070  (0.98835)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.60951  (4.03210)
     | > current_lr: 0.00007 
     | > step_time: 3.62660  (3.75858)
     | > loader_time: 0.01940  (0.03512)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.40701 [0m(-0.20419)
     | > avg_decoder_loss:[91m 1.70256 [0m(+0.02178)
     | > avg_postnet_loss:[91m 1.71305 [0m(+0.08289)
     | > avg_stopnet_loss:[91m 1.45866 [0m(+0.00031)
     | > avg_decoder_coarse_loss:[92m 1.62614 [0m(-0.00293)
     | > avg_decoder_ddc_loss:[91m 0.00105 [0m(+0.00002)
     | > avg_ga_loss:[92m 0.00212 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.42611 [0m(-0.00011)
     | > avg_postnet_diff_spec_loss:[91m 0.79152 [0m(+0.00037)
     | > avg_decoder_ssim_loss:[92m 0.30328 [0m(-0.00002)
     | > avg_postnet_ssim_loss:[91m 0.32890 [0m(+0.00099)
     | > avg_loss:[91m 3.19242 [0m(+0.02603)
     | > avg_align_error:[92m 0.99009 [0m(-0.00009)


[4m[1m > EPOCH: 93/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 21:06:29) [0m

[1m   --> STEP: 16/87 -- GLOBAL_STEP: 278200[0m
     | > decoder_loss: 1.63805  (1.75750)
     | > postnet_loss: 1.42141  (1.40377)
     | > stopnet_loss: 2.06419  (1.69898)
     | > decoder_coarse_loss: 1.60264  (1.71637)
     | > decoder_ddc_loss: 0.00149  (0.00125)
     | > ga_loss: 0.00240  (0.00214)
     | > decoder_diff_spec_loss: 0.46019  (0.48106)
     | > postnet_diff_spec_loss: 0.81683  (0.79805)
     | > decoder_ssim_loss: 0.23368  (0.28663)
     | > postnet_ssim_loss: 0.25024  (0.30148)
     | > loss: 3.68231  (3.39622)
     | > align_error: 0.98500  (0.98820)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.49013  (3.95263)
     | > current_lr: 0.00007 
     | > step_time: 3.98490  (4.06323)
     | > loader_time: 0.02340  (0.02590)


[1m   --> STEP: 41/87 -- GLOBAL_STEP: 278225[0m
     | > decoder_loss: 1.67037  (1.77122)
     | > postnet_loss: 1.39046  (1.40519)
     | > stopnet_loss: 2.43277  (1.62300)
     | > decoder_coarse_loss: 1.60209  (1.72204)
     | > decoder_ddc_loss: 0.00074  (0.00131)
     | > ga_loss: 0.00181  (0.00225)
     | > decoder_diff_spec_loss: 0.45437  (0.48985)
     | > postnet_diff_spec_loss: 0.80076  (0.80265)
     | > decoder_ssim_loss: 0.19697  (0.29758)
     | > postnet_ssim_loss: 0.21240  (0.31359)
     | > loss: 4.02383  (3.33512)
     | > align_error: 0.99213  (0.98779)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.42518  (4.66783)
     | > current_lr: 0.00007 
     | > step_time: 5.12490  (3.76051)
     | > loader_time: 0.05500  (0.03143)


[1m   --> STEP: 66/87 -- GLOBAL_STEP: 278250[0m
     | > decoder_loss: 1.86284  (1.75531)
     | > postnet_loss: 1.49334  (1.39806)
     | > stopnet_loss: 1.29933  (1.61720)
     | > decoder_coarse_loss: 1.81213  (1.70188)
     | > decoder_ddc_loss: 0.00137  (0.00129)
     | > ga_loss: 0.00207  (0.00221)
     | > decoder_diff_spec_loss: 0.51854  (0.48709)
     | > postnet_diff_spec_loss: 0.82979  (0.80136)
     | > decoder_ssim_loss: 0.33429  (0.29480)
     | > postnet_ssim_loss: 0.35555  (0.31097)
     | > loss: 3.11166  (3.31591)
     | > align_error: 0.98733  (0.98800)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.30542  (4.35464)
     | > current_lr: 0.00007 
     | > step_time: 3.50580  (3.77395)
     | > loader_time: 0.01840  (0.03173)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.39542 [0m(-0.01160)
     | > avg_decoder_loss:[91m 1.73787 [0m(+0.03531)
     | > avg_postnet_loss:[91m 1.85589 [0m(+0.14284)
     | > avg_stopnet_loss:[92m 1.45755 [0m(-0.00111)
     | > avg_decoder_coarse_loss:[91m 1.64513 [0m(+0.01899)
     | > avg_decoder_ddc_loss:[91m 0.00105 [0m(+0.00000)
     | > avg_ga_loss:[92m 0.00211 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[92m 0.42486 [0m(-0.00125)
     | > avg_postnet_diff_spec_loss:[91m 0.79242 [0m(+0.00090)
     | > avg_decoder_ssim_loss:[91m 0.30335 [0m(+0.00007)
     | > avg_postnet_ssim_loss:[91m 0.33097 [0m(+0.00207)
     | > avg_loss:[91m 3.24099 [0m(+0.04858)
     | > avg_align_error:[92m 0.99007 [0m(-0.00002)


[4m[1m > EPOCH: 94/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 21:13:38) [0m

[1m   --> STEP: 3/87 -- GLOBAL_STEP: 278275[0m
     | > decoder_loss: 1.73293  (1.77679)
     | > postnet_loss: 1.42345  (1.41757)
     | > stopnet_loss: 1.73486  (1.44158)
     | > decoder_coarse_loss: 1.67038  (1.71978)
     | > decoder_ddc_loss: 0.00118  (0.00123)
     | > ga_loss: 0.00224  (0.00205)
     | > decoder_diff_spec_loss: 0.47841  (0.49776)
     | > postnet_diff_spec_loss: 0.81267  (0.81606)
     | > decoder_ssim_loss: 0.27196  (0.31342)
     | > postnet_ssim_loss: 0.28478  (0.33123)
     | > loss: 3.41500  (3.17027)
     | > align_error: 0.98862  (0.98826)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.04648  (2.59669)
     | > current_lr: 0.00007 
     | > step_time: 4.10580  (5.39157)
     | > loader_time: 0.09330  (0.06944)


[1m   --> STEP: 28/87 -- GLOBAL_STEP: 278300[0m
     | > decoder_loss: 1.62966  (1.76484)
     | > postnet_loss: 1.31260  (1.39996)
     | > stopnet_loss: 2.14657  (1.65805)
     | > decoder_coarse_loss: 1.57141  (1.71188)
     | > decoder_ddc_loss: 0.00100  (0.00120)
     | > ga_loss: 0.00214  (0.00215)
     | > decoder_diff_spec_loss: 0.48722  (0.48632)
     | > postnet_diff_spec_loss: 0.79600  (0.79952)
     | > decoder_ssim_loss: 0.22373  (0.29041)
     | > postnet_ssim_loss: 0.23454  (0.30544)
     | > loss: 3.72132  (3.35869)
     | > align_error: 0.98983  (0.98843)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.77335  (4.27978)
     | > current_lr: 0.00007 
     | > step_time: 4.51910  (3.87250)
     | > loader_time: 0.04140  (0.03809)


[1m   --> STEP: 53/87 -- GLOBAL_STEP: 278325[0m
     | > decoder_loss: 1.66038  (1.76042)
     | > postnet_loss: 1.38482  (1.40253)
     | > stopnet_loss: 1.61155  (1.61077)
     | > decoder_coarse_loss: 1.58692  (1.69978)
     | > decoder_ddc_loss: 0.00090  (0.00128)
     | > ga_loss: 0.00159  (0.00223)
     | > decoder_diff_spec_loss: 0.46326  (0.48591)
     | > postnet_diff_spec_loss: 0.78392  (0.80110)
     | > decoder_ssim_loss: 0.27737  (0.29622)
     | > postnet_ssim_loss: 0.29372  (0.31241)
     | > loss: 3.23234  (3.31181)
     | > align_error: 0.99138  (0.98789)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.68529  (4.43972)
     | > current_lr: 0.00007 
     | > step_time: 4.85960  (3.66520)
     | > loader_time: 0.03650  (0.03668)


[1m   --> STEP: 78/87 -- GLOBAL_STEP: 278350[0m
     | > decoder_loss: 1.92868  (1.75639)
     | > postnet_loss: 1.50746  (1.39425)
     | > stopnet_loss: 1.54482  (1.62027)
     | > decoder_coarse_loss: 1.84481  (1.69631)
     | > decoder_ddc_loss: 0.00100  (0.00124)
     | > ga_loss: 0.00184  (0.00216)
     | > decoder_diff_spec_loss: 0.52426  (0.48788)
     | > postnet_diff_spec_loss: 0.82878  (0.80137)
     | > decoder_ssim_loss: 0.29005  (0.29255)
     | > postnet_ssim_loss: 0.30418  (0.30856)
     | > loss: 3.36131  (3.31572)
     | > align_error: 0.99088  (0.98832)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.34765  (4.09135)
     | > current_lr: 0.00007 
     | > step_time: 3.74330  (3.70617)
     | > loader_time: 0.02380  (0.03731)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

warning: audio amplitude out of range, auto clipped.
 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.35186 [0m(-0.04356)
     | > avg_decoder_loss:[92m 1.69145 [0m(-0.04641)
     | > avg_postnet_loss:[92m 1.72869 [0m(-0.12720)
     | > avg_stopnet_loss:[91m 1.45867 [0m(+0.00112)
     | > avg_decoder_coarse_loss:[92m 1.62579 [0m(-0.01934)
     | > avg_decoder_ddc_loss:[91m 0.00110 [0m(+0.00005)
     | > avg_ga_loss:[92m 0.00211 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[91m 0.42642 [0m(+0.00156)
     | > avg_postnet_diff_spec_loss:[92m 0.79069 [0m(-0.00173)
     | > avg_decoder_ssim_loss:[92m 0.30268 [0m(-0.00067)
     | > avg_postnet_ssim_loss:[92m 0.32954 [0m(-0.00143)
     | > avg_loss:[92m 3.19329 [0m(-0.04770)
     | > avg_align_error:[92m 0.98986 [0m(-0.00020)


[4m[1m > EPOCH: 95/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 21:20:40) [0m

[1m   --> STEP: 15/87 -- GLOBAL_STEP: 278375[0m
     | > decoder_loss: 1.72905  (1.76803)
     | > postnet_loss: 1.40622  (1.40225)
     | > stopnet_loss: 2.32452  (1.67427)
     | > decoder_coarse_loss: 1.68677  (1.70598)
     | > decoder_ddc_loss: 0.00089  (0.00122)
     | > ga_loss: 0.00199  (0.00212)
     | > decoder_diff_spec_loss: 0.47443  (0.48287)
     | > postnet_diff_spec_loss: 0.80404  (0.79655)
     | > decoder_ssim_loss: 0.21395  (0.28984)
     | > postnet_ssim_loss: 0.22374  (0.30484)
     | > loss: 3.96926  (3.37277)
     | > align_error: 0.99107  (0.98838)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.46374  (4.08686)
     | > current_lr: 0.00007 
     | > step_time: 4.76860  (4.16061)
     | > loader_time: 0.02600  (0.04129)


[1m   --> STEP: 40/87 -- GLOBAL_STEP: 278400[0m
     | > decoder_loss: 1.70521  (1.76949)
     | > postnet_loss: 1.30714  (1.40303)
     | > stopnet_loss: 1.05574  (1.59520)
     | > decoder_coarse_loss: 1.59590  (1.71034)
     | > decoder_ddc_loss: 0.00170  (0.00131)
     | > ga_loss: 0.00262  (0.00225)
     | > decoder_diff_spec_loss: 0.46636  (0.49018)
     | > postnet_diff_spec_loss: 0.79100  (0.80220)
     | > decoder_ssim_loss: 0.39801  (0.29980)
     | > postnet_ssim_loss: 0.42689  (0.31590)
     | > loss: 2.74190  (3.30452)
     | > align_error: 0.98570  (0.98763)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.55187  (4.44790)
     | > current_lr: 0.00007 
     | > step_time: 2.70810  (3.74029)
     | > loader_time: 0.01520  (0.04144)


[1m   --> STEP: 65/87 -- GLOBAL_STEP: 278425[0m
     | > decoder_loss: 1.60123  (1.74940)
     | > postnet_loss: 1.39409  (1.39195)
     | > stopnet_loss: 1.60838  (1.61769)
     | > decoder_coarse_loss: 1.53676  (1.69095)
     | > decoder_ddc_loss: 0.00095  (0.00128)
     | > ga_loss: 0.00176  (0.00219)
     | > decoder_diff_spec_loss: 0.44990  (0.48636)
     | > postnet_diff_spec_loss: 0.80413  (0.80045)
     | > decoder_ssim_loss: 0.27791  (0.29406)
     | > postnet_ssim_loss: 0.29797  (0.31007)
     | > loss: 3.20790  (3.30979)
     | > align_error: 0.99115  (0.98793)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.20897  (4.15998)
     | > current_lr: 0.00007 
     | > step_time: 4.81030  (3.78858)
     | > loader_time: 0.13150  (0.04161)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

warning: audio amplitude out of range, auto clipped.
 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.49845 [0m(+0.14660)
     | > avg_decoder_loss:[91m 1.69863 [0m(+0.00718)
     | > avg_postnet_loss:[91m 1.73455 [0m(+0.00586)
     | > avg_stopnet_loss:[91m 1.45984 [0m(+0.00117)
     | > avg_decoder_coarse_loss:[91m 1.63906 [0m(+0.01327)
     | > avg_decoder_ddc_loss:[92m 0.00108 [0m(-0.00002)
     | > avg_ga_loss:[92m 0.00210 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[92m 0.42602 [0m(-0.00040)
     | > avg_postnet_diff_spec_loss:[91m 0.79143 [0m(+0.00074)
     | > avg_decoder_ssim_loss:[91m 0.30277 [0m(+0.00010)
     | > avg_postnet_ssim_loss:[92m 0.32895 [0m(-0.00060)
     | > avg_loss:[91m 3.20098 [0m(+0.00769)
     | > avg_align_error:[91m 0.98998 [0m(+0.00011)


[4m[1m > EPOCH: 96/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 21:27:47) [0m

[1m   --> STEP: 2/87 -- GLOBAL_STEP: 278450[0m
     | > decoder_loss: 1.89977  (1.80429)
     | > postnet_loss: 1.46497  (1.40561)
     | > stopnet_loss: 1.28691  (1.25440)
     | > decoder_coarse_loss: 1.82470  (1.76040)
     | > decoder_ddc_loss: 0.00130  (0.00128)
     | > ga_loss: 0.00210  (0.00194)
     | > decoder_diff_spec_loss: 0.53227  (0.51124)
     | > postnet_diff_spec_loss: 0.83660  (0.81807)
     | > decoder_ssim_loss: 0.32714  (0.33408)
     | > postnet_ssim_loss: 0.34854  (0.35335)
     | > loss: 3.10623  (3.01117)
     | > align_error: 0.98708  (0.98797)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 1.66113  (2.34097)
     | > current_lr: 0.00007 
     | > step_time: 5.25630  (6.11069)
     | > loader_time: 0.01800  (0.04023)


[1m   --> STEP: 27/87 -- GLOBAL_STEP: 278475[0m
     | > decoder_loss: 1.76321  (1.76829)
     | > postnet_loss: 1.37480  (1.40129)
     | > stopnet_loss: 1.42136  (1.64700)
     | > decoder_coarse_loss: 1.68391  (1.70912)
     | > decoder_ddc_loss: 0.00106  (0.00124)
     | > ga_loss: 0.00205  (0.00212)
     | > decoder_diff_spec_loss: 0.49003  (0.48735)
     | > postnet_diff_spec_loss: 0.80066  (0.79962)
     | > decoder_ssim_loss: 0.31531  (0.29268)
     | > postnet_ssim_loss: 0.33013  (0.30782)
     | > loss: 3.12139  (3.34945)
     | > align_error: 0.99005  (0.98818)
     | > amp_scaler: 65536.00000  (52186.07407)
     | > grad_norm: 4.67365  (4.70990)
     | > current_lr: 0.00007 
     | > step_time: 3.58560  (4.03864)
     | > loader_time: 0.02060  (0.03811)


[1m   --> STEP: 52/87 -- GLOBAL_STEP: 278500[0m
     | > decoder_loss: 1.86003  (1.75976)
     | > postnet_loss: 1.34448  (1.39839)
     | > stopnet_loss: 1.53214  (1.61733)
     | > decoder_coarse_loss: 1.76549  (1.69570)
     | > decoder_ddc_loss: 0.00133  (0.00133)
     | > ga_loss: 0.00199  (0.00221)
     | > decoder_diff_spec_loss: 0.50291  (0.48660)
     | > postnet_diff_spec_loss: 0.79578  (0.80142)
     | > decoder_ssim_loss: 0.29722  (0.29639)
     | > postnet_ssim_loss: 0.30897  (0.31270)
     | > loss: 3.26114  (3.31647)
     | > align_error: 0.98694  (0.98763)
     | > amp_scaler: 32768.00000  (47891.69231)
     | > grad_norm: 1.74009  (4.22899)
     | > current_lr: 0.00007 
     | > step_time: 3.12350  (3.70818)
     | > loader_time: 0.02420  (0.03532)


[1m   --> STEP: 77/87 -- GLOBAL_STEP: 278525[0m
     | > decoder_loss: 1.76005  (1.75103)
     | > postnet_loss: 1.48637  (1.38689)
     | > stopnet_loss: 1.46174  (1.62485)
     | > decoder_coarse_loss: 1.66205  (1.68758)
     | > decoder_ddc_loss: 0.00092  (0.00127)
     | > ga_loss: 0.00197  (0.00214)
     | > decoder_diff_spec_loss: 0.49067  (0.48728)
     | > postnet_diff_spec_loss: 0.82463  (0.80074)
     | > decoder_ssim_loss: 0.30212  (0.29241)
     | > postnet_ssim_loss: 0.31649  (0.30842)
     | > loss: 3.18242  (3.31448)
     | > align_error: 0.99120  (0.98811)
     | > amp_scaler: 32768.00000  (42981.40260)
     | > grad_norm: 2.87813  (3.92738)
     | > current_lr: 0.00007 
     | > step_time: 2.67350  (3.76323)
     | > loader_time: 0.02110  (0.03572)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.24388 [0m(-0.25457)
     | > avg_decoder_loss:[91m 1.70202 [0m(+0.00339)
     | > avg_postnet_loss:[92m 1.73199 [0m(-0.00257)
     | > avg_stopnet_loss:[92m 1.45855 [0m(-0.00129)
     | > avg_decoder_coarse_loss:[92m 1.63058 [0m(-0.00848)
     | > avg_decoder_ddc_loss:[92m 0.00107 [0m(-0.00001)
     | > avg_ga_loss:[91m 0.00210 [0m(+0.00000)
     | > avg_decoder_diff_spec_loss:[92m 0.42479 [0m(-0.00123)
     | > avg_postnet_diff_spec_loss:[91m 0.79146 [0m(+0.00003)
     | > avg_decoder_ssim_loss:[92m 0.30275 [0m(-0.00003)
     | > avg_postnet_ssim_loss:[91m 0.32961 [0m(+0.00067)
     | > avg_loss:[92m 3.19764 [0m(-0.00335)
     | > avg_align_error:[92m 0.98993 [0m(-0.00005)


[4m[1m > EPOCH: 97/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 21:34:51) [0m

[1m   --> STEP: 14/87 -- GLOBAL_STEP: 278550[0m
     | > decoder_loss: 1.93790  (1.77231)
     | > postnet_loss: 1.43611  (1.39791)
     | > stopnet_loss: 1.71442  (1.62930)
     | > decoder_coarse_loss: 1.84655  (1.69634)
     | > decoder_ddc_loss: 0.00110  (0.00129)
     | > ga_loss: 0.00202  (0.00210)
     | > decoder_diff_spec_loss: 0.50518  (0.48364)
     | > postnet_diff_spec_loss: 0.79091  (0.79623)
     | > decoder_ssim_loss: 0.27276  (0.29554)
     | > postnet_ssim_loss: 0.29007  (0.31054)
     | > loss: 3.49466  (3.32823)
     | > align_error: 0.98983  (0.98802)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.28292  (4.15214)
     | > current_lr: 0.00007 
     | > step_time: 3.16720  (4.26030)
     | > loader_time: 0.01640  (0.05013)


[1m   --> STEP: 39/87 -- GLOBAL_STEP: 278575[0m
     | > decoder_loss: 1.74276  (1.77166)
     | > postnet_loss: 1.39766  (1.40115)
     | > stopnet_loss: 1.31052  (1.60806)
     | > decoder_coarse_loss: 1.64715  (1.70436)
     | > decoder_ddc_loss: 0.00175  (0.00134)
     | > ga_loss: 0.00269  (0.00222)
     | > decoder_diff_spec_loss: 0.48877  (0.49004)
     | > postnet_diff_spec_loss: 0.80653  (0.80282)
     | > decoder_ssim_loss: 0.34845  (0.29732)
     | > postnet_ssim_loss: 0.36679  (0.31303)
     | > loss: 3.02395  (3.31458)
     | > align_error: 0.98424  (0.98755)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.04328  (4.58214)
     | > current_lr: 0.00007 
     | > step_time: 2.76060  (3.83573)
     | > loader_time: 0.01550  (0.04087)


[1m   --> STEP: 64/87 -- GLOBAL_STEP: 278600[0m
     | > decoder_loss: 1.76421  (1.75201)
     | > postnet_loss: 1.40160  (1.38873)
     | > stopnet_loss: 1.32171  (1.62088)
     | > decoder_coarse_loss: 1.71581  (1.68507)
     | > decoder_ddc_loss: 0.00153  (0.00132)
     | > ga_loss: 0.00221  (0.00219)
     | > decoder_diff_spec_loss: 0.49847  (0.48644)
     | > postnet_diff_spec_loss: 0.80575  (0.80060)
     | > decoder_ssim_loss: 0.32274  (0.29422)
     | > postnet_ssim_loss: 0.33735  (0.31018)
     | > loss: 3.04462  (3.31145)
     | > align_error: 0.98685  (0.98782)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.87558  (4.27287)
     | > current_lr: 0.00007 
     | > step_time: 3.99890  (3.88056)
     | > loader_time: 0.05760  (0.03991)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.39759 [0m(+0.15371)
     | > avg_decoder_loss:[92m 1.66855 [0m(-0.03347)
     | > avg_postnet_loss:[91m 1.74240 [0m(+0.01042)
     | > avg_stopnet_loss:[92m 1.45781 [0m(-0.00073)
     | > avg_decoder_coarse_loss:[91m 1.64623 [0m(+0.01565)
     | > avg_decoder_ddc_loss:[92m 0.00107 [0m(-0.00000)
     | > avg_ga_loss:[92m 0.00209 [0m(-0.00001)
     | > avg_decoder_diff_spec_loss:[91m 0.42777 [0m(+0.00297)
     | > avg_postnet_diff_spec_loss:[91m 0.79163 [0m(+0.00017)
     | > avg_decoder_ssim_loss:[92m 0.30232 [0m(-0.00043)
     | > avg_postnet_ssim_loss:[92m 0.32932 [0m(-0.00029)
     | > avg_loss:[92m 3.19561 [0m(-0.00203)
     | > avg_align_error:[92m 0.98992 [0m(-0.00001)


[4m[1m > EPOCH: 98/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 21:42:04) [0m

[1m   --> STEP: 1/87 -- GLOBAL_STEP: 278625[0m
     | > decoder_loss: 1.72604  (1.72604)
     | > postnet_loss: 1.34472  (1.34472)
     | > stopnet_loss: 1.26559  (1.26559)
     | > decoder_coarse_loss: 1.68176  (1.68176)
     | > decoder_ddc_loss: 0.00125  (0.00125)
     | > ga_loss: 0.00178  (0.00178)
     | > decoder_diff_spec_loss: 0.49110  (0.49110)
     | > postnet_diff_spec_loss: 0.79908  (0.79908)
     | > decoder_ssim_loss: 0.34050  (0.34050)
     | > postnet_ssim_loss: 0.35804  (0.35804)
     | > loss: 2.96009  (2.96009)
     | > align_error: 0.98870  (0.98870)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.27426  (3.27426)
     | > current_lr: 0.00007 
     | > step_time: 7.11860  (7.11856)
     | > loader_time: 0.03090  (0.03092)


[1m   --> STEP: 26/87 -- GLOBAL_STEP: 278650[0m
     | > decoder_loss: 1.86808  (1.77038)
     | > postnet_loss: 1.38473  (1.39862)
     | > stopnet_loss: 1.69051  (1.64473)
     | > decoder_coarse_loss: 1.83916  (1.71482)
     | > decoder_ddc_loss: 0.00086  (0.00125)
     | > ga_loss: 0.00174  (0.00211)
     | > decoder_diff_spec_loss: 0.51506  (0.48671)
     | > postnet_diff_spec_loss: 0.79502  (0.79864)
     | > decoder_ssim_loss: 0.26390  (0.29158)
     | > postnet_ssim_loss: 0.28092  (0.30677)
     | > loss: 3.43616  (3.34749)
     | > align_error: 0.99111  (0.98809)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.12039  (4.15087)
     | > current_lr: 0.00007 
     | > step_time: 4.99290  (4.00339)
     | > loader_time: 0.02440  (0.03669)


[1m   --> STEP: 51/87 -- GLOBAL_STEP: 278675[0m
     | > decoder_loss: 1.63951  (1.75518)
     | > postnet_loss: 1.43093  (1.39718)
     | > stopnet_loss: 1.56394  (1.61153)
     | > decoder_coarse_loss: 1.59021  (1.69643)
     | > decoder_ddc_loss: 0.00164  (0.00135)
     | > ga_loss: 0.00231  (0.00220)
     | > decoder_diff_spec_loss: 0.45189  (0.48606)
     | > postnet_diff_spec_loss: 0.79411  (0.80072)
     | > decoder_ssim_loss: 0.28501  (0.29603)
     | > postnet_ssim_loss: 0.30632  (0.31230)
     | > loss: 3.20039  (3.30883)
     | > align_error: 0.98599  (0.98754)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.34317  (4.34097)
     | > current_lr: 0.00007 
     | > step_time: 3.61740  (3.73333)
     | > loader_time: 0.01750  (0.03645)


[1m   --> STEP: 76/87 -- GLOBAL_STEP: 278700[0m
     | > decoder_loss: 1.74539  (1.74812)
     | > postnet_loss: 1.33508  (1.38237)
     | > stopnet_loss: 1.84317  (1.61981)
     | > decoder_coarse_loss: 1.69687  (1.69119)
     | > decoder_ddc_loss: 0.00106  (0.00130)
     | > ga_loss: 0.00180  (0.00213)
     | > decoder_diff_spec_loss: 0.50354  (0.48717)
     | > postnet_diff_spec_loss: 0.79483  (0.79977)
     | > decoder_ssim_loss: 0.24719  (0.29190)
     | > postnet_ssim_loss: 0.25850  (0.30792)
     | > loss: 3.49780  (3.30790)
     | > align_error: 0.98979  (0.98798)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 2.76077  (4.06571)
     | > current_lr: 0.00007 
     | > step_time: 3.89040  (3.77148)
     | > loader_time: 0.02390  (0.03918)


 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[92m 3.37065 [0m(-0.02694)
     | > avg_decoder_loss:[92m 1.66666 [0m(-0.00189)
     | > avg_postnet_loss:[92m 1.64361 [0m(-0.09880)
     | > avg_stopnet_loss:[92m 1.45714 [0m(-0.00068)
     | > avg_decoder_coarse_loss:[92m 1.60370 [0m(-0.04253)
     | > avg_decoder_ddc_loss:[91m 0.00110 [0m(+0.00003)
     | > avg_ga_loss:[92m 0.00209 [0m(-0.00000)
     | > avg_decoder_diff_spec_loss:[92m 0.42330 [0m(-0.00447)
     | > avg_postnet_diff_spec_loss:[92m 0.79066 [0m(-0.00097)
     | > avg_decoder_ssim_loss:[92m 0.30189 [0m(-0.00043)
     | > avg_postnet_ssim_loss:[92m 0.32790 [0m(-0.00142)
     | > avg_loss:[92m 3.15729 [0m(-0.03832)
     | > avg_align_error:[92m 0.98981 [0m(-0.00010)


[4m[1m > EPOCH: 99/100[0m
 --> /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 5679
 | > Max length sequence: 721942.0
 | > Min length sequence: 7462.0
 | > Avg length sequence: 135172.60802958268
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 101
 | > Batch group size: 0.

[1m > TRAINING (2021-12-10 21:49:09) [0m

[1m   --> STEP: 13/87 -- GLOBAL_STEP: 278725[0m
     | > decoder_loss: 1.85302  (1.74738)
     | > postnet_loss: 1.47895  (1.39988)
     | > stopnet_loss: 2.03063  (1.61599)
     | > decoder_coarse_loss: 1.79382  (1.68254)
     | > decoder_ddc_loss: 0.00153  (0.00132)
     | > ga_loss: 0.00222  (0.00209)
     | > decoder_diff_spec_loss: 0.47610  (0.48112)
     | > postnet_diff_spec_loss: 0.79537  (0.79620)
     | > decoder_ssim_loss: 0.23012  (0.29635)
     | > postnet_ssim_loss: 0.24827  (0.31157)
     | > loss: 3.76104  (3.30553)
     | > align_error: 0.98495  (0.98776)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.61667  (4.07288)
     | > current_lr: 0.00007 
     | > step_time: 3.68210  (4.18936)
     | > loader_time: 0.02010  (0.03751)


[1m   --> STEP: 38/87 -- GLOBAL_STEP: 278750[0m
     | > decoder_loss: 1.94249  (1.76640)
     | > postnet_loss: 1.46700  (1.40222)
     | > stopnet_loss: 1.64793  (1.61018)
     | > decoder_coarse_loss: 1.87895  (1.70557)
     | > decoder_ddc_loss: 0.00142  (0.00133)
     | > ga_loss: 0.00203  (0.00219)
     | > decoder_diff_spec_loss: 0.54340  (0.49055)
     | > postnet_diff_spec_loss: 0.82674  (0.80206)
     | > decoder_ssim_loss: 0.27251  (0.29536)
     | > postnet_ssim_loss: 0.28758  (0.31125)
     | > loss: 3.46312  (3.31481)
     | > align_error: 0.98640  (0.98756)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 3.89906  (4.47243)
     | > current_lr: 0.00007 
     | > step_time: 3.69340  (3.80441)
     | > loader_time: 0.07540  (0.03291)


[1m   --> STEP: 63/87 -- GLOBAL_STEP: 278775[0m
     | > decoder_loss: 1.60334  (1.74255)
     | > postnet_loss: 1.34041  (1.38661)
     | > stopnet_loss: 2.14110  (1.61784)
     | > decoder_coarse_loss: 1.56844  (1.68420)
     | > decoder_ddc_loss: 0.00127  (0.00132)
     | > ga_loss: 0.00240  (0.00217)
     | > decoder_diff_spec_loss: 0.46929  (0.48576)
     | > postnet_diff_spec_loss: 0.79224  (0.80004)
     | > decoder_ssim_loss: 0.22698  (0.29313)
     | > postnet_ssim_loss: 0.24278  (0.30949)
     | > loss: 3.71431  (3.30444)
     | > align_error: 0.98796  (0.98771)
     | > amp_scaler: 32768.00000  (32768.00000)
     | > grad_norm: 4.45751  (4.26100)
     | > current_lr: 0.00007 
     | > step_time: 3.68090  (3.75015)
     | > loader_time: 0.01920  (0.03390)


 > CHECKPOINT : /mnt/aibb_data/development/atseng/VCproject/trained_models/tacotron-DDC/tecotronddc_libritts-December-10-2021_09+37AM-f92878cf/checkpoint_278775.pth.tar

 > DataLoader initialization
 | > Use phonemes: True
   | > phoneme language: en-us
 | > Number of instances : 57
 | > Max length sequence: 338182.0
 | > Min length sequence: 13462.0
 | > Avg length sequence: 131701.94736842104
 | > Num. instances discarded by max-min (max=440000, min=0) seq limits: 0
 | > Batch group size: 0.

[1m > EVALUATION [0m

warning: audio amplitude out of range, auto clipped.
 | > Synthesizing test sentences.
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps
   | > Decoder stopped with 'max_decoder_steps

  [1m--> EVAL PERFORMANCE[0m
     | > avg_loader_time:[91m 3.49352 [0m(+0.12287)
     | > avg_decoder_loss:[91m 1.70158 [0m(+0.03491)
     | > avg_postnet_loss:[91m 1.70227 [0m(+0.05866)
     | > avg_stopnet_loss:[92m 1.45615 [0m(-0.00099)
     | > avg_decoder_coarse_loss:[91m 1.62892 [0m(+0.02522)
     | > avg_decoder_ddc_loss:[92m 0.00107 [0m(-0.00003)
     | > avg_ga_loss:[91m 0.00209 [0m(+0.00000)
     | > avg_decoder_diff_spec_loss:[91m 0.42460 [0m(+0.00130)
     | > avg_postnet_diff_spec_loss:[91m 0.79159 [0m(+0.00093)
     | > avg_decoder_ssim_loss:[91m 0.30206 [0m(+0.00017)
     | > avg_postnet_ssim_loss:[91m 0.32895 [0m(+0.00105)
     | > avg_loss:[91m 3.18687 [0m(+0.02958)
     | > avg_align_error:[91m 0.98984 [0m(+0.00003)

